{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.24972811858057758,
  "eval_steps": 500,
  "global_step": 1550,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00016111491521327587,
      "grad_norm": 16.989198684692383,
      "learning_rate": 0.0005,
      "loss": 7.6948,
      "step": 1
    },
    {
      "epoch": 0.00032222983042655174,
      "grad_norm": 9.909006118774414,
      "learning_rate": 0.0004996778350515464,
      "loss": 6.3008,
      "step": 2
    },
    {
      "epoch": 0.00048334474563982763,
      "grad_norm": 10.246139526367188,
      "learning_rate": 0.0004993556701030927,
      "loss": 5.1154,
      "step": 3
    },
    {
      "epoch": 0.0006444596608531035,
      "grad_norm": 10.740690231323242,
      "learning_rate": 0.0004990335051546392,
      "loss": 3.9137,
      "step": 4
    },
    {
      "epoch": 0.0008055745760663794,
      "grad_norm": 6.362662315368652,
      "learning_rate": 0.0004987113402061856,
      "loss": 3.2885,
      "step": 5
    },
    {
      "epoch": 0.0009666894912796553,
      "grad_norm": 4.254363536834717,
      "learning_rate": 0.000498389175257732,
      "loss": 2.421,
      "step": 6
    },
    {
      "epoch": 0.0011278044064929312,
      "grad_norm": 4.151869297027588,
      "learning_rate": 0.0004980670103092783,
      "loss": 2.524,
      "step": 7
    },
    {
      "epoch": 0.001288919321706207,
      "grad_norm": 3.7938544750213623,
      "learning_rate": 0.0004977448453608248,
      "loss": 1.81,
      "step": 8
    },
    {
      "epoch": 0.0014500342369194827,
      "grad_norm": 5.964208126068115,
      "learning_rate": 0.0004974226804123712,
      "loss": 2.1283,
      "step": 9
    },
    {
      "epoch": 0.0016111491521327587,
      "grad_norm": 4.7205095291137695,
      "learning_rate": 0.0004971005154639175,
      "loss": 2.087,
      "step": 10
    },
    {
      "epoch": 0.0017722640673460345,
      "grad_norm": 3.0366857051849365,
      "learning_rate": 0.0004967783505154639,
      "loss": 1.6025,
      "step": 11
    },
    {
      "epoch": 0.0019333789825593105,
      "grad_norm": 3.6344399452209473,
      "learning_rate": 0.0004964561855670104,
      "loss": 1.8535,
      "step": 12
    },
    {
      "epoch": 0.002094493897772586,
      "grad_norm": 3.3062567710876465,
      "learning_rate": 0.0004961340206185567,
      "loss": 1.4937,
      "step": 13
    },
    {
      "epoch": 0.0022556088129858623,
      "grad_norm": 2.7995877265930176,
      "learning_rate": 0.0004958118556701031,
      "loss": 1.1375,
      "step": 14
    },
    {
      "epoch": 0.002416723728199138,
      "grad_norm": 5.186984062194824,
      "learning_rate": 0.0004954896907216495,
      "loss": 1.734,
      "step": 15
    },
    {
      "epoch": 0.002577838643412414,
      "grad_norm": 2.968339443206787,
      "learning_rate": 0.000495167525773196,
      "loss": 1.5803,
      "step": 16
    },
    {
      "epoch": 0.0027389535586256897,
      "grad_norm": 3.0189669132232666,
      "learning_rate": 0.0004948453608247423,
      "loss": 1.4167,
      "step": 17
    },
    {
      "epoch": 0.0029000684738389655,
      "grad_norm": 3.644780397415161,
      "learning_rate": 0.0004945231958762887,
      "loss": 1.5826,
      "step": 18
    },
    {
      "epoch": 0.0030611833890522417,
      "grad_norm": 2.1753506660461426,
      "learning_rate": 0.000494201030927835,
      "loss": 1.1113,
      "step": 19
    },
    {
      "epoch": 0.0032222983042655175,
      "grad_norm": 3.0685741901397705,
      "learning_rate": 0.0004938788659793815,
      "loss": 1.2247,
      "step": 20
    },
    {
      "epoch": 0.0033834132194787933,
      "grad_norm": 3.0341813564300537,
      "learning_rate": 0.0004935567010309279,
      "loss": 1.1584,
      "step": 21
    },
    {
      "epoch": 0.003544528134692069,
      "grad_norm": 2.543766498565674,
      "learning_rate": 0.0004932345360824743,
      "loss": 1.2859,
      "step": 22
    },
    {
      "epoch": 0.003705643049905345,
      "grad_norm": 3.1267688274383545,
      "learning_rate": 0.0004929123711340206,
      "loss": 1.1435,
      "step": 23
    },
    {
      "epoch": 0.003866757965118621,
      "grad_norm": 2.5512044429779053,
      "learning_rate": 0.0004925902061855671,
      "loss": 1.0666,
      "step": 24
    },
    {
      "epoch": 0.004027872880331897,
      "grad_norm": 2.757141590118408,
      "learning_rate": 0.0004922680412371135,
      "loss": 1.0104,
      "step": 25
    },
    {
      "epoch": 0.004188987795545172,
      "grad_norm": 2.5576248168945312,
      "learning_rate": 0.0004919458762886597,
      "loss": 1.0321,
      "step": 26
    },
    {
      "epoch": 0.004350102710758448,
      "grad_norm": 2.458871841430664,
      "learning_rate": 0.0004916237113402062,
      "loss": 0.8868,
      "step": 27
    },
    {
      "epoch": 0.004511217625971725,
      "grad_norm": 2.4319264888763428,
      "learning_rate": 0.0004913015463917526,
      "loss": 0.7096,
      "step": 28
    },
    {
      "epoch": 0.004672332541185,
      "grad_norm": 1.9354804754257202,
      "learning_rate": 0.0004909793814432989,
      "loss": 0.63,
      "step": 29
    },
    {
      "epoch": 0.004833447456398276,
      "grad_norm": 2.281217098236084,
      "learning_rate": 0.0004906572164948453,
      "loss": 0.6562,
      "step": 30
    },
    {
      "epoch": 0.0049945623716115516,
      "grad_norm": 2.623706579208374,
      "learning_rate": 0.0004903350515463918,
      "loss": 0.7585,
      "step": 31
    },
    {
      "epoch": 0.005155677286824828,
      "grad_norm": 2.5421597957611084,
      "learning_rate": 0.0004900128865979381,
      "loss": 0.7046,
      "step": 32
    },
    {
      "epoch": 0.005316792202038104,
      "grad_norm": 3.248033046722412,
      "learning_rate": 0.0004896907216494845,
      "loss": 0.8537,
      "step": 33
    },
    {
      "epoch": 0.005477907117251379,
      "grad_norm": 2.4455349445343018,
      "learning_rate": 0.0004893685567010309,
      "loss": 0.81,
      "step": 34
    },
    {
      "epoch": 0.005639022032464656,
      "grad_norm": 2.1325228214263916,
      "learning_rate": 0.0004890463917525774,
      "loss": 0.693,
      "step": 35
    },
    {
      "epoch": 0.005800136947677931,
      "grad_norm": 2.247349977493286,
      "learning_rate": 0.0004887242268041237,
      "loss": 0.6385,
      "step": 36
    },
    {
      "epoch": 0.005961251862891207,
      "grad_norm": 2.1133346557617188,
      "learning_rate": 0.0004884020618556701,
      "loss": 0.5816,
      "step": 37
    },
    {
      "epoch": 0.006122366778104483,
      "grad_norm": 2.4887454509735107,
      "learning_rate": 0.0004880798969072165,
      "loss": 0.6583,
      "step": 38
    },
    {
      "epoch": 0.006283481693317759,
      "grad_norm": 2.250190258026123,
      "learning_rate": 0.0004877577319587629,
      "loss": 0.6399,
      "step": 39
    },
    {
      "epoch": 0.006444596608531035,
      "grad_norm": 2.1287457942962646,
      "learning_rate": 0.0004874355670103093,
      "loss": 0.5731,
      "step": 40
    },
    {
      "epoch": 0.00660571152374431,
      "grad_norm": 2.246196746826172,
      "learning_rate": 0.00048711340206185566,
      "loss": 0.6228,
      "step": 41
    },
    {
      "epoch": 0.0067668264389575865,
      "grad_norm": 2.3380916118621826,
      "learning_rate": 0.0004867912371134021,
      "loss": 0.5815,
      "step": 42
    },
    {
      "epoch": 0.006927941354170863,
      "grad_norm": 1.9778051376342773,
      "learning_rate": 0.00048646907216494845,
      "loss": 0.6186,
      "step": 43
    },
    {
      "epoch": 0.007089056269384138,
      "grad_norm": 2.5901577472686768,
      "learning_rate": 0.00048614690721649487,
      "loss": 0.5342,
      "step": 44
    },
    {
      "epoch": 0.007250171184597414,
      "grad_norm": 2.659701108932495,
      "learning_rate": 0.00048582474226804124,
      "loss": 0.5887,
      "step": 45
    },
    {
      "epoch": 0.00741128609981069,
      "grad_norm": 2.2181406021118164,
      "learning_rate": 0.00048550257731958766,
      "loss": 0.49,
      "step": 46
    },
    {
      "epoch": 0.007572401015023966,
      "grad_norm": 2.078618288040161,
      "learning_rate": 0.000485180412371134,
      "loss": 0.5164,
      "step": 47
    },
    {
      "epoch": 0.007733515930237242,
      "grad_norm": 1.7721823453903198,
      "learning_rate": 0.00048485824742268045,
      "loss": 0.4755,
      "step": 48
    },
    {
      "epoch": 0.007894630845450518,
      "grad_norm": 2.1516361236572266,
      "learning_rate": 0.0004845360824742268,
      "loss": 0.5427,
      "step": 49
    },
    {
      "epoch": 0.008055745760663794,
      "grad_norm": 3.0584404468536377,
      "learning_rate": 0.0004842139175257732,
      "loss": 0.5643,
      "step": 50
    },
    {
      "epoch": 0.008216860675877069,
      "grad_norm": 2.6693174839019775,
      "learning_rate": 0.0004838917525773196,
      "loss": 0.5486,
      "step": 51
    },
    {
      "epoch": 0.008377975591090344,
      "grad_norm": 2.3394811153411865,
      "learning_rate": 0.00048356958762886597,
      "loss": 0.5974,
      "step": 52
    },
    {
      "epoch": 0.008539090506303621,
      "grad_norm": 2.366424322128296,
      "learning_rate": 0.0004832474226804124,
      "loss": 0.4513,
      "step": 53
    },
    {
      "epoch": 0.008700205421516897,
      "grad_norm": 2.9080798625946045,
      "learning_rate": 0.00048292525773195876,
      "loss": 0.4219,
      "step": 54
    },
    {
      "epoch": 0.008861320336730172,
      "grad_norm": 2.9994263648986816,
      "learning_rate": 0.0004826030927835052,
      "loss": 0.5913,
      "step": 55
    },
    {
      "epoch": 0.00902243525194345,
      "grad_norm": 2.524223566055298,
      "learning_rate": 0.00048228092783505154,
      "loss": 0.5326,
      "step": 56
    },
    {
      "epoch": 0.009183550167156725,
      "grad_norm": 2.076624870300293,
      "learning_rate": 0.00048195876288659796,
      "loss": 0.5521,
      "step": 57
    },
    {
      "epoch": 0.00934466508237,
      "grad_norm": 2.1580936908721924,
      "learning_rate": 0.00048163659793814433,
      "loss": 0.4394,
      "step": 58
    },
    {
      "epoch": 0.009505779997583277,
      "grad_norm": 3.5567896366119385,
      "learning_rate": 0.00048131443298969075,
      "loss": 0.424,
      "step": 59
    },
    {
      "epoch": 0.009666894912796552,
      "grad_norm": 2.136643409729004,
      "learning_rate": 0.0004809922680412371,
      "loss": 0.5266,
      "step": 60
    },
    {
      "epoch": 0.009828009828009828,
      "grad_norm": 1.5821194648742676,
      "learning_rate": 0.00048067010309278354,
      "loss": 0.5222,
      "step": 61
    },
    {
      "epoch": 0.009989124743223103,
      "grad_norm": 1.7895991802215576,
      "learning_rate": 0.0004803479381443299,
      "loss": 0.5064,
      "step": 62
    },
    {
      "epoch": 0.01015023965843638,
      "grad_norm": 2.48769211769104,
      "learning_rate": 0.00048002577319587633,
      "loss": 0.4619,
      "step": 63
    },
    {
      "epoch": 0.010311354573649656,
      "grad_norm": 2.095229387283325,
      "learning_rate": 0.0004797036082474227,
      "loss": 0.4305,
      "step": 64
    },
    {
      "epoch": 0.010472469488862931,
      "grad_norm": 2.1067910194396973,
      "learning_rate": 0.0004793814432989691,
      "loss": 0.4615,
      "step": 65
    },
    {
      "epoch": 0.010633584404076208,
      "grad_norm": 1.5985925197601318,
      "learning_rate": 0.0004790592783505155,
      "loss": 0.3859,
      "step": 66
    },
    {
      "epoch": 0.010794699319289483,
      "grad_norm": 1.7708303928375244,
      "learning_rate": 0.0004787371134020619,
      "loss": 0.4296,
      "step": 67
    },
    {
      "epoch": 0.010955814234502759,
      "grad_norm": 2.0352933406829834,
      "learning_rate": 0.00047841494845360827,
      "loss": 0.4754,
      "step": 68
    },
    {
      "epoch": 0.011116929149716036,
      "grad_norm": 2.0030248165130615,
      "learning_rate": 0.0004780927835051547,
      "loss": 0.5421,
      "step": 69
    },
    {
      "epoch": 0.011278044064929311,
      "grad_norm": 1.5826507806777954,
      "learning_rate": 0.00047777061855670106,
      "loss": 0.4276,
      "step": 70
    },
    {
      "epoch": 0.011439158980142587,
      "grad_norm": 1.9434146881103516,
      "learning_rate": 0.0004774484536082475,
      "loss": 0.505,
      "step": 71
    },
    {
      "epoch": 0.011600273895355862,
      "grad_norm": 1.4505151510238647,
      "learning_rate": 0.00047712628865979385,
      "loss": 0.5093,
      "step": 72
    },
    {
      "epoch": 0.011761388810569139,
      "grad_norm": 1.6524708271026611,
      "learning_rate": 0.00047680412371134027,
      "loss": 0.412,
      "step": 73
    },
    {
      "epoch": 0.011922503725782414,
      "grad_norm": 2.439998149871826,
      "learning_rate": 0.0004764819587628866,
      "loss": 0.4448,
      "step": 74
    },
    {
      "epoch": 0.01208361864099569,
      "grad_norm": 1.7464946508407593,
      "learning_rate": 0.00047615979381443295,
      "loss": 0.5202,
      "step": 75
    },
    {
      "epoch": 0.012244733556208967,
      "grad_norm": 1.9702059030532837,
      "learning_rate": 0.00047583762886597937,
      "loss": 0.5047,
      "step": 76
    },
    {
      "epoch": 0.012405848471422242,
      "grad_norm": 2.1595335006713867,
      "learning_rate": 0.00047551546391752574,
      "loss": 0.4993,
      "step": 77
    },
    {
      "epoch": 0.012566963386635517,
      "grad_norm": 2.1230149269104004,
      "learning_rate": 0.00047519329896907216,
      "loss": 0.5038,
      "step": 78
    },
    {
      "epoch": 0.012728078301848793,
      "grad_norm": 1.1707223653793335,
      "learning_rate": 0.0004748711340206185,
      "loss": 0.4475,
      "step": 79
    },
    {
      "epoch": 0.01288919321706207,
      "grad_norm": 1.1383082866668701,
      "learning_rate": 0.00047454896907216495,
      "loss": 0.5093,
      "step": 80
    },
    {
      "epoch": 0.013050308132275345,
      "grad_norm": 3.0513756275177,
      "learning_rate": 0.0004742268041237113,
      "loss": 0.4976,
      "step": 81
    },
    {
      "epoch": 0.01321142304748862,
      "grad_norm": 1.3671786785125732,
      "learning_rate": 0.00047390463917525773,
      "loss": 0.4708,
      "step": 82
    },
    {
      "epoch": 0.013372537962701898,
      "grad_norm": 1.4893923997879028,
      "learning_rate": 0.0004735824742268041,
      "loss": 0.4542,
      "step": 83
    },
    {
      "epoch": 0.013533652877915173,
      "grad_norm": 1.4309135675430298,
      "learning_rate": 0.0004732603092783505,
      "loss": 0.4058,
      "step": 84
    },
    {
      "epoch": 0.013694767793128448,
      "grad_norm": 2.321528911590576,
      "learning_rate": 0.0004729381443298969,
      "loss": 0.4874,
      "step": 85
    },
    {
      "epoch": 0.013855882708341725,
      "grad_norm": 1.6265134811401367,
      "learning_rate": 0.0004726159793814433,
      "loss": 0.5634,
      "step": 86
    },
    {
      "epoch": 0.014016997623555,
      "grad_norm": 1.2053555250167847,
      "learning_rate": 0.0004722938144329897,
      "loss": 0.4379,
      "step": 87
    },
    {
      "epoch": 0.014178112538768276,
      "grad_norm": 1.8652007579803467,
      "learning_rate": 0.0004719716494845361,
      "loss": 0.4742,
      "step": 88
    },
    {
      "epoch": 0.014339227453981552,
      "grad_norm": 2.1167988777160645,
      "learning_rate": 0.00047164948453608246,
      "loss": 0.7324,
      "step": 89
    },
    {
      "epoch": 0.014500342369194829,
      "grad_norm": 1.7262436151504517,
      "learning_rate": 0.0004713273195876289,
      "loss": 0.3697,
      "step": 90
    },
    {
      "epoch": 0.014661457284408104,
      "grad_norm": 1.8914552927017212,
      "learning_rate": 0.00047100515463917525,
      "loss": 0.3697,
      "step": 91
    },
    {
      "epoch": 0.01482257219962138,
      "grad_norm": 1.3565309047698975,
      "learning_rate": 0.0004706829896907217,
      "loss": 0.4216,
      "step": 92
    },
    {
      "epoch": 0.014983687114834656,
      "grad_norm": 1.6039961576461792,
      "learning_rate": 0.00047036082474226804,
      "loss": 0.4934,
      "step": 93
    },
    {
      "epoch": 0.015144802030047932,
      "grad_norm": 1.4572175741195679,
      "learning_rate": 0.00047003865979381446,
      "loss": 0.3514,
      "step": 94
    },
    {
      "epoch": 0.015305916945261207,
      "grad_norm": 1.342907428741455,
      "learning_rate": 0.00046971649484536083,
      "loss": 0.4708,
      "step": 95
    },
    {
      "epoch": 0.015467031860474484,
      "grad_norm": 1.480038046836853,
      "learning_rate": 0.00046939432989690725,
      "loss": 0.4377,
      "step": 96
    },
    {
      "epoch": 0.01562814677568776,
      "grad_norm": 2.7156097888946533,
      "learning_rate": 0.0004690721649484536,
      "loss": 0.4583,
      "step": 97
    },
    {
      "epoch": 0.015789261690901037,
      "grad_norm": 2.7065722942352295,
      "learning_rate": 0.00046875,
      "loss": 0.5165,
      "step": 98
    },
    {
      "epoch": 0.01595037660611431,
      "grad_norm": 2.255750894546509,
      "learning_rate": 0.0004684278350515464,
      "loss": 0.4761,
      "step": 99
    },
    {
      "epoch": 0.016111491521327587,
      "grad_norm": 1.7812459468841553,
      "learning_rate": 0.00046810567010309277,
      "loss": 0.5435,
      "step": 100
    },
    {
      "epoch": 0.016272606436540864,
      "grad_norm": 1.8639202117919922,
      "learning_rate": 0.0004677835051546392,
      "loss": 0.3382,
      "step": 101
    },
    {
      "epoch": 0.016433721351754138,
      "grad_norm": 2.271258592605591,
      "learning_rate": 0.00046746134020618556,
      "loss": 0.4276,
      "step": 102
    },
    {
      "epoch": 0.016594836266967415,
      "grad_norm": 3.097734212875366,
      "learning_rate": 0.000467139175257732,
      "loss": 0.5046,
      "step": 103
    },
    {
      "epoch": 0.01675595118218069,
      "grad_norm": 2.322845935821533,
      "learning_rate": 0.00046681701030927835,
      "loss": 0.4262,
      "step": 104
    },
    {
      "epoch": 0.016917066097393966,
      "grad_norm": 2.0340757369995117,
      "learning_rate": 0.00046649484536082477,
      "loss": 0.4625,
      "step": 105
    },
    {
      "epoch": 0.017078181012607243,
      "grad_norm": 1.9772604703903198,
      "learning_rate": 0.00046617268041237113,
      "loss": 0.3058,
      "step": 106
    },
    {
      "epoch": 0.017239295927820517,
      "grad_norm": 1.0351489782333374,
      "learning_rate": 0.00046585051546391756,
      "loss": 0.313,
      "step": 107
    },
    {
      "epoch": 0.017400410843033794,
      "grad_norm": 1.4366508722305298,
      "learning_rate": 0.0004655283505154639,
      "loss": 0.6543,
      "step": 108
    },
    {
      "epoch": 0.01756152575824707,
      "grad_norm": 2.2373621463775635,
      "learning_rate": 0.00046520618556701034,
      "loss": 0.4778,
      "step": 109
    },
    {
      "epoch": 0.017722640673460344,
      "grad_norm": 1.803508996963501,
      "learning_rate": 0.0004648840206185567,
      "loss": 0.4661,
      "step": 110
    },
    {
      "epoch": 0.01788375558867362,
      "grad_norm": 3.0159971714019775,
      "learning_rate": 0.00046456185567010313,
      "loss": 0.4743,
      "step": 111
    },
    {
      "epoch": 0.0180448705038869,
      "grad_norm": 2.8996522426605225,
      "learning_rate": 0.0004642396907216495,
      "loss": 0.4871,
      "step": 112
    },
    {
      "epoch": 0.018205985419100172,
      "grad_norm": 2.5207226276397705,
      "learning_rate": 0.0004639175257731959,
      "loss": 0.4893,
      "step": 113
    },
    {
      "epoch": 0.01836710033431345,
      "grad_norm": 2.1001224517822266,
      "learning_rate": 0.0004635953608247423,
      "loss": 0.3883,
      "step": 114
    },
    {
      "epoch": 0.018528215249526726,
      "grad_norm": 2.3337814807891846,
      "learning_rate": 0.0004632731958762887,
      "loss": 0.4411,
      "step": 115
    },
    {
      "epoch": 0.01868933016474,
      "grad_norm": 2.3541128635406494,
      "learning_rate": 0.0004629510309278351,
      "loss": 0.3959,
      "step": 116
    },
    {
      "epoch": 0.018850445079953277,
      "grad_norm": 2.384458065032959,
      "learning_rate": 0.0004626288659793815,
      "loss": 0.4198,
      "step": 117
    },
    {
      "epoch": 0.019011559995166554,
      "grad_norm": 1.3779234886169434,
      "learning_rate": 0.00046230670103092786,
      "loss": 0.4923,
      "step": 118
    },
    {
      "epoch": 0.019172674910379828,
      "grad_norm": 1.6099841594696045,
      "learning_rate": 0.0004619845360824743,
      "loss": 0.3962,
      "step": 119
    },
    {
      "epoch": 0.019333789825593105,
      "grad_norm": 1.3136175870895386,
      "learning_rate": 0.00046166237113402065,
      "loss": 0.4597,
      "step": 120
    },
    {
      "epoch": 0.01949490474080638,
      "grad_norm": 1.85826575756073,
      "learning_rate": 0.00046134020618556707,
      "loss": 0.3437,
      "step": 121
    },
    {
      "epoch": 0.019656019656019656,
      "grad_norm": 2.1084625720977783,
      "learning_rate": 0.00046101804123711344,
      "loss": 0.417,
      "step": 122
    },
    {
      "epoch": 0.019817134571232933,
      "grad_norm": 1.7659131288528442,
      "learning_rate": 0.00046069587628865975,
      "loss": 0.453,
      "step": 123
    },
    {
      "epoch": 0.019978249486446206,
      "grad_norm": 1.0441848039627075,
      "learning_rate": 0.00046037371134020617,
      "loss": 0.3973,
      "step": 124
    },
    {
      "epoch": 0.020139364401659483,
      "grad_norm": 2.69222354888916,
      "learning_rate": 0.00046005154639175254,
      "loss": 0.4098,
      "step": 125
    },
    {
      "epoch": 0.02030047931687276,
      "grad_norm": 1.6188992261886597,
      "learning_rate": 0.00045972938144329896,
      "loss": 0.3461,
      "step": 126
    },
    {
      "epoch": 0.020461594232086034,
      "grad_norm": 1.9556668996810913,
      "learning_rate": 0.0004594072164948453,
      "loss": 0.4027,
      "step": 127
    },
    {
      "epoch": 0.02062270914729931,
      "grad_norm": 2.642672300338745,
      "learning_rate": 0.00045908505154639175,
      "loss": 0.4306,
      "step": 128
    },
    {
      "epoch": 0.020783824062512588,
      "grad_norm": 3.110391855239868,
      "learning_rate": 0.0004587628865979381,
      "loss": 0.4476,
      "step": 129
    },
    {
      "epoch": 0.020944938977725862,
      "grad_norm": 2.7028653621673584,
      "learning_rate": 0.00045844072164948454,
      "loss": 0.4204,
      "step": 130
    },
    {
      "epoch": 0.02110605389293914,
      "grad_norm": 1.8069080114364624,
      "learning_rate": 0.0004581185567010309,
      "loss": 0.4168,
      "step": 131
    },
    {
      "epoch": 0.021267168808152416,
      "grad_norm": 2.3621301651000977,
      "learning_rate": 0.0004577963917525773,
      "loss": 0.329,
      "step": 132
    },
    {
      "epoch": 0.02142828372336569,
      "grad_norm": 1.019182801246643,
      "learning_rate": 0.0004574742268041237,
      "loss": 0.2495,
      "step": 133
    },
    {
      "epoch": 0.021589398638578967,
      "grad_norm": 1.5158065557479858,
      "learning_rate": 0.0004571520618556701,
      "loss": 0.4053,
      "step": 134
    },
    {
      "epoch": 0.021750513553792244,
      "grad_norm": 2.5905885696411133,
      "learning_rate": 0.0004568298969072165,
      "loss": 0.4973,
      "step": 135
    },
    {
      "epoch": 0.021911628469005517,
      "grad_norm": 3.4895589351654053,
      "learning_rate": 0.0004565077319587629,
      "loss": 0.4379,
      "step": 136
    },
    {
      "epoch": 0.022072743384218795,
      "grad_norm": 2.3189949989318848,
      "learning_rate": 0.00045618556701030927,
      "loss": 0.4431,
      "step": 137
    },
    {
      "epoch": 0.02223385829943207,
      "grad_norm": 2.2412307262420654,
      "learning_rate": 0.0004558634020618557,
      "loss": 0.4359,
      "step": 138
    },
    {
      "epoch": 0.022394973214645345,
      "grad_norm": 1.7989705801010132,
      "learning_rate": 0.00045554123711340205,
      "loss": 0.3169,
      "step": 139
    },
    {
      "epoch": 0.022556088129858622,
      "grad_norm": 1.5596919059753418,
      "learning_rate": 0.0004552190721649485,
      "loss": 0.4426,
      "step": 140
    },
    {
      "epoch": 0.022717203045071896,
      "grad_norm": 2.428297758102417,
      "learning_rate": 0.00045489690721649484,
      "loss": 0.3829,
      "step": 141
    },
    {
      "epoch": 0.022878317960285173,
      "grad_norm": 2.5673632621765137,
      "learning_rate": 0.00045457474226804126,
      "loss": 0.378,
      "step": 142
    },
    {
      "epoch": 0.02303943287549845,
      "grad_norm": 1.929572582244873,
      "learning_rate": 0.00045425257731958763,
      "loss": 0.4321,
      "step": 143
    },
    {
      "epoch": 0.023200547790711724,
      "grad_norm": 2.468630790710449,
      "learning_rate": 0.00045393041237113405,
      "loss": 0.4748,
      "step": 144
    },
    {
      "epoch": 0.023361662705925,
      "grad_norm": 1.8782981634140015,
      "learning_rate": 0.0004536082474226804,
      "loss": 0.421,
      "step": 145
    },
    {
      "epoch": 0.023522777621138278,
      "grad_norm": 2.4912774562835693,
      "learning_rate": 0.00045328608247422684,
      "loss": 0.3881,
      "step": 146
    },
    {
      "epoch": 0.02368389253635155,
      "grad_norm": 2.2990801334381104,
      "learning_rate": 0.0004529639175257732,
      "loss": 0.3865,
      "step": 147
    },
    {
      "epoch": 0.02384500745156483,
      "grad_norm": 2.372972011566162,
      "learning_rate": 0.0004526417525773196,
      "loss": 0.4114,
      "step": 148
    },
    {
      "epoch": 0.024006122366778106,
      "grad_norm": 1.4536192417144775,
      "learning_rate": 0.000452319587628866,
      "loss": 0.3721,
      "step": 149
    },
    {
      "epoch": 0.02416723728199138,
      "grad_norm": 1.5894724130630493,
      "learning_rate": 0.00045199742268041236,
      "loss": 0.3078,
      "step": 150
    },
    {
      "epoch": 0.024328352197204656,
      "grad_norm": 1.591138482093811,
      "learning_rate": 0.0004516752577319588,
      "loss": 0.3835,
      "step": 151
    },
    {
      "epoch": 0.024489467112417933,
      "grad_norm": 1.8229172229766846,
      "learning_rate": 0.00045135309278350515,
      "loss": 0.348,
      "step": 152
    },
    {
      "epoch": 0.024650582027631207,
      "grad_norm": 2.075530529022217,
      "learning_rate": 0.00045103092783505157,
      "loss": 0.3795,
      "step": 153
    },
    {
      "epoch": 0.024811696942844484,
      "grad_norm": 2.1827526092529297,
      "learning_rate": 0.00045070876288659794,
      "loss": 0.4113,
      "step": 154
    },
    {
      "epoch": 0.02497281185805776,
      "grad_norm": 2.4091615676879883,
      "learning_rate": 0.00045038659793814436,
      "loss": 0.4532,
      "step": 155
    },
    {
      "epoch": 0.025133926773271035,
      "grad_norm": 2.5817174911499023,
      "learning_rate": 0.0004500644329896907,
      "loss": 0.381,
      "step": 156
    },
    {
      "epoch": 0.025295041688484312,
      "grad_norm": 2.583249092102051,
      "learning_rate": 0.00044974226804123715,
      "loss": 0.4555,
      "step": 157
    },
    {
      "epoch": 0.025456156603697586,
      "grad_norm": 2.0131170749664307,
      "learning_rate": 0.0004494201030927835,
      "loss": 0.4564,
      "step": 158
    },
    {
      "epoch": 0.025617271518910863,
      "grad_norm": 1.7747182846069336,
      "learning_rate": 0.00044909793814432993,
      "loss": 0.3437,
      "step": 159
    },
    {
      "epoch": 0.02577838643412414,
      "grad_norm": 1.9177372455596924,
      "learning_rate": 0.0004487757731958763,
      "loss": 0.4078,
      "step": 160
    },
    {
      "epoch": 0.025939501349337413,
      "grad_norm": 2.317671775817871,
      "learning_rate": 0.0004484536082474227,
      "loss": 0.3176,
      "step": 161
    },
    {
      "epoch": 0.02610061626455069,
      "grad_norm": 2.063570976257324,
      "learning_rate": 0.0004481314432989691,
      "loss": 0.4126,
      "step": 162
    },
    {
      "epoch": 0.026261731179763968,
      "grad_norm": 2.3399829864501953,
      "learning_rate": 0.0004478092783505155,
      "loss": 0.4223,
      "step": 163
    },
    {
      "epoch": 0.02642284609497724,
      "grad_norm": 2.6195907592773438,
      "learning_rate": 0.0004474871134020619,
      "loss": 0.4739,
      "step": 164
    },
    {
      "epoch": 0.02658396101019052,
      "grad_norm": 1.5408908128738403,
      "learning_rate": 0.0004471649484536083,
      "loss": 0.3082,
      "step": 165
    },
    {
      "epoch": 0.026745075925403795,
      "grad_norm": 2.2052369117736816,
      "learning_rate": 0.00044684278350515466,
      "loss": 0.3261,
      "step": 166
    },
    {
      "epoch": 0.02690619084061707,
      "grad_norm": 2.424006700515747,
      "learning_rate": 0.0004465206185567011,
      "loss": 0.354,
      "step": 167
    },
    {
      "epoch": 0.027067305755830346,
      "grad_norm": 2.539599657058716,
      "learning_rate": 0.00044619845360824745,
      "loss": 0.4871,
      "step": 168
    },
    {
      "epoch": 0.027228420671043623,
      "grad_norm": 3.1662776470184326,
      "learning_rate": 0.0004458762886597939,
      "loss": 0.3255,
      "step": 169
    },
    {
      "epoch": 0.027389535586256897,
      "grad_norm": 2.0717270374298096,
      "learning_rate": 0.00044555412371134024,
      "loss": 0.4015,
      "step": 170
    },
    {
      "epoch": 0.027550650501470174,
      "grad_norm": 2.1980762481689453,
      "learning_rate": 0.00044523195876288655,
      "loss": 0.3696,
      "step": 171
    },
    {
      "epoch": 0.02771176541668345,
      "grad_norm": 4.086962699890137,
      "learning_rate": 0.000444909793814433,
      "loss": 0.4488,
      "step": 172
    },
    {
      "epoch": 0.027872880331896725,
      "grad_norm": 2.943358898162842,
      "learning_rate": 0.00044458762886597934,
      "loss": 0.4779,
      "step": 173
    },
    {
      "epoch": 0.02803399524711,
      "grad_norm": 2.5102827548980713,
      "learning_rate": 0.00044426546391752576,
      "loss": 0.3931,
      "step": 174
    },
    {
      "epoch": 0.02819511016232328,
      "grad_norm": 3.188279867172241,
      "learning_rate": 0.00044394329896907213,
      "loss": 0.3974,
      "step": 175
    },
    {
      "epoch": 0.028356225077536552,
      "grad_norm": 2.7477943897247314,
      "learning_rate": 0.00044362113402061855,
      "loss": 0.4265,
      "step": 176
    },
    {
      "epoch": 0.02851733999274983,
      "grad_norm": 1.9073768854141235,
      "learning_rate": 0.0004432989690721649,
      "loss": 0.3818,
      "step": 177
    },
    {
      "epoch": 0.028678454907963103,
      "grad_norm": 1.9675066471099854,
      "learning_rate": 0.00044297680412371134,
      "loss": 0.4392,
      "step": 178
    },
    {
      "epoch": 0.02883956982317638,
      "grad_norm": 1.8464733362197876,
      "learning_rate": 0.0004426546391752577,
      "loss": 0.3736,
      "step": 179
    },
    {
      "epoch": 0.029000684738389657,
      "grad_norm": 1.4760245084762573,
      "learning_rate": 0.0004423324742268041,
      "loss": 0.2822,
      "step": 180
    },
    {
      "epoch": 0.02916179965360293,
      "grad_norm": 2.1594419479370117,
      "learning_rate": 0.0004420103092783505,
      "loss": 0.3876,
      "step": 181
    },
    {
      "epoch": 0.029322914568816208,
      "grad_norm": 1.791157841682434,
      "learning_rate": 0.0004416881443298969,
      "loss": 0.4001,
      "step": 182
    },
    {
      "epoch": 0.029484029484029485,
      "grad_norm": 1.4637248516082764,
      "learning_rate": 0.0004413659793814433,
      "loss": 0.3324,
      "step": 183
    },
    {
      "epoch": 0.02964514439924276,
      "grad_norm": 1.7769125699996948,
      "learning_rate": 0.0004410438144329897,
      "loss": 0.3231,
      "step": 184
    },
    {
      "epoch": 0.029806259314456036,
      "grad_norm": 2.109351396560669,
      "learning_rate": 0.00044072164948453607,
      "loss": 0.3625,
      "step": 185
    },
    {
      "epoch": 0.029967374229669313,
      "grad_norm": 2.335125207901001,
      "learning_rate": 0.0004403994845360825,
      "loss": 0.4393,
      "step": 186
    },
    {
      "epoch": 0.030128489144882586,
      "grad_norm": 3.1332011222839355,
      "learning_rate": 0.00044007731958762886,
      "loss": 0.3762,
      "step": 187
    },
    {
      "epoch": 0.030289604060095864,
      "grad_norm": 3.4230706691741943,
      "learning_rate": 0.0004397551546391753,
      "loss": 0.4355,
      "step": 188
    },
    {
      "epoch": 0.03045071897530914,
      "grad_norm": 1.8456496000289917,
      "learning_rate": 0.00043943298969072165,
      "loss": 0.2844,
      "step": 189
    },
    {
      "epoch": 0.030611833890522414,
      "grad_norm": 2.2673258781433105,
      "learning_rate": 0.00043911082474226807,
      "loss": 0.3361,
      "step": 190
    },
    {
      "epoch": 0.03077294880573569,
      "grad_norm": 2.465858221054077,
      "learning_rate": 0.00043878865979381443,
      "loss": 0.3555,
      "step": 191
    },
    {
      "epoch": 0.03093406372094897,
      "grad_norm": 1.605825662612915,
      "learning_rate": 0.00043846649484536085,
      "loss": 0.2838,
      "step": 192
    },
    {
      "epoch": 0.031095178636162242,
      "grad_norm": 2.4753220081329346,
      "learning_rate": 0.0004381443298969072,
      "loss": 0.3716,
      "step": 193
    },
    {
      "epoch": 0.03125629355137552,
      "grad_norm": 1.292529821395874,
      "learning_rate": 0.00043782216494845364,
      "loss": 0.3651,
      "step": 194
    },
    {
      "epoch": 0.031417408466588796,
      "grad_norm": 1.845875859260559,
      "learning_rate": 0.0004375,
      "loss": 0.3281,
      "step": 195
    },
    {
      "epoch": 0.03157852338180207,
      "grad_norm": 1.8217531442642212,
      "learning_rate": 0.0004371778350515464,
      "loss": 0.2987,
      "step": 196
    },
    {
      "epoch": 0.03173963829701534,
      "grad_norm": 1.7723309993743896,
      "learning_rate": 0.0004368556701030928,
      "loss": 0.3355,
      "step": 197
    },
    {
      "epoch": 0.03190075321222862,
      "grad_norm": 2.1144561767578125,
      "learning_rate": 0.00043653350515463916,
      "loss": 0.3675,
      "step": 198
    },
    {
      "epoch": 0.0320618681274419,
      "grad_norm": 2.6268959045410156,
      "learning_rate": 0.0004362113402061856,
      "loss": 0.4111,
      "step": 199
    },
    {
      "epoch": 0.032222983042655175,
      "grad_norm": 2.7868781089782715,
      "learning_rate": 0.00043588917525773195,
      "loss": 0.3634,
      "step": 200
    },
    {
      "epoch": 0.03238409795786845,
      "grad_norm": 2.801619291305542,
      "learning_rate": 0.0004355670103092784,
      "loss": 0.4164,
      "step": 201
    },
    {
      "epoch": 0.03254521287308173,
      "grad_norm": 1.961910605430603,
      "learning_rate": 0.00043524484536082474,
      "loss": 0.266,
      "step": 202
    },
    {
      "epoch": 0.032706327788295,
      "grad_norm": 1.4713094234466553,
      "learning_rate": 0.00043492268041237116,
      "loss": 0.225,
      "step": 203
    },
    {
      "epoch": 0.032867442703508276,
      "grad_norm": 2.0150558948516846,
      "learning_rate": 0.00043460051546391753,
      "loss": 0.2953,
      "step": 204
    },
    {
      "epoch": 0.03302855761872155,
      "grad_norm": 3.631696939468384,
      "learning_rate": 0.00043427835051546395,
      "loss": 0.4851,
      "step": 205
    },
    {
      "epoch": 0.03318967253393483,
      "grad_norm": 2.6033661365509033,
      "learning_rate": 0.0004339561855670103,
      "loss": 0.4222,
      "step": 206
    },
    {
      "epoch": 0.03335078744914811,
      "grad_norm": 2.59287691116333,
      "learning_rate": 0.00043363402061855674,
      "loss": 0.2636,
      "step": 207
    },
    {
      "epoch": 0.03351190236436138,
      "grad_norm": 2.799741506576538,
      "learning_rate": 0.0004333118556701031,
      "loss": 0.3338,
      "step": 208
    },
    {
      "epoch": 0.033673017279574655,
      "grad_norm": 2.099804401397705,
      "learning_rate": 0.0004329896907216495,
      "loss": 0.3076,
      "step": 209
    },
    {
      "epoch": 0.03383413219478793,
      "grad_norm": 3.2113230228424072,
      "learning_rate": 0.0004326675257731959,
      "loss": 0.334,
      "step": 210
    },
    {
      "epoch": 0.03399524711000121,
      "grad_norm": 2.4629364013671875,
      "learning_rate": 0.0004323453608247423,
      "loss": 0.3705,
      "step": 211
    },
    {
      "epoch": 0.034156362025214486,
      "grad_norm": 2.398482322692871,
      "learning_rate": 0.0004320231958762887,
      "loss": 0.3866,
      "step": 212
    },
    {
      "epoch": 0.03431747694042776,
      "grad_norm": 2.3077540397644043,
      "learning_rate": 0.0004317010309278351,
      "loss": 0.3557,
      "step": 213
    },
    {
      "epoch": 0.03447859185564103,
      "grad_norm": 2.374601125717163,
      "learning_rate": 0.00043137886597938147,
      "loss": 0.2838,
      "step": 214
    },
    {
      "epoch": 0.03463970677085431,
      "grad_norm": 2.1135027408599854,
      "learning_rate": 0.0004310567010309279,
      "loss": 0.3631,
      "step": 215
    },
    {
      "epoch": 0.03480082168606759,
      "grad_norm": 1.0701348781585693,
      "learning_rate": 0.00043073453608247426,
      "loss": 0.2199,
      "step": 216
    },
    {
      "epoch": 0.034961936601280864,
      "grad_norm": 2.3332102298736572,
      "learning_rate": 0.0004304123711340207,
      "loss": 0.3241,
      "step": 217
    },
    {
      "epoch": 0.03512305151649414,
      "grad_norm": 2.117387056350708,
      "learning_rate": 0.00043009020618556704,
      "loss": 0.297,
      "step": 218
    },
    {
      "epoch": 0.03528416643170742,
      "grad_norm": 2.5959103107452393,
      "learning_rate": 0.00042976804123711346,
      "loss": 0.3852,
      "step": 219
    },
    {
      "epoch": 0.03544528134692069,
      "grad_norm": 1.7355772256851196,
      "learning_rate": 0.0004294458762886598,
      "loss": 0.3228,
      "step": 220
    },
    {
      "epoch": 0.035606396262133966,
      "grad_norm": 1.8736622333526611,
      "learning_rate": 0.00042912371134020614,
      "loss": 0.3869,
      "step": 221
    },
    {
      "epoch": 0.03576751117734724,
      "grad_norm": 1.8214669227600098,
      "learning_rate": 0.00042880154639175257,
      "loss": 0.2589,
      "step": 222
    },
    {
      "epoch": 0.03592862609256052,
      "grad_norm": 1.4349334239959717,
      "learning_rate": 0.00042847938144329893,
      "loss": 0.2841,
      "step": 223
    },
    {
      "epoch": 0.0360897410077738,
      "grad_norm": 1.7163299322128296,
      "learning_rate": 0.00042815721649484535,
      "loss": 0.2737,
      "step": 224
    },
    {
      "epoch": 0.03625085592298707,
      "grad_norm": 2.964803457260132,
      "learning_rate": 0.0004278350515463917,
      "loss": 0.3567,
      "step": 225
    },
    {
      "epoch": 0.036411970838200344,
      "grad_norm": 1.9769994020462036,
      "learning_rate": 0.00042751288659793814,
      "loss": 0.3563,
      "step": 226
    },
    {
      "epoch": 0.03657308575341362,
      "grad_norm": 2.5904126167297363,
      "learning_rate": 0.0004271907216494845,
      "loss": 0.3662,
      "step": 227
    },
    {
      "epoch": 0.0367342006686269,
      "grad_norm": 2.434934616088867,
      "learning_rate": 0.00042686855670103093,
      "loss": 0.3003,
      "step": 228
    },
    {
      "epoch": 0.036895315583840176,
      "grad_norm": 2.3788416385650635,
      "learning_rate": 0.0004265463917525773,
      "loss": 0.3204,
      "step": 229
    },
    {
      "epoch": 0.03705643049905345,
      "grad_norm": 2.243389129638672,
      "learning_rate": 0.0004262242268041237,
      "loss": 0.3555,
      "step": 230
    },
    {
      "epoch": 0.03721754541426672,
      "grad_norm": 2.871736764907837,
      "learning_rate": 0.0004259020618556701,
      "loss": 0.3217,
      "step": 231
    },
    {
      "epoch": 0.03737866032948,
      "grad_norm": 2.606398105621338,
      "learning_rate": 0.0004255798969072165,
      "loss": 0.3263,
      "step": 232
    },
    {
      "epoch": 0.03753977524469328,
      "grad_norm": 1.9069278240203857,
      "learning_rate": 0.00042525773195876287,
      "loss": 0.188,
      "step": 233
    },
    {
      "epoch": 0.037700890159906554,
      "grad_norm": 2.646174192428589,
      "learning_rate": 0.0004249355670103093,
      "loss": 0.4015,
      "step": 234
    },
    {
      "epoch": 0.03786200507511983,
      "grad_norm": 3.088085412979126,
      "learning_rate": 0.00042461340206185566,
      "loss": 0.4045,
      "step": 235
    },
    {
      "epoch": 0.03802311999033311,
      "grad_norm": 2.239553213119507,
      "learning_rate": 0.0004242912371134021,
      "loss": 0.3767,
      "step": 236
    },
    {
      "epoch": 0.03818423490554638,
      "grad_norm": 2.31286358833313,
      "learning_rate": 0.00042396907216494845,
      "loss": 0.2939,
      "step": 237
    },
    {
      "epoch": 0.038345349820759655,
      "grad_norm": 1.8142653703689575,
      "learning_rate": 0.00042364690721649487,
      "loss": 0.2678,
      "step": 238
    },
    {
      "epoch": 0.03850646473597293,
      "grad_norm": 4.2191925048828125,
      "learning_rate": 0.00042332474226804124,
      "loss": 0.3789,
      "step": 239
    },
    {
      "epoch": 0.03866757965118621,
      "grad_norm": 1.926149606704712,
      "learning_rate": 0.00042300257731958766,
      "loss": 0.2352,
      "step": 240
    },
    {
      "epoch": 0.03882869456639949,
      "grad_norm": 2.258925199508667,
      "learning_rate": 0.000422680412371134,
      "loss": 0.3695,
      "step": 241
    },
    {
      "epoch": 0.03898980948161276,
      "grad_norm": 2.579336643218994,
      "learning_rate": 0.00042235824742268044,
      "loss": 0.3444,
      "step": 242
    },
    {
      "epoch": 0.039150924396826034,
      "grad_norm": 1.9528363943099976,
      "learning_rate": 0.0004220360824742268,
      "loss": 0.2973,
      "step": 243
    },
    {
      "epoch": 0.03931203931203931,
      "grad_norm": 2.323676347732544,
      "learning_rate": 0.0004217139175257732,
      "loss": 0.4438,
      "step": 244
    },
    {
      "epoch": 0.03947315422725259,
      "grad_norm": 1.2929701805114746,
      "learning_rate": 0.0004213917525773196,
      "loss": 0.2855,
      "step": 245
    },
    {
      "epoch": 0.039634269142465865,
      "grad_norm": 1.8088250160217285,
      "learning_rate": 0.00042106958762886597,
      "loss": 0.2871,
      "step": 246
    },
    {
      "epoch": 0.03979538405767914,
      "grad_norm": 1.3090132474899292,
      "learning_rate": 0.0004207474226804124,
      "loss": 0.3031,
      "step": 247
    },
    {
      "epoch": 0.03995649897289241,
      "grad_norm": 1.8489693403244019,
      "learning_rate": 0.00042042525773195875,
      "loss": 0.2713,
      "step": 248
    },
    {
      "epoch": 0.04011761388810569,
      "grad_norm": 2.597301721572876,
      "learning_rate": 0.0004201030927835052,
      "loss": 0.3105,
      "step": 249
    },
    {
      "epoch": 0.04027872880331897,
      "grad_norm": 2.0358221530914307,
      "learning_rate": 0.00041978092783505154,
      "loss": 0.3644,
      "step": 250
    },
    {
      "epoch": 0.040439843718532244,
      "grad_norm": 2.352053165435791,
      "learning_rate": 0.00041945876288659796,
      "loss": 0.3575,
      "step": 251
    },
    {
      "epoch": 0.04060095863374552,
      "grad_norm": 2.217918872833252,
      "learning_rate": 0.00041913659793814433,
      "loss": 0.3171,
      "step": 252
    },
    {
      "epoch": 0.0407620735489588,
      "grad_norm": 1.6496376991271973,
      "learning_rate": 0.00041881443298969075,
      "loss": 0.2381,
      "step": 253
    },
    {
      "epoch": 0.04092318846417207,
      "grad_norm": 1.964562177658081,
      "learning_rate": 0.0004184922680412371,
      "loss": 0.2698,
      "step": 254
    },
    {
      "epoch": 0.041084303379385345,
      "grad_norm": 2.068683385848999,
      "learning_rate": 0.00041817010309278354,
      "loss": 0.3324,
      "step": 255
    },
    {
      "epoch": 0.04124541829459862,
      "grad_norm": 1.7732609510421753,
      "learning_rate": 0.0004178479381443299,
      "loss": 0.3048,
      "step": 256
    },
    {
      "epoch": 0.0414065332098119,
      "grad_norm": 2.6272454261779785,
      "learning_rate": 0.00041752577319587633,
      "loss": 0.3501,
      "step": 257
    },
    {
      "epoch": 0.041567648125025176,
      "grad_norm": 1.8076558113098145,
      "learning_rate": 0.0004172036082474227,
      "loss": 0.2549,
      "step": 258
    },
    {
      "epoch": 0.041728763040238454,
      "grad_norm": 1.8654621839523315,
      "learning_rate": 0.0004168814432989691,
      "loss": 0.2871,
      "step": 259
    },
    {
      "epoch": 0.041889877955451724,
      "grad_norm": 2.1548855304718018,
      "learning_rate": 0.0004165592783505155,
      "loss": 0.3982,
      "step": 260
    },
    {
      "epoch": 0.042050992870665,
      "grad_norm": 2.195256471633911,
      "learning_rate": 0.0004162371134020619,
      "loss": 0.3493,
      "step": 261
    },
    {
      "epoch": 0.04221210778587828,
      "grad_norm": 2.0532870292663574,
      "learning_rate": 0.00041591494845360827,
      "loss": 0.2379,
      "step": 262
    },
    {
      "epoch": 0.042373222701091555,
      "grad_norm": 2.4485604763031006,
      "learning_rate": 0.0004155927835051547,
      "loss": 0.2747,
      "step": 263
    },
    {
      "epoch": 0.04253433761630483,
      "grad_norm": 3.400388479232788,
      "learning_rate": 0.00041527061855670106,
      "loss": 0.3947,
      "step": 264
    },
    {
      "epoch": 0.0426954525315181,
      "grad_norm": 1.9743298292160034,
      "learning_rate": 0.0004149484536082475,
      "loss": 0.2575,
      "step": 265
    },
    {
      "epoch": 0.04285656744673138,
      "grad_norm": 1.6639984846115112,
      "learning_rate": 0.00041462628865979385,
      "loss": 0.2933,
      "step": 266
    },
    {
      "epoch": 0.043017682361944656,
      "grad_norm": 1.6418489217758179,
      "learning_rate": 0.00041430412371134027,
      "loss": 0.2918,
      "step": 267
    },
    {
      "epoch": 0.04317879727715793,
      "grad_norm": 1.9238808155059814,
      "learning_rate": 0.0004139819587628866,
      "loss": 0.2737,
      "step": 268
    },
    {
      "epoch": 0.04333991219237121,
      "grad_norm": 1.6459296941757202,
      "learning_rate": 0.00041365979381443295,
      "loss": 0.2779,
      "step": 269
    },
    {
      "epoch": 0.04350102710758449,
      "grad_norm": 1.6172802448272705,
      "learning_rate": 0.00041333762886597937,
      "loss": 0.2656,
      "step": 270
    },
    {
      "epoch": 0.04366214202279776,
      "grad_norm": 1.8684957027435303,
      "learning_rate": 0.00041301546391752573,
      "loss": 0.3161,
      "step": 271
    },
    {
      "epoch": 0.043823256938011035,
      "grad_norm": 2.0918867588043213,
      "learning_rate": 0.00041269329896907216,
      "loss": 0.2681,
      "step": 272
    },
    {
      "epoch": 0.04398437185322431,
      "grad_norm": 2.6169748306274414,
      "learning_rate": 0.0004123711340206185,
      "loss": 0.3522,
      "step": 273
    },
    {
      "epoch": 0.04414548676843759,
      "grad_norm": 1.8779349327087402,
      "learning_rate": 0.00041204896907216494,
      "loss": 0.3074,
      "step": 274
    },
    {
      "epoch": 0.044306601683650866,
      "grad_norm": 2.6420602798461914,
      "learning_rate": 0.0004117268041237113,
      "loss": 0.2177,
      "step": 275
    },
    {
      "epoch": 0.04446771659886414,
      "grad_norm": 2.4739067554473877,
      "learning_rate": 0.00041140463917525773,
      "loss": 0.3827,
      "step": 276
    },
    {
      "epoch": 0.04462883151407741,
      "grad_norm": 2.2683496475219727,
      "learning_rate": 0.0004110824742268041,
      "loss": 0.2807,
      "step": 277
    },
    {
      "epoch": 0.04478994642929069,
      "grad_norm": 2.749033212661743,
      "learning_rate": 0.0004107603092783505,
      "loss": 0.2532,
      "step": 278
    },
    {
      "epoch": 0.04495106134450397,
      "grad_norm": 1.958237886428833,
      "learning_rate": 0.0004104381443298969,
      "loss": 0.2656,
      "step": 279
    },
    {
      "epoch": 0.045112176259717245,
      "grad_norm": 2.7049436569213867,
      "learning_rate": 0.0004101159793814433,
      "loss": 0.2761,
      "step": 280
    },
    {
      "epoch": 0.04527329117493052,
      "grad_norm": 1.7615808248519897,
      "learning_rate": 0.0004097938144329897,
      "loss": 0.2153,
      "step": 281
    },
    {
      "epoch": 0.04543440609014379,
      "grad_norm": 2.0560638904571533,
      "learning_rate": 0.0004094716494845361,
      "loss": 0.2502,
      "step": 282
    },
    {
      "epoch": 0.04559552100535707,
      "grad_norm": 1.9380133152008057,
      "learning_rate": 0.00040914948453608246,
      "loss": 0.2325,
      "step": 283
    },
    {
      "epoch": 0.045756635920570346,
      "grad_norm": 2.7455320358276367,
      "learning_rate": 0.0004088273195876289,
      "loss": 0.296,
      "step": 284
    },
    {
      "epoch": 0.04591775083578362,
      "grad_norm": 4.029857158660889,
      "learning_rate": 0.00040850515463917525,
      "loss": 0.3535,
      "step": 285
    },
    {
      "epoch": 0.0460788657509969,
      "grad_norm": 3.3770601749420166,
      "learning_rate": 0.00040818298969072167,
      "loss": 0.3016,
      "step": 286
    },
    {
      "epoch": 0.04623998066621018,
      "grad_norm": 2.077756881713867,
      "learning_rate": 0.00040786082474226804,
      "loss": 0.2429,
      "step": 287
    },
    {
      "epoch": 0.04640109558142345,
      "grad_norm": 2.3157763481140137,
      "learning_rate": 0.00040753865979381446,
      "loss": 0.2689,
      "step": 288
    },
    {
      "epoch": 0.046562210496636725,
      "grad_norm": 4.171531677246094,
      "learning_rate": 0.0004072164948453608,
      "loss": 0.2539,
      "step": 289
    },
    {
      "epoch": 0.04672332541185,
      "grad_norm": 2.2045881748199463,
      "learning_rate": 0.00040689432989690725,
      "loss": 0.2986,
      "step": 290
    },
    {
      "epoch": 0.04688444032706328,
      "grad_norm": 1.829737663269043,
      "learning_rate": 0.0004065721649484536,
      "loss": 0.1861,
      "step": 291
    },
    {
      "epoch": 0.047045555242276556,
      "grad_norm": 2.995302200317383,
      "learning_rate": 0.00040625000000000004,
      "loss": 0.2985,
      "step": 292
    },
    {
      "epoch": 0.04720667015748983,
      "grad_norm": 2.651905059814453,
      "learning_rate": 0.0004059278350515464,
      "loss": 0.334,
      "step": 293
    },
    {
      "epoch": 0.0473677850727031,
      "grad_norm": 2.6098103523254395,
      "learning_rate": 0.00040560567010309277,
      "loss": 0.2424,
      "step": 294
    },
    {
      "epoch": 0.04752889998791638,
      "grad_norm": 2.2342374324798584,
      "learning_rate": 0.0004052835051546392,
      "loss": 0.2486,
      "step": 295
    },
    {
      "epoch": 0.04769001490312966,
      "grad_norm": 1.675087571144104,
      "learning_rate": 0.00040496134020618556,
      "loss": 0.2716,
      "step": 296
    },
    {
      "epoch": 0.047851129818342934,
      "grad_norm": 2.887840747833252,
      "learning_rate": 0.000404639175257732,
      "loss": 0.3343,
      "step": 297
    },
    {
      "epoch": 0.04801224473355621,
      "grad_norm": 1.9715698957443237,
      "learning_rate": 0.00040431701030927835,
      "loss": 0.3244,
      "step": 298
    },
    {
      "epoch": 0.04817335964876948,
      "grad_norm": 2.6509954929351807,
      "learning_rate": 0.00040399484536082477,
      "loss": 0.3214,
      "step": 299
    },
    {
      "epoch": 0.04833447456398276,
      "grad_norm": 2.6757850646972656,
      "learning_rate": 0.00040367268041237113,
      "loss": 0.2549,
      "step": 300
    },
    {
      "epoch": 0.048495589479196036,
      "grad_norm": 1.775062918663025,
      "learning_rate": 0.00040335051546391755,
      "loss": 0.2639,
      "step": 301
    },
    {
      "epoch": 0.04865670439440931,
      "grad_norm": 1.955620288848877,
      "learning_rate": 0.0004030283505154639,
      "loss": 0.2486,
      "step": 302
    },
    {
      "epoch": 0.04881781930962259,
      "grad_norm": 1.6990914344787598,
      "learning_rate": 0.00040270618556701034,
      "loss": 0.267,
      "step": 303
    },
    {
      "epoch": 0.04897893422483587,
      "grad_norm": 2.8306026458740234,
      "learning_rate": 0.0004023840206185567,
      "loss": 0.296,
      "step": 304
    },
    {
      "epoch": 0.04914004914004914,
      "grad_norm": 1.7846667766571045,
      "learning_rate": 0.00040206185567010313,
      "loss": 0.1896,
      "step": 305
    },
    {
      "epoch": 0.049301164055262414,
      "grad_norm": 3.0062906742095947,
      "learning_rate": 0.0004017396907216495,
      "loss": 0.4143,
      "step": 306
    },
    {
      "epoch": 0.04946227897047569,
      "grad_norm": 1.759640097618103,
      "learning_rate": 0.0004014175257731959,
      "loss": 0.2538,
      "step": 307
    },
    {
      "epoch": 0.04962339388568897,
      "grad_norm": 2.3366644382476807,
      "learning_rate": 0.0004010953608247423,
      "loss": 0.2519,
      "step": 308
    },
    {
      "epoch": 0.049784508800902245,
      "grad_norm": 2.0307388305664062,
      "learning_rate": 0.0004007731958762887,
      "loss": 0.2212,
      "step": 309
    },
    {
      "epoch": 0.04994562371611552,
      "grad_norm": 2.2969143390655518,
      "learning_rate": 0.0004004510309278351,
      "loss": 0.2488,
      "step": 310
    },
    {
      "epoch": 0.05010673863132879,
      "grad_norm": 2.232356548309326,
      "learning_rate": 0.0004001288659793815,
      "loss": 0.2727,
      "step": 311
    },
    {
      "epoch": 0.05026785354654207,
      "grad_norm": 3.2930946350097656,
      "learning_rate": 0.00039980670103092786,
      "loss": 0.2576,
      "step": 312
    },
    {
      "epoch": 0.05042896846175535,
      "grad_norm": 3.5657880306243896,
      "learning_rate": 0.0003994845360824743,
      "loss": 0.2504,
      "step": 313
    },
    {
      "epoch": 0.050590083376968624,
      "grad_norm": 2.7357184886932373,
      "learning_rate": 0.00039916237113402065,
      "loss": 0.2725,
      "step": 314
    },
    {
      "epoch": 0.0507511982921819,
      "grad_norm": 3.551011085510254,
      "learning_rate": 0.00039884020618556707,
      "loss": 0.2727,
      "step": 315
    },
    {
      "epoch": 0.05091231320739517,
      "grad_norm": 2.9261837005615234,
      "learning_rate": 0.00039851804123711344,
      "loss": 0.321,
      "step": 316
    },
    {
      "epoch": 0.05107342812260845,
      "grad_norm": 2.65415358543396,
      "learning_rate": 0.00039819587628865975,
      "loss": 0.2665,
      "step": 317
    },
    {
      "epoch": 0.051234543037821725,
      "grad_norm": 4.3561835289001465,
      "learning_rate": 0.00039787371134020617,
      "loss": 0.5464,
      "step": 318
    },
    {
      "epoch": 0.051395657953035,
      "grad_norm": 2.962968111038208,
      "learning_rate": 0.00039755154639175254,
      "loss": 0.3388,
      "step": 319
    },
    {
      "epoch": 0.05155677286824828,
      "grad_norm": 2.26025128364563,
      "learning_rate": 0.00039722938144329896,
      "loss": 0.285,
      "step": 320
    },
    {
      "epoch": 0.05171788778346156,
      "grad_norm": 1.288312315940857,
      "learning_rate": 0.0003969072164948453,
      "loss": 0.1753,
      "step": 321
    },
    {
      "epoch": 0.05187900269867483,
      "grad_norm": 2.007716417312622,
      "learning_rate": 0.00039658505154639175,
      "loss": 0.2645,
      "step": 322
    },
    {
      "epoch": 0.052040117613888104,
      "grad_norm": 2.179851531982422,
      "learning_rate": 0.0003962628865979381,
      "loss": 0.2383,
      "step": 323
    },
    {
      "epoch": 0.05220123252910138,
      "grad_norm": 2.4874820709228516,
      "learning_rate": 0.00039594072164948453,
      "loss": 0.2657,
      "step": 324
    },
    {
      "epoch": 0.05236234744431466,
      "grad_norm": 2.2492141723632812,
      "learning_rate": 0.0003956185567010309,
      "loss": 0.2748,
      "step": 325
    },
    {
      "epoch": 0.052523462359527935,
      "grad_norm": 2.7154195308685303,
      "learning_rate": 0.0003952963917525773,
      "loss": 0.2842,
      "step": 326
    },
    {
      "epoch": 0.05268457727474121,
      "grad_norm": 2.185682773590088,
      "learning_rate": 0.0003949742268041237,
      "loss": 0.2451,
      "step": 327
    },
    {
      "epoch": 0.05284569218995448,
      "grad_norm": 3.63957142829895,
      "learning_rate": 0.0003946520618556701,
      "loss": 0.3453,
      "step": 328
    },
    {
      "epoch": 0.05300680710516776,
      "grad_norm": 2.770188331604004,
      "learning_rate": 0.0003943298969072165,
      "loss": 0.2818,
      "step": 329
    },
    {
      "epoch": 0.05316792202038104,
      "grad_norm": 2.6255180835723877,
      "learning_rate": 0.0003940077319587629,
      "loss": 0.2738,
      "step": 330
    },
    {
      "epoch": 0.053329036935594314,
      "grad_norm": 1.99891996383667,
      "learning_rate": 0.00039368556701030927,
      "loss": 0.2382,
      "step": 331
    },
    {
      "epoch": 0.05349015185080759,
      "grad_norm": 1.6470037698745728,
      "learning_rate": 0.0003933634020618557,
      "loss": 0.1804,
      "step": 332
    },
    {
      "epoch": 0.05365126676602087,
      "grad_norm": 2.1313304901123047,
      "learning_rate": 0.00039304123711340205,
      "loss": 0.2148,
      "step": 333
    },
    {
      "epoch": 0.05381238168123414,
      "grad_norm": 3.1178691387176514,
      "learning_rate": 0.0003927190721649485,
      "loss": 0.2576,
      "step": 334
    },
    {
      "epoch": 0.053973496596447415,
      "grad_norm": 2.5373547077178955,
      "learning_rate": 0.00039239690721649484,
      "loss": 0.3001,
      "step": 335
    },
    {
      "epoch": 0.05413461151166069,
      "grad_norm": 2.3514938354492188,
      "learning_rate": 0.00039207474226804126,
      "loss": 0.2713,
      "step": 336
    },
    {
      "epoch": 0.05429572642687397,
      "grad_norm": 2.108391284942627,
      "learning_rate": 0.00039175257731958763,
      "loss": 0.2917,
      "step": 337
    },
    {
      "epoch": 0.054456841342087246,
      "grad_norm": 3.8088607788085938,
      "learning_rate": 0.00039143041237113405,
      "loss": 0.3449,
      "step": 338
    },
    {
      "epoch": 0.054617956257300516,
      "grad_norm": 2.008068799972534,
      "learning_rate": 0.0003911082474226804,
      "loss": 0.2326,
      "step": 339
    },
    {
      "epoch": 0.054779071172513794,
      "grad_norm": 4.187433242797852,
      "learning_rate": 0.00039078608247422684,
      "loss": 0.2234,
      "step": 340
    },
    {
      "epoch": 0.05494018608772707,
      "grad_norm": 5.016900062561035,
      "learning_rate": 0.0003904639175257732,
      "loss": 0.2979,
      "step": 341
    },
    {
      "epoch": 0.05510130100294035,
      "grad_norm": 2.5216023921966553,
      "learning_rate": 0.00039014175257731957,
      "loss": 0.2859,
      "step": 342
    },
    {
      "epoch": 0.055262415918153625,
      "grad_norm": 2.3913333415985107,
      "learning_rate": 0.000389819587628866,
      "loss": 0.3163,
      "step": 343
    },
    {
      "epoch": 0.0554235308333669,
      "grad_norm": 2.545013427734375,
      "learning_rate": 0.00038949742268041236,
      "loss": 0.3271,
      "step": 344
    },
    {
      "epoch": 0.05558464574858017,
      "grad_norm": 2.0393104553222656,
      "learning_rate": 0.0003891752577319588,
      "loss": 0.2144,
      "step": 345
    },
    {
      "epoch": 0.05574576066379345,
      "grad_norm": 2.4468066692352295,
      "learning_rate": 0.00038885309278350515,
      "loss": 0.2438,
      "step": 346
    },
    {
      "epoch": 0.055906875579006726,
      "grad_norm": 1.9110088348388672,
      "learning_rate": 0.00038853092783505157,
      "loss": 0.1509,
      "step": 347
    },
    {
      "epoch": 0.05606799049422,
      "grad_norm": 2.3863348960876465,
      "learning_rate": 0.00038820876288659794,
      "loss": 0.3121,
      "step": 348
    },
    {
      "epoch": 0.05622910540943328,
      "grad_norm": 2.3690898418426514,
      "learning_rate": 0.00038788659793814436,
      "loss": 0.2264,
      "step": 349
    },
    {
      "epoch": 0.05639022032464656,
      "grad_norm": 2.132162570953369,
      "learning_rate": 0.0003875644329896907,
      "loss": 0.2846,
      "step": 350
    },
    {
      "epoch": 0.05655133523985983,
      "grad_norm": 2.9237923622131348,
      "learning_rate": 0.00038724226804123714,
      "loss": 0.3127,
      "step": 351
    },
    {
      "epoch": 0.056712450155073105,
      "grad_norm": 1.383338451385498,
      "learning_rate": 0.0003869201030927835,
      "loss": 0.1784,
      "step": 352
    },
    {
      "epoch": 0.05687356507028638,
      "grad_norm": 2.28791880607605,
      "learning_rate": 0.00038659793814432993,
      "loss": 0.3067,
      "step": 353
    },
    {
      "epoch": 0.05703467998549966,
      "grad_norm": 2.557772397994995,
      "learning_rate": 0.0003862757731958763,
      "loss": 0.298,
      "step": 354
    },
    {
      "epoch": 0.057195794900712936,
      "grad_norm": 2.4434027671813965,
      "learning_rate": 0.0003859536082474227,
      "loss": 0.3058,
      "step": 355
    },
    {
      "epoch": 0.057356909815926206,
      "grad_norm": 2.20173978805542,
      "learning_rate": 0.0003856314432989691,
      "loss": 0.2748,
      "step": 356
    },
    {
      "epoch": 0.05751802473113948,
      "grad_norm": 2.6283977031707764,
      "learning_rate": 0.0003853092783505155,
      "loss": 0.4209,
      "step": 357
    },
    {
      "epoch": 0.05767913964635276,
      "grad_norm": 2.5042197704315186,
      "learning_rate": 0.0003849871134020619,
      "loss": 0.3029,
      "step": 358
    },
    {
      "epoch": 0.05784025456156604,
      "grad_norm": 1.6208170652389526,
      "learning_rate": 0.0003846649484536083,
      "loss": 0.2588,
      "step": 359
    },
    {
      "epoch": 0.058001369476779314,
      "grad_norm": 1.9576218128204346,
      "learning_rate": 0.00038434278350515466,
      "loss": 0.2435,
      "step": 360
    },
    {
      "epoch": 0.05816248439199259,
      "grad_norm": 1.9775100946426392,
      "learning_rate": 0.0003840206185567011,
      "loss": 0.2546,
      "step": 361
    },
    {
      "epoch": 0.05832359930720586,
      "grad_norm": 1.9132277965545654,
      "learning_rate": 0.00038369845360824745,
      "loss": 0.3302,
      "step": 362
    },
    {
      "epoch": 0.05848471422241914,
      "grad_norm": 2.134699583053589,
      "learning_rate": 0.00038337628865979387,
      "loss": 0.2379,
      "step": 363
    },
    {
      "epoch": 0.058645829137632416,
      "grad_norm": 2.0460002422332764,
      "learning_rate": 0.00038305412371134024,
      "loss": 0.2691,
      "step": 364
    },
    {
      "epoch": 0.05880694405284569,
      "grad_norm": 2.0563623905181885,
      "learning_rate": 0.00038273195876288655,
      "loss": 0.2816,
      "step": 365
    },
    {
      "epoch": 0.05896805896805897,
      "grad_norm": 1.399924397468567,
      "learning_rate": 0.000382409793814433,
      "loss": 0.2139,
      "step": 366
    },
    {
      "epoch": 0.05912917388327225,
      "grad_norm": 2.1608121395111084,
      "learning_rate": 0.00038208762886597934,
      "loss": 0.2623,
      "step": 367
    },
    {
      "epoch": 0.05929028879848552,
      "grad_norm": 2.081350088119507,
      "learning_rate": 0.00038176546391752576,
      "loss": 0.2569,
      "step": 368
    },
    {
      "epoch": 0.059451403713698794,
      "grad_norm": 3.2392210960388184,
      "learning_rate": 0.00038144329896907213,
      "loss": 0.4041,
      "step": 369
    },
    {
      "epoch": 0.05961251862891207,
      "grad_norm": 2.234248161315918,
      "learning_rate": 0.00038112113402061855,
      "loss": 0.2183,
      "step": 370
    },
    {
      "epoch": 0.05977363354412535,
      "grad_norm": 2.010498523712158,
      "learning_rate": 0.0003807989690721649,
      "loss": 0.2548,
      "step": 371
    },
    {
      "epoch": 0.059934748459338626,
      "grad_norm": 2.931427478790283,
      "learning_rate": 0.00038047680412371134,
      "loss": 0.2217,
      "step": 372
    },
    {
      "epoch": 0.060095863374551896,
      "grad_norm": 1.7914336919784546,
      "learning_rate": 0.0003801546391752577,
      "loss": 0.2568,
      "step": 373
    },
    {
      "epoch": 0.06025697828976517,
      "grad_norm": 3.747952938079834,
      "learning_rate": 0.0003798324742268041,
      "loss": 0.3521,
      "step": 374
    },
    {
      "epoch": 0.06041809320497845,
      "grad_norm": 2.6157569885253906,
      "learning_rate": 0.0003795103092783505,
      "loss": 0.282,
      "step": 375
    },
    {
      "epoch": 0.06057920812019173,
      "grad_norm": 2.583035707473755,
      "learning_rate": 0.0003791881443298969,
      "loss": 0.2575,
      "step": 376
    },
    {
      "epoch": 0.060740323035405004,
      "grad_norm": 4.037543773651123,
      "learning_rate": 0.0003788659793814433,
      "loss": 0.3285,
      "step": 377
    },
    {
      "epoch": 0.06090143795061828,
      "grad_norm": 2.754746913909912,
      "learning_rate": 0.0003785438144329897,
      "loss": 0.2787,
      "step": 378
    },
    {
      "epoch": 0.06106255286583155,
      "grad_norm": 2.498307228088379,
      "learning_rate": 0.00037822164948453607,
      "loss": 0.2532,
      "step": 379
    },
    {
      "epoch": 0.06122366778104483,
      "grad_norm": 1.666962742805481,
      "learning_rate": 0.0003778994845360825,
      "loss": 0.2076,
      "step": 380
    },
    {
      "epoch": 0.061384782696258106,
      "grad_norm": 2.379676103591919,
      "learning_rate": 0.00037757731958762886,
      "loss": 0.3556,
      "step": 381
    },
    {
      "epoch": 0.06154589761147138,
      "grad_norm": 2.1286187171936035,
      "learning_rate": 0.0003772551546391753,
      "loss": 0.3034,
      "step": 382
    },
    {
      "epoch": 0.06170701252668466,
      "grad_norm": 2.3203253746032715,
      "learning_rate": 0.00037693298969072164,
      "loss": 0.2894,
      "step": 383
    },
    {
      "epoch": 0.06186812744189794,
      "grad_norm": 2.512477397918701,
      "learning_rate": 0.00037661082474226807,
      "loss": 0.2883,
      "step": 384
    },
    {
      "epoch": 0.06202924235711121,
      "grad_norm": 1.9302541017532349,
      "learning_rate": 0.00037628865979381443,
      "loss": 0.3008,
      "step": 385
    },
    {
      "epoch": 0.062190357272324484,
      "grad_norm": 2.0071206092834473,
      "learning_rate": 0.00037596649484536085,
      "loss": 0.2234,
      "step": 386
    },
    {
      "epoch": 0.06235147218753776,
      "grad_norm": 1.8122369050979614,
      "learning_rate": 0.0003756443298969072,
      "loss": 0.2013,
      "step": 387
    },
    {
      "epoch": 0.06251258710275104,
      "grad_norm": 2.6229543685913086,
      "learning_rate": 0.00037532216494845364,
      "loss": 0.1924,
      "step": 388
    },
    {
      "epoch": 0.06267370201796431,
      "grad_norm": 1.389499545097351,
      "learning_rate": 0.000375,
      "loss": 0.1726,
      "step": 389
    },
    {
      "epoch": 0.06283481693317759,
      "grad_norm": 3.218369722366333,
      "learning_rate": 0.0003746778350515464,
      "loss": 0.2899,
      "step": 390
    },
    {
      "epoch": 0.06299593184839086,
      "grad_norm": 2.768728494644165,
      "learning_rate": 0.0003743556701030928,
      "loss": 0.2979,
      "step": 391
    },
    {
      "epoch": 0.06315704676360415,
      "grad_norm": 3.2321674823760986,
      "learning_rate": 0.00037403350515463916,
      "loss": 0.2784,
      "step": 392
    },
    {
      "epoch": 0.06331816167881742,
      "grad_norm": 2.250969409942627,
      "learning_rate": 0.0003737113402061856,
      "loss": 0.231,
      "step": 393
    },
    {
      "epoch": 0.06347927659403069,
      "grad_norm": 2.0970823764801025,
      "learning_rate": 0.00037338917525773195,
      "loss": 0.2349,
      "step": 394
    },
    {
      "epoch": 0.06364039150924397,
      "grad_norm": 2.140167236328125,
      "learning_rate": 0.00037306701030927837,
      "loss": 0.1993,
      "step": 395
    },
    {
      "epoch": 0.06380150642445724,
      "grad_norm": 2.425901174545288,
      "learning_rate": 0.00037274484536082474,
      "loss": 0.2773,
      "step": 396
    },
    {
      "epoch": 0.06396262133967053,
      "grad_norm": 2.2366626262664795,
      "learning_rate": 0.00037242268041237116,
      "loss": 0.19,
      "step": 397
    },
    {
      "epoch": 0.0641237362548838,
      "grad_norm": 3.0366275310516357,
      "learning_rate": 0.0003721005154639175,
      "loss": 0.3255,
      "step": 398
    },
    {
      "epoch": 0.06428485117009707,
      "grad_norm": 1.962707757949829,
      "learning_rate": 0.00037177835051546395,
      "loss": 0.2415,
      "step": 399
    },
    {
      "epoch": 0.06444596608531035,
      "grad_norm": 2.774691343307495,
      "learning_rate": 0.0003714561855670103,
      "loss": 0.1808,
      "step": 400
    },
    {
      "epoch": 0.06460708100052362,
      "grad_norm": 2.1571099758148193,
      "learning_rate": 0.00037113402061855674,
      "loss": 0.1823,
      "step": 401
    },
    {
      "epoch": 0.0647681959157369,
      "grad_norm": 2.879239082336426,
      "learning_rate": 0.0003708118556701031,
      "loss": 0.2031,
      "step": 402
    },
    {
      "epoch": 0.06492931083095017,
      "grad_norm": 2.357351541519165,
      "learning_rate": 0.0003704896907216495,
      "loss": 0.2002,
      "step": 403
    },
    {
      "epoch": 0.06509042574616346,
      "grad_norm": 2.8586344718933105,
      "learning_rate": 0.0003701675257731959,
      "loss": 0.3226,
      "step": 404
    },
    {
      "epoch": 0.06525154066137673,
      "grad_norm": 2.2920658588409424,
      "learning_rate": 0.0003698453608247423,
      "loss": 0.2362,
      "step": 405
    },
    {
      "epoch": 0.06541265557659,
      "grad_norm": 3.2616958618164062,
      "learning_rate": 0.0003695231958762887,
      "loss": 0.3051,
      "step": 406
    },
    {
      "epoch": 0.06557377049180328,
      "grad_norm": 2.666334629058838,
      "learning_rate": 0.0003692010309278351,
      "loss": 0.26,
      "step": 407
    },
    {
      "epoch": 0.06573488540701655,
      "grad_norm": 2.518995523452759,
      "learning_rate": 0.00036887886597938147,
      "loss": 0.2498,
      "step": 408
    },
    {
      "epoch": 0.06589600032222984,
      "grad_norm": 2.2192959785461426,
      "learning_rate": 0.0003685567010309279,
      "loss": 0.2557,
      "step": 409
    },
    {
      "epoch": 0.0660571152374431,
      "grad_norm": 1.8224272727966309,
      "learning_rate": 0.00036823453608247425,
      "loss": 0.2056,
      "step": 410
    },
    {
      "epoch": 0.06621823015265638,
      "grad_norm": 1.913841962814331,
      "learning_rate": 0.0003679123711340207,
      "loss": 0.2091,
      "step": 411
    },
    {
      "epoch": 0.06637934506786966,
      "grad_norm": 1.9991739988327026,
      "learning_rate": 0.00036759020618556704,
      "loss": 0.2279,
      "step": 412
    },
    {
      "epoch": 0.06654045998308293,
      "grad_norm": 1.9456026554107666,
      "learning_rate": 0.00036726804123711346,
      "loss": 0.2202,
      "step": 413
    },
    {
      "epoch": 0.06670157489829621,
      "grad_norm": 2.4331586360931396,
      "learning_rate": 0.0003669458762886598,
      "loss": 0.2185,
      "step": 414
    },
    {
      "epoch": 0.06686268981350948,
      "grad_norm": 2.479551315307617,
      "learning_rate": 0.00036662371134020614,
      "loss": 0.2362,
      "step": 415
    },
    {
      "epoch": 0.06702380472872276,
      "grad_norm": 2.400575637817383,
      "learning_rate": 0.00036630154639175256,
      "loss": 0.2616,
      "step": 416
    },
    {
      "epoch": 0.06718491964393604,
      "grad_norm": 2.6677303314208984,
      "learning_rate": 0.00036597938144329893,
      "loss": 0.2112,
      "step": 417
    },
    {
      "epoch": 0.06734603455914931,
      "grad_norm": 3.5992040634155273,
      "learning_rate": 0.00036565721649484535,
      "loss": 0.2438,
      "step": 418
    },
    {
      "epoch": 0.0675071494743626,
      "grad_norm": 2.249464511871338,
      "learning_rate": 0.0003653350515463917,
      "loss": 0.2248,
      "step": 419
    },
    {
      "epoch": 0.06766826438957586,
      "grad_norm": 2.564779043197632,
      "learning_rate": 0.00036501288659793814,
      "loss": 0.211,
      "step": 420
    },
    {
      "epoch": 0.06782937930478915,
      "grad_norm": 1.9772529602050781,
      "learning_rate": 0.0003646907216494845,
      "loss": 0.1968,
      "step": 421
    },
    {
      "epoch": 0.06799049422000242,
      "grad_norm": 2.674145460128784,
      "learning_rate": 0.00036436855670103093,
      "loss": 0.2254,
      "step": 422
    },
    {
      "epoch": 0.06815160913521569,
      "grad_norm": 2.1348347663879395,
      "learning_rate": 0.0003640463917525773,
      "loss": 0.2053,
      "step": 423
    },
    {
      "epoch": 0.06831272405042897,
      "grad_norm": 2.7674527168273926,
      "learning_rate": 0.0003637242268041237,
      "loss": 0.2648,
      "step": 424
    },
    {
      "epoch": 0.06847383896564224,
      "grad_norm": 3.2734687328338623,
      "learning_rate": 0.0003634020618556701,
      "loss": 0.215,
      "step": 425
    },
    {
      "epoch": 0.06863495388085553,
      "grad_norm": 3.3614590167999268,
      "learning_rate": 0.0003630798969072165,
      "loss": 0.364,
      "step": 426
    },
    {
      "epoch": 0.0687960687960688,
      "grad_norm": 2.468780040740967,
      "learning_rate": 0.00036275773195876287,
      "loss": 0.2012,
      "step": 427
    },
    {
      "epoch": 0.06895718371128207,
      "grad_norm": 2.9233717918395996,
      "learning_rate": 0.0003624355670103093,
      "loss": 0.2526,
      "step": 428
    },
    {
      "epoch": 0.06911829862649535,
      "grad_norm": 2.9486868381500244,
      "learning_rate": 0.00036211340206185566,
      "loss": 0.2743,
      "step": 429
    },
    {
      "epoch": 0.06927941354170862,
      "grad_norm": 2.2632250785827637,
      "learning_rate": 0.0003617912371134021,
      "loss": 0.1965,
      "step": 430
    },
    {
      "epoch": 0.0694405284569219,
      "grad_norm": 1.8968448638916016,
      "learning_rate": 0.00036146907216494845,
      "loss": 0.1862,
      "step": 431
    },
    {
      "epoch": 0.06960164337213517,
      "grad_norm": 2.1435370445251465,
      "learning_rate": 0.00036114690721649487,
      "loss": 0.2141,
      "step": 432
    },
    {
      "epoch": 0.06976275828734844,
      "grad_norm": 1.9581577777862549,
      "learning_rate": 0.00036082474226804123,
      "loss": 0.2301,
      "step": 433
    },
    {
      "epoch": 0.06992387320256173,
      "grad_norm": 1.9897805452346802,
      "learning_rate": 0.00036050257731958766,
      "loss": 0.1869,
      "step": 434
    },
    {
      "epoch": 0.070084988117775,
      "grad_norm": 2.3988311290740967,
      "learning_rate": 0.000360180412371134,
      "loss": 0.2141,
      "step": 435
    },
    {
      "epoch": 0.07024610303298828,
      "grad_norm": 1.8954321146011353,
      "learning_rate": 0.00035985824742268044,
      "loss": 0.1686,
      "step": 436
    },
    {
      "epoch": 0.07040721794820155,
      "grad_norm": 2.3760218620300293,
      "learning_rate": 0.0003595360824742268,
      "loss": 0.1791,
      "step": 437
    },
    {
      "epoch": 0.07056833286341484,
      "grad_norm": 2.38767409324646,
      "learning_rate": 0.0003592139175257732,
      "loss": 0.1794,
      "step": 438
    },
    {
      "epoch": 0.07072944777862811,
      "grad_norm": 2.380232095718384,
      "learning_rate": 0.0003588917525773196,
      "loss": 0.1816,
      "step": 439
    },
    {
      "epoch": 0.07089056269384138,
      "grad_norm": 2.665675163269043,
      "learning_rate": 0.00035856958762886597,
      "loss": 0.1597,
      "step": 440
    },
    {
      "epoch": 0.07105167760905466,
      "grad_norm": 3.6030056476593018,
      "learning_rate": 0.0003582474226804124,
      "loss": 0.3001,
      "step": 441
    },
    {
      "epoch": 0.07121279252426793,
      "grad_norm": 3.488985300064087,
      "learning_rate": 0.00035792525773195875,
      "loss": 0.2929,
      "step": 442
    },
    {
      "epoch": 0.07137390743948122,
      "grad_norm": 2.586043119430542,
      "learning_rate": 0.0003576030927835052,
      "loss": 0.2363,
      "step": 443
    },
    {
      "epoch": 0.07153502235469449,
      "grad_norm": 4.341652870178223,
      "learning_rate": 0.00035728092783505154,
      "loss": 0.3148,
      "step": 444
    },
    {
      "epoch": 0.07169613726990776,
      "grad_norm": 3.243163824081421,
      "learning_rate": 0.00035695876288659796,
      "loss": 0.3214,
      "step": 445
    },
    {
      "epoch": 0.07185725218512104,
      "grad_norm": 3.4180355072021484,
      "learning_rate": 0.00035663659793814433,
      "loss": 0.3158,
      "step": 446
    },
    {
      "epoch": 0.07201836710033431,
      "grad_norm": 3.013139247894287,
      "learning_rate": 0.00035631443298969075,
      "loss": 0.2033,
      "step": 447
    },
    {
      "epoch": 0.0721794820155476,
      "grad_norm": 2.4991188049316406,
      "learning_rate": 0.0003559922680412371,
      "loss": 0.283,
      "step": 448
    },
    {
      "epoch": 0.07234059693076086,
      "grad_norm": 1.8533926010131836,
      "learning_rate": 0.00035567010309278354,
      "loss": 0.1945,
      "step": 449
    },
    {
      "epoch": 0.07250171184597413,
      "grad_norm": 2.1890084743499756,
      "learning_rate": 0.0003553479381443299,
      "loss": 0.236,
      "step": 450
    },
    {
      "epoch": 0.07266282676118742,
      "grad_norm": 2.034461498260498,
      "learning_rate": 0.0003550257731958763,
      "loss": 0.2192,
      "step": 451
    },
    {
      "epoch": 0.07282394167640069,
      "grad_norm": 2.1856071949005127,
      "learning_rate": 0.0003547036082474227,
      "loss": 0.3071,
      "step": 452
    },
    {
      "epoch": 0.07298505659161397,
      "grad_norm": 1.7042914628982544,
      "learning_rate": 0.0003543814432989691,
      "loss": 0.1907,
      "step": 453
    },
    {
      "epoch": 0.07314617150682724,
      "grad_norm": 2.7188737392425537,
      "learning_rate": 0.0003540592783505155,
      "loss": 0.2866,
      "step": 454
    },
    {
      "epoch": 0.07330728642204053,
      "grad_norm": 2.007195472717285,
      "learning_rate": 0.0003537371134020619,
      "loss": 0.2342,
      "step": 455
    },
    {
      "epoch": 0.0734684013372538,
      "grad_norm": 1.8261600732803345,
      "learning_rate": 0.00035341494845360827,
      "loss": 0.2607,
      "step": 456
    },
    {
      "epoch": 0.07362951625246707,
      "grad_norm": 2.015860080718994,
      "learning_rate": 0.0003530927835051547,
      "loss": 0.2008,
      "step": 457
    },
    {
      "epoch": 0.07379063116768035,
      "grad_norm": 1.3982278108596802,
      "learning_rate": 0.00035277061855670106,
      "loss": 0.2038,
      "step": 458
    },
    {
      "epoch": 0.07395174608289362,
      "grad_norm": 1.311387300491333,
      "learning_rate": 0.0003524484536082475,
      "loss": 0.1484,
      "step": 459
    },
    {
      "epoch": 0.0741128609981069,
      "grad_norm": 1.891360878944397,
      "learning_rate": 0.00035212628865979384,
      "loss": 0.1922,
      "step": 460
    },
    {
      "epoch": 0.07427397591332018,
      "grad_norm": 2.6250946521759033,
      "learning_rate": 0.00035180412371134027,
      "loss": 0.1867,
      "step": 461
    },
    {
      "epoch": 0.07443509082853345,
      "grad_norm": 1.9161999225616455,
      "learning_rate": 0.0003514819587628866,
      "loss": 0.2181,
      "step": 462
    },
    {
      "epoch": 0.07459620574374673,
      "grad_norm": 1.7243473529815674,
      "learning_rate": 0.00035115979381443295,
      "loss": 0.1295,
      "step": 463
    },
    {
      "epoch": 0.07475732065896,
      "grad_norm": 2.156362771987915,
      "learning_rate": 0.00035083762886597937,
      "loss": 0.1827,
      "step": 464
    },
    {
      "epoch": 0.07491843557417328,
      "grad_norm": 2.3141238689422607,
      "learning_rate": 0.00035051546391752573,
      "loss": 0.2126,
      "step": 465
    },
    {
      "epoch": 0.07507955048938655,
      "grad_norm": 2.3099048137664795,
      "learning_rate": 0.00035019329896907215,
      "loss": 0.2126,
      "step": 466
    },
    {
      "epoch": 0.07524066540459982,
      "grad_norm": 2.526893138885498,
      "learning_rate": 0.0003498711340206185,
      "loss": 0.2281,
      "step": 467
    },
    {
      "epoch": 0.07540178031981311,
      "grad_norm": 2.532357692718506,
      "learning_rate": 0.00034954896907216494,
      "loss": 0.1827,
      "step": 468
    },
    {
      "epoch": 0.07556289523502638,
      "grad_norm": 2.6136956214904785,
      "learning_rate": 0.0003492268041237113,
      "loss": 0.183,
      "step": 469
    },
    {
      "epoch": 0.07572401015023966,
      "grad_norm": 3.003889322280884,
      "learning_rate": 0.00034890463917525773,
      "loss": 0.2373,
      "step": 470
    },
    {
      "epoch": 0.07588512506545293,
      "grad_norm": 1.9139409065246582,
      "learning_rate": 0.0003485824742268041,
      "loss": 0.1735,
      "step": 471
    },
    {
      "epoch": 0.07604623998066622,
      "grad_norm": 2.228816032409668,
      "learning_rate": 0.0003482603092783505,
      "loss": 0.2098,
      "step": 472
    },
    {
      "epoch": 0.07620735489587949,
      "grad_norm": 3.2495036125183105,
      "learning_rate": 0.0003479381443298969,
      "loss": 0.2962,
      "step": 473
    },
    {
      "epoch": 0.07636846981109276,
      "grad_norm": 2.619446277618408,
      "learning_rate": 0.0003476159793814433,
      "loss": 0.2545,
      "step": 474
    },
    {
      "epoch": 0.07652958472630604,
      "grad_norm": 1.518890142440796,
      "learning_rate": 0.0003472938144329897,
      "loss": 0.195,
      "step": 475
    },
    {
      "epoch": 0.07669069964151931,
      "grad_norm": 1.888769268989563,
      "learning_rate": 0.0003469716494845361,
      "loss": 0.2201,
      "step": 476
    },
    {
      "epoch": 0.0768518145567326,
      "grad_norm": 2.979008436203003,
      "learning_rate": 0.00034664948453608246,
      "loss": 0.2134,
      "step": 477
    },
    {
      "epoch": 0.07701292947194587,
      "grad_norm": 1.7505918741226196,
      "learning_rate": 0.0003463273195876289,
      "loss": 0.1956,
      "step": 478
    },
    {
      "epoch": 0.07717404438715914,
      "grad_norm": 1.435767650604248,
      "learning_rate": 0.00034600515463917525,
      "loss": 0.2073,
      "step": 479
    },
    {
      "epoch": 0.07733515930237242,
      "grad_norm": 2.1685051918029785,
      "learning_rate": 0.00034568298969072167,
      "loss": 0.1891,
      "step": 480
    },
    {
      "epoch": 0.07749627421758569,
      "grad_norm": 1.78818678855896,
      "learning_rate": 0.00034536082474226804,
      "loss": 0.2755,
      "step": 481
    },
    {
      "epoch": 0.07765738913279897,
      "grad_norm": 1.5817357301712036,
      "learning_rate": 0.00034503865979381446,
      "loss": 0.1855,
      "step": 482
    },
    {
      "epoch": 0.07781850404801224,
      "grad_norm": 2.006964921951294,
      "learning_rate": 0.0003447164948453608,
      "loss": 0.2011,
      "step": 483
    },
    {
      "epoch": 0.07797961896322551,
      "grad_norm": 2.270946979522705,
      "learning_rate": 0.00034439432989690725,
      "loss": 0.1962,
      "step": 484
    },
    {
      "epoch": 0.0781407338784388,
      "grad_norm": 2.2922959327697754,
      "learning_rate": 0.0003440721649484536,
      "loss": 0.1914,
      "step": 485
    },
    {
      "epoch": 0.07830184879365207,
      "grad_norm": 2.0950582027435303,
      "learning_rate": 0.00034375,
      "loss": 0.163,
      "step": 486
    },
    {
      "epoch": 0.07846296370886535,
      "grad_norm": 2.0762712955474854,
      "learning_rate": 0.0003434278350515464,
      "loss": 0.2181,
      "step": 487
    },
    {
      "epoch": 0.07862407862407862,
      "grad_norm": 2.152665376663208,
      "learning_rate": 0.00034310567010309277,
      "loss": 0.1476,
      "step": 488
    },
    {
      "epoch": 0.0787851935392919,
      "grad_norm": 2.7204174995422363,
      "learning_rate": 0.0003427835051546392,
      "loss": 0.1893,
      "step": 489
    },
    {
      "epoch": 0.07894630845450518,
      "grad_norm": 3.0283617973327637,
      "learning_rate": 0.00034246134020618556,
      "loss": 0.1916,
      "step": 490
    },
    {
      "epoch": 0.07910742336971845,
      "grad_norm": 2.8309292793273926,
      "learning_rate": 0.000342139175257732,
      "loss": 0.1635,
      "step": 491
    },
    {
      "epoch": 0.07926853828493173,
      "grad_norm": 2.524590015411377,
      "learning_rate": 0.00034181701030927834,
      "loss": 0.1917,
      "step": 492
    },
    {
      "epoch": 0.079429653200145,
      "grad_norm": 3.577038049697876,
      "learning_rate": 0.00034149484536082477,
      "loss": 0.3202,
      "step": 493
    },
    {
      "epoch": 0.07959076811535828,
      "grad_norm": 2.551281213760376,
      "learning_rate": 0.00034117268041237113,
      "loss": 0.2544,
      "step": 494
    },
    {
      "epoch": 0.07975188303057155,
      "grad_norm": 3.4986159801483154,
      "learning_rate": 0.00034085051546391755,
      "loss": 0.3107,
      "step": 495
    },
    {
      "epoch": 0.07991299794578482,
      "grad_norm": 3.428227424621582,
      "learning_rate": 0.0003405283505154639,
      "loss": 0.2537,
      "step": 496
    },
    {
      "epoch": 0.08007411286099811,
      "grad_norm": 4.2652268409729,
      "learning_rate": 0.00034020618556701034,
      "loss": 0.2967,
      "step": 497
    },
    {
      "epoch": 0.08023522777621138,
      "grad_norm": 2.105372428894043,
      "learning_rate": 0.0003398840206185567,
      "loss": 0.1937,
      "step": 498
    },
    {
      "epoch": 0.08039634269142466,
      "grad_norm": 1.9998162984848022,
      "learning_rate": 0.00033956185567010313,
      "loss": 0.1829,
      "step": 499
    },
    {
      "epoch": 0.08055745760663793,
      "grad_norm": 2.4103643894195557,
      "learning_rate": 0.0003392396907216495,
      "loss": 0.3025,
      "step": 500
    },
    {
      "epoch": 0.0807185725218512,
      "grad_norm": 3.117419719696045,
      "learning_rate": 0.0003389175257731959,
      "loss": 0.2478,
      "step": 501
    },
    {
      "epoch": 0.08087968743706449,
      "grad_norm": 3.1468372344970703,
      "learning_rate": 0.0003385953608247423,
      "loss": 0.2873,
      "step": 502
    },
    {
      "epoch": 0.08104080235227776,
      "grad_norm": 2.744896173477173,
      "learning_rate": 0.0003382731958762887,
      "loss": 0.2488,
      "step": 503
    },
    {
      "epoch": 0.08120191726749104,
      "grad_norm": 1.6195399761199951,
      "learning_rate": 0.00033795103092783507,
      "loss": 0.201,
      "step": 504
    },
    {
      "epoch": 0.08136303218270431,
      "grad_norm": 1.720001459121704,
      "learning_rate": 0.0003376288659793815,
      "loss": 0.2282,
      "step": 505
    },
    {
      "epoch": 0.0815241470979176,
      "grad_norm": 1.7487422227859497,
      "learning_rate": 0.00033730670103092786,
      "loss": 0.1805,
      "step": 506
    },
    {
      "epoch": 0.08168526201313087,
      "grad_norm": 2.337805986404419,
      "learning_rate": 0.0003369845360824743,
      "loss": 0.2466,
      "step": 507
    },
    {
      "epoch": 0.08184637692834414,
      "grad_norm": 2.6994786262512207,
      "learning_rate": 0.00033666237113402065,
      "loss": 0.2799,
      "step": 508
    },
    {
      "epoch": 0.08200749184355742,
      "grad_norm": 1.9488059282302856,
      "learning_rate": 0.00033634020618556707,
      "loss": 0.1692,
      "step": 509
    },
    {
      "epoch": 0.08216860675877069,
      "grad_norm": 2.303338050842285,
      "learning_rate": 0.00033601804123711344,
      "loss": 0.2408,
      "step": 510
    },
    {
      "epoch": 0.08232972167398397,
      "grad_norm": 1.901342511177063,
      "learning_rate": 0.00033569587628865975,
      "loss": 0.1709,
      "step": 511
    },
    {
      "epoch": 0.08249083658919724,
      "grad_norm": 1.6360007524490356,
      "learning_rate": 0.00033537371134020617,
      "loss": 0.116,
      "step": 512
    },
    {
      "epoch": 0.08265195150441051,
      "grad_norm": 1.9833916425704956,
      "learning_rate": 0.00033505154639175254,
      "loss": 0.1432,
      "step": 513
    },
    {
      "epoch": 0.0828130664196238,
      "grad_norm": 2.642000436782837,
      "learning_rate": 0.00033472938144329896,
      "loss": 0.2654,
      "step": 514
    },
    {
      "epoch": 0.08297418133483707,
      "grad_norm": 2.860081434249878,
      "learning_rate": 0.0003344072164948453,
      "loss": 0.2151,
      "step": 515
    },
    {
      "epoch": 0.08313529625005035,
      "grad_norm": 1.917969822883606,
      "learning_rate": 0.00033408505154639175,
      "loss": 0.1531,
      "step": 516
    },
    {
      "epoch": 0.08329641116526362,
      "grad_norm": 3.0976402759552,
      "learning_rate": 0.0003337628865979381,
      "loss": 0.2501,
      "step": 517
    },
    {
      "epoch": 0.08345752608047691,
      "grad_norm": 1.8531910181045532,
      "learning_rate": 0.00033344072164948453,
      "loss": 0.1504,
      "step": 518
    },
    {
      "epoch": 0.08361864099569018,
      "grad_norm": 2.0486772060394287,
      "learning_rate": 0.0003331185567010309,
      "loss": 0.158,
      "step": 519
    },
    {
      "epoch": 0.08377975591090345,
      "grad_norm": 2.1142995357513428,
      "learning_rate": 0.0003327963917525773,
      "loss": 0.1738,
      "step": 520
    },
    {
      "epoch": 0.08394087082611673,
      "grad_norm": 1.6821985244750977,
      "learning_rate": 0.0003324742268041237,
      "loss": 0.1495,
      "step": 521
    },
    {
      "epoch": 0.08410198574133,
      "grad_norm": 1.9827914237976074,
      "learning_rate": 0.0003321520618556701,
      "loss": 0.1737,
      "step": 522
    },
    {
      "epoch": 0.08426310065654329,
      "grad_norm": 2.329108238220215,
      "learning_rate": 0.0003318298969072165,
      "loss": 0.2071,
      "step": 523
    },
    {
      "epoch": 0.08442421557175656,
      "grad_norm": 2.5297493934631348,
      "learning_rate": 0.0003315077319587629,
      "loss": 0.2841,
      "step": 524
    },
    {
      "epoch": 0.08458533048696983,
      "grad_norm": 4.335241794586182,
      "learning_rate": 0.00033118556701030926,
      "loss": 0.2856,
      "step": 525
    },
    {
      "epoch": 0.08474644540218311,
      "grad_norm": 2.015735626220703,
      "learning_rate": 0.0003308634020618557,
      "loss": 0.1879,
      "step": 526
    },
    {
      "epoch": 0.08490756031739638,
      "grad_norm": 2.7720959186553955,
      "learning_rate": 0.00033054123711340205,
      "loss": 0.289,
      "step": 527
    },
    {
      "epoch": 0.08506867523260966,
      "grad_norm": 2.4796106815338135,
      "learning_rate": 0.0003302190721649485,
      "loss": 0.2063,
      "step": 528
    },
    {
      "epoch": 0.08522979014782293,
      "grad_norm": 1.864898920059204,
      "learning_rate": 0.00032989690721649484,
      "loss": 0.2107,
      "step": 529
    },
    {
      "epoch": 0.0853909050630362,
      "grad_norm": 2.0946907997131348,
      "learning_rate": 0.00032957474226804126,
      "loss": 0.153,
      "step": 530
    },
    {
      "epoch": 0.08555201997824949,
      "grad_norm": 2.62062406539917,
      "learning_rate": 0.00032925257731958763,
      "loss": 0.2365,
      "step": 531
    },
    {
      "epoch": 0.08571313489346276,
      "grad_norm": 3.878737449645996,
      "learning_rate": 0.00032893041237113405,
      "loss": 0.3471,
      "step": 532
    },
    {
      "epoch": 0.08587424980867604,
      "grad_norm": 2.253488302230835,
      "learning_rate": 0.0003286082474226804,
      "loss": 0.2294,
      "step": 533
    },
    {
      "epoch": 0.08603536472388931,
      "grad_norm": 3.0655696392059326,
      "learning_rate": 0.00032828608247422684,
      "loss": 0.1971,
      "step": 534
    },
    {
      "epoch": 0.0861964796391026,
      "grad_norm": 2.216844320297241,
      "learning_rate": 0.0003279639175257732,
      "loss": 0.2219,
      "step": 535
    },
    {
      "epoch": 0.08635759455431587,
      "grad_norm": 2.523479461669922,
      "learning_rate": 0.00032764175257731957,
      "loss": 0.2254,
      "step": 536
    },
    {
      "epoch": 0.08651870946952914,
      "grad_norm": 2.4784767627716064,
      "learning_rate": 0.000327319587628866,
      "loss": 0.2312,
      "step": 537
    },
    {
      "epoch": 0.08667982438474242,
      "grad_norm": 2.7602505683898926,
      "learning_rate": 0.00032699742268041236,
      "loss": 0.171,
      "step": 538
    },
    {
      "epoch": 0.08684093929995569,
      "grad_norm": 2.211337089538574,
      "learning_rate": 0.0003266752577319588,
      "loss": 0.1867,
      "step": 539
    },
    {
      "epoch": 0.08700205421516898,
      "grad_norm": 2.6306445598602295,
      "learning_rate": 0.00032635309278350515,
      "loss": 0.2944,
      "step": 540
    },
    {
      "epoch": 0.08716316913038225,
      "grad_norm": 1.9977916479110718,
      "learning_rate": 0.00032603092783505157,
      "loss": 0.2826,
      "step": 541
    },
    {
      "epoch": 0.08732428404559552,
      "grad_norm": 1.9951459169387817,
      "learning_rate": 0.00032570876288659793,
      "loss": 0.1945,
      "step": 542
    },
    {
      "epoch": 0.0874853989608088,
      "grad_norm": 1.8166064023971558,
      "learning_rate": 0.00032538659793814436,
      "loss": 0.1961,
      "step": 543
    },
    {
      "epoch": 0.08764651387602207,
      "grad_norm": 2.427006244659424,
      "learning_rate": 0.0003250644329896907,
      "loss": 0.2474,
      "step": 544
    },
    {
      "epoch": 0.08780762879123535,
      "grad_norm": 2.101820945739746,
      "learning_rate": 0.00032474226804123714,
      "loss": 0.1695,
      "step": 545
    },
    {
      "epoch": 0.08796874370644862,
      "grad_norm": 1.9046344757080078,
      "learning_rate": 0.0003244201030927835,
      "loss": 0.2457,
      "step": 546
    },
    {
      "epoch": 0.0881298586216619,
      "grad_norm": 2.072385787963867,
      "learning_rate": 0.00032409793814432993,
      "loss": 0.2014,
      "step": 547
    },
    {
      "epoch": 0.08829097353687518,
      "grad_norm": 2.1242318153381348,
      "learning_rate": 0.0003237757731958763,
      "loss": 0.171,
      "step": 548
    },
    {
      "epoch": 0.08845208845208845,
      "grad_norm": 2.1502087116241455,
      "learning_rate": 0.0003234536082474227,
      "loss": 0.1809,
      "step": 549
    },
    {
      "epoch": 0.08861320336730173,
      "grad_norm": 1.7253220081329346,
      "learning_rate": 0.0003231314432989691,
      "loss": 0.2113,
      "step": 550
    },
    {
      "epoch": 0.088774318282515,
      "grad_norm": 1.811344027519226,
      "learning_rate": 0.0003228092783505155,
      "loss": 0.1482,
      "step": 551
    },
    {
      "epoch": 0.08893543319772829,
      "grad_norm": 1.5654041767120361,
      "learning_rate": 0.0003224871134020619,
      "loss": 0.1485,
      "step": 552
    },
    {
      "epoch": 0.08909654811294156,
      "grad_norm": 1.5349705219268799,
      "learning_rate": 0.0003221649484536083,
      "loss": 0.1233,
      "step": 553
    },
    {
      "epoch": 0.08925766302815483,
      "grad_norm": 2.0071980953216553,
      "learning_rate": 0.00032184278350515466,
      "loss": 0.1386,
      "step": 554
    },
    {
      "epoch": 0.08941877794336811,
      "grad_norm": 1.8269444704055786,
      "learning_rate": 0.0003215206185567011,
      "loss": 0.1274,
      "step": 555
    },
    {
      "epoch": 0.08957989285858138,
      "grad_norm": 3.0343897342681885,
      "learning_rate": 0.00032119845360824745,
      "loss": 0.2287,
      "step": 556
    },
    {
      "epoch": 0.08974100777379466,
      "grad_norm": 2.4317691326141357,
      "learning_rate": 0.00032087628865979387,
      "loss": 0.2151,
      "step": 557
    },
    {
      "epoch": 0.08990212268900794,
      "grad_norm": 2.4198813438415527,
      "learning_rate": 0.00032055412371134024,
      "loss": 0.2346,
      "step": 558
    },
    {
      "epoch": 0.0900632376042212,
      "grad_norm": 2.6795222759246826,
      "learning_rate": 0.00032023195876288655,
      "loss": 0.1949,
      "step": 559
    },
    {
      "epoch": 0.09022435251943449,
      "grad_norm": 2.349757432937622,
      "learning_rate": 0.00031990979381443297,
      "loss": 0.2284,
      "step": 560
    },
    {
      "epoch": 0.09038546743464776,
      "grad_norm": 2.8680965900421143,
      "learning_rate": 0.00031958762886597934,
      "loss": 0.1651,
      "step": 561
    },
    {
      "epoch": 0.09054658234986104,
      "grad_norm": 1.7432090044021606,
      "learning_rate": 0.00031926546391752576,
      "loss": 0.1824,
      "step": 562
    },
    {
      "epoch": 0.09070769726507431,
      "grad_norm": 1.6496862173080444,
      "learning_rate": 0.0003189432989690721,
      "loss": 0.2049,
      "step": 563
    },
    {
      "epoch": 0.09086881218028758,
      "grad_norm": 2.894350290298462,
      "learning_rate": 0.00031862113402061855,
      "loss": 0.2151,
      "step": 564
    },
    {
      "epoch": 0.09102992709550087,
      "grad_norm": 3.2191545963287354,
      "learning_rate": 0.0003182989690721649,
      "loss": 0.2513,
      "step": 565
    },
    {
      "epoch": 0.09119104201071414,
      "grad_norm": 2.191600799560547,
      "learning_rate": 0.00031797680412371134,
      "loss": 0.1645,
      "step": 566
    },
    {
      "epoch": 0.09135215692592742,
      "grad_norm": 1.910836935043335,
      "learning_rate": 0.0003176546391752577,
      "loss": 0.247,
      "step": 567
    },
    {
      "epoch": 0.09151327184114069,
      "grad_norm": 1.3056467771530151,
      "learning_rate": 0.0003173324742268041,
      "loss": 0.1159,
      "step": 568
    },
    {
      "epoch": 0.09167438675635398,
      "grad_norm": 1.8122305870056152,
      "learning_rate": 0.0003170103092783505,
      "loss": 0.174,
      "step": 569
    },
    {
      "epoch": 0.09183550167156725,
      "grad_norm": 1.8673827648162842,
      "learning_rate": 0.0003166881443298969,
      "loss": 0.2118,
      "step": 570
    },
    {
      "epoch": 0.09199661658678052,
      "grad_norm": 1.6920374631881714,
      "learning_rate": 0.0003163659793814433,
      "loss": 0.1363,
      "step": 571
    },
    {
      "epoch": 0.0921577315019938,
      "grad_norm": 1.9351856708526611,
      "learning_rate": 0.0003160438144329897,
      "loss": 0.1955,
      "step": 572
    },
    {
      "epoch": 0.09231884641720707,
      "grad_norm": 1.8738126754760742,
      "learning_rate": 0.00031572164948453607,
      "loss": 0.1395,
      "step": 573
    },
    {
      "epoch": 0.09247996133242035,
      "grad_norm": 2.253098487854004,
      "learning_rate": 0.0003153994845360825,
      "loss": 0.2044,
      "step": 574
    },
    {
      "epoch": 0.09264107624763362,
      "grad_norm": 2.429579734802246,
      "learning_rate": 0.00031507731958762885,
      "loss": 0.2266,
      "step": 575
    },
    {
      "epoch": 0.0928021911628469,
      "grad_norm": 2.8438053131103516,
      "learning_rate": 0.0003147551546391753,
      "loss": 0.2021,
      "step": 576
    },
    {
      "epoch": 0.09296330607806018,
      "grad_norm": 2.370506525039673,
      "learning_rate": 0.00031443298969072164,
      "loss": 0.1931,
      "step": 577
    },
    {
      "epoch": 0.09312442099327345,
      "grad_norm": 2.266441822052002,
      "learning_rate": 0.00031411082474226806,
      "loss": 0.239,
      "step": 578
    },
    {
      "epoch": 0.09328553590848673,
      "grad_norm": 2.526536464691162,
      "learning_rate": 0.00031378865979381443,
      "loss": 0.194,
      "step": 579
    },
    {
      "epoch": 0.0934466508237,
      "grad_norm": 3.800238847732544,
      "learning_rate": 0.00031346649484536085,
      "loss": 0.2922,
      "step": 580
    },
    {
      "epoch": 0.09360776573891327,
      "grad_norm": 3.6546897888183594,
      "learning_rate": 0.0003131443298969072,
      "loss": 0.3019,
      "step": 581
    },
    {
      "epoch": 0.09376888065412656,
      "grad_norm": 1.7063452005386353,
      "learning_rate": 0.00031282216494845364,
      "loss": 0.1657,
      "step": 582
    },
    {
      "epoch": 0.09392999556933983,
      "grad_norm": 2.7483103275299072,
      "learning_rate": 0.0003125,
      "loss": 0.2385,
      "step": 583
    },
    {
      "epoch": 0.09409111048455311,
      "grad_norm": 2.1814217567443848,
      "learning_rate": 0.0003121778350515464,
      "loss": 0.1929,
      "step": 584
    },
    {
      "epoch": 0.09425222539976638,
      "grad_norm": 2.0613605976104736,
      "learning_rate": 0.0003118556701030928,
      "loss": 0.1954,
      "step": 585
    },
    {
      "epoch": 0.09441334031497967,
      "grad_norm": 2.2652456760406494,
      "learning_rate": 0.00031153350515463916,
      "loss": 0.1689,
      "step": 586
    },
    {
      "epoch": 0.09457445523019294,
      "grad_norm": 2.1537134647369385,
      "learning_rate": 0.0003112113402061856,
      "loss": 0.207,
      "step": 587
    },
    {
      "epoch": 0.0947355701454062,
      "grad_norm": 1.7893668413162231,
      "learning_rate": 0.00031088917525773195,
      "loss": 0.1207,
      "step": 588
    },
    {
      "epoch": 0.09489668506061949,
      "grad_norm": 2.2813496589660645,
      "learning_rate": 0.00031056701030927837,
      "loss": 0.1954,
      "step": 589
    },
    {
      "epoch": 0.09505779997583276,
      "grad_norm": 1.7070921659469604,
      "learning_rate": 0.00031024484536082474,
      "loss": 0.1718,
      "step": 590
    },
    {
      "epoch": 0.09521891489104604,
      "grad_norm": 1.8417445421218872,
      "learning_rate": 0.00030992268041237116,
      "loss": 0.1722,
      "step": 591
    },
    {
      "epoch": 0.09538002980625931,
      "grad_norm": 2.3517723083496094,
      "learning_rate": 0.0003096005154639175,
      "loss": 0.1685,
      "step": 592
    },
    {
      "epoch": 0.09554114472147258,
      "grad_norm": 2.6959986686706543,
      "learning_rate": 0.00030927835051546395,
      "loss": 0.2616,
      "step": 593
    },
    {
      "epoch": 0.09570225963668587,
      "grad_norm": 1.7303348779678345,
      "learning_rate": 0.0003089561855670103,
      "loss": 0.1584,
      "step": 594
    },
    {
      "epoch": 0.09586337455189914,
      "grad_norm": 1.5316238403320312,
      "learning_rate": 0.00030863402061855673,
      "loss": 0.1485,
      "step": 595
    },
    {
      "epoch": 0.09602448946711242,
      "grad_norm": 2.157329797744751,
      "learning_rate": 0.0003083118556701031,
      "loss": 0.1834,
      "step": 596
    },
    {
      "epoch": 0.09618560438232569,
      "grad_norm": 1.4519221782684326,
      "learning_rate": 0.0003079896907216495,
      "loss": 0.1997,
      "step": 597
    },
    {
      "epoch": 0.09634671929753896,
      "grad_norm": 2.4552650451660156,
      "learning_rate": 0.0003076675257731959,
      "loss": 0.2116,
      "step": 598
    },
    {
      "epoch": 0.09650783421275225,
      "grad_norm": 2.755105972290039,
      "learning_rate": 0.0003073453608247423,
      "loss": 0.2471,
      "step": 599
    },
    {
      "epoch": 0.09666894912796552,
      "grad_norm": 2.274003744125366,
      "learning_rate": 0.0003070231958762887,
      "loss": 0.149,
      "step": 600
    },
    {
      "epoch": 0.0968300640431788,
      "grad_norm": 2.211496353149414,
      "learning_rate": 0.0003067010309278351,
      "loss": 0.2067,
      "step": 601
    },
    {
      "epoch": 0.09699117895839207,
      "grad_norm": 1.7763617038726807,
      "learning_rate": 0.00030637886597938147,
      "loss": 0.226,
      "step": 602
    },
    {
      "epoch": 0.09715229387360536,
      "grad_norm": 1.8010880947113037,
      "learning_rate": 0.0003060567010309279,
      "loss": 0.2155,
      "step": 603
    },
    {
      "epoch": 0.09731340878881863,
      "grad_norm": 3.283416509628296,
      "learning_rate": 0.00030573453608247425,
      "loss": 0.1931,
      "step": 604
    },
    {
      "epoch": 0.0974745237040319,
      "grad_norm": 1.6688868999481201,
      "learning_rate": 0.0003054123711340207,
      "loss": 0.2137,
      "step": 605
    },
    {
      "epoch": 0.09763563861924518,
      "grad_norm": 2.065164089202881,
      "learning_rate": 0.00030509020618556704,
      "loss": 0.2344,
      "step": 606
    },
    {
      "epoch": 0.09779675353445845,
      "grad_norm": 2.2237000465393066,
      "learning_rate": 0.00030476804123711346,
      "loss": 0.1849,
      "step": 607
    },
    {
      "epoch": 0.09795786844967173,
      "grad_norm": 2.2122113704681396,
      "learning_rate": 0.0003044458762886598,
      "loss": 0.1806,
      "step": 608
    },
    {
      "epoch": 0.098118983364885,
      "grad_norm": 2.6714835166931152,
      "learning_rate": 0.00030412371134020614,
      "loss": 0.2276,
      "step": 609
    },
    {
      "epoch": 0.09828009828009827,
      "grad_norm": 2.48305344581604,
      "learning_rate": 0.00030380154639175256,
      "loss": 0.2114,
      "step": 610
    },
    {
      "epoch": 0.09844121319531156,
      "grad_norm": 2.022460460662842,
      "learning_rate": 0.00030347938144329893,
      "loss": 0.2102,
      "step": 611
    },
    {
      "epoch": 0.09860232811052483,
      "grad_norm": 1.8907431364059448,
      "learning_rate": 0.00030315721649484535,
      "loss": 0.1655,
      "step": 612
    },
    {
      "epoch": 0.09876344302573811,
      "grad_norm": 1.8121665716171265,
      "learning_rate": 0.0003028350515463917,
      "loss": 0.1929,
      "step": 613
    },
    {
      "epoch": 0.09892455794095138,
      "grad_norm": 1.7180715799331665,
      "learning_rate": 0.00030251288659793814,
      "loss": 0.185,
      "step": 614
    },
    {
      "epoch": 0.09908567285616465,
      "grad_norm": 2.3329505920410156,
      "learning_rate": 0.0003021907216494845,
      "loss": 0.205,
      "step": 615
    },
    {
      "epoch": 0.09924678777137794,
      "grad_norm": 1.9647272825241089,
      "learning_rate": 0.0003018685567010309,
      "loss": 0.2458,
      "step": 616
    },
    {
      "epoch": 0.0994079026865912,
      "grad_norm": 1.8365874290466309,
      "learning_rate": 0.0003015463917525773,
      "loss": 0.1929,
      "step": 617
    },
    {
      "epoch": 0.09956901760180449,
      "grad_norm": 1.3880155086517334,
      "learning_rate": 0.0003012242268041237,
      "loss": 0.125,
      "step": 618
    },
    {
      "epoch": 0.09973013251701776,
      "grad_norm": 2.1894896030426025,
      "learning_rate": 0.0003009020618556701,
      "loss": 0.1574,
      "step": 619
    },
    {
      "epoch": 0.09989124743223105,
      "grad_norm": 2.5642552375793457,
      "learning_rate": 0.0003005798969072165,
      "loss": 0.2349,
      "step": 620
    },
    {
      "epoch": 0.10005236234744432,
      "grad_norm": 1.7821003198623657,
      "learning_rate": 0.00030025773195876287,
      "loss": 0.199,
      "step": 621
    },
    {
      "epoch": 0.10021347726265759,
      "grad_norm": 3.162050247192383,
      "learning_rate": 0.0002999355670103093,
      "loss": 0.1752,
      "step": 622
    },
    {
      "epoch": 0.10037459217787087,
      "grad_norm": 1.3391847610473633,
      "learning_rate": 0.00029961340206185566,
      "loss": 0.1294,
      "step": 623
    },
    {
      "epoch": 0.10053570709308414,
      "grad_norm": 1.7607780694961548,
      "learning_rate": 0.0002992912371134021,
      "loss": 0.1158,
      "step": 624
    },
    {
      "epoch": 0.10069682200829742,
      "grad_norm": 2.8704190254211426,
      "learning_rate": 0.00029896907216494845,
      "loss": 0.2139,
      "step": 625
    },
    {
      "epoch": 0.1008579369235107,
      "grad_norm": 1.6206343173980713,
      "learning_rate": 0.00029864690721649487,
      "loss": 0.1548,
      "step": 626
    },
    {
      "epoch": 0.10101905183872396,
      "grad_norm": 1.5717051029205322,
      "learning_rate": 0.00029832474226804123,
      "loss": 0.1069,
      "step": 627
    },
    {
      "epoch": 0.10118016675393725,
      "grad_norm": 2.444258213043213,
      "learning_rate": 0.00029800257731958765,
      "loss": 0.3053,
      "step": 628
    },
    {
      "epoch": 0.10134128166915052,
      "grad_norm": 1.4415465593338013,
      "learning_rate": 0.000297680412371134,
      "loss": 0.1578,
      "step": 629
    },
    {
      "epoch": 0.1015023965843638,
      "grad_norm": 1.9402391910552979,
      "learning_rate": 0.00029735824742268044,
      "loss": 0.212,
      "step": 630
    },
    {
      "epoch": 0.10166351149957707,
      "grad_norm": 1.4670401811599731,
      "learning_rate": 0.0002970360824742268,
      "loss": 0.167,
      "step": 631
    },
    {
      "epoch": 0.10182462641479034,
      "grad_norm": 1.6798601150512695,
      "learning_rate": 0.0002967139175257732,
      "loss": 0.2069,
      "step": 632
    },
    {
      "epoch": 0.10198574133000363,
      "grad_norm": 2.0652689933776855,
      "learning_rate": 0.0002963917525773196,
      "loss": 0.1853,
      "step": 633
    },
    {
      "epoch": 0.1021468562452169,
      "grad_norm": 1.8799468278884888,
      "learning_rate": 0.00029606958762886596,
      "loss": 0.2064,
      "step": 634
    },
    {
      "epoch": 0.10230797116043018,
      "grad_norm": 1.6874711513519287,
      "learning_rate": 0.0002957474226804124,
      "loss": 0.1305,
      "step": 635
    },
    {
      "epoch": 0.10246908607564345,
      "grad_norm": 2.2471015453338623,
      "learning_rate": 0.00029542525773195875,
      "loss": 0.156,
      "step": 636
    },
    {
      "epoch": 0.10263020099085673,
      "grad_norm": 1.9249438047409058,
      "learning_rate": 0.0002951030927835052,
      "loss": 0.2147,
      "step": 637
    },
    {
      "epoch": 0.10279131590607,
      "grad_norm": 2.219221353530884,
      "learning_rate": 0.00029478092783505154,
      "loss": 0.181,
      "step": 638
    },
    {
      "epoch": 0.10295243082128328,
      "grad_norm": 3.244722843170166,
      "learning_rate": 0.00029445876288659796,
      "loss": 0.2561,
      "step": 639
    },
    {
      "epoch": 0.10311354573649656,
      "grad_norm": 2.2352938652038574,
      "learning_rate": 0.00029413659793814433,
      "loss": 0.2244,
      "step": 640
    },
    {
      "epoch": 0.10327466065170983,
      "grad_norm": 1.479521632194519,
      "learning_rate": 0.00029381443298969075,
      "loss": 0.136,
      "step": 641
    },
    {
      "epoch": 0.10343577556692311,
      "grad_norm": 2.185532569885254,
      "learning_rate": 0.0002934922680412371,
      "loss": 0.211,
      "step": 642
    },
    {
      "epoch": 0.10359689048213638,
      "grad_norm": 1.4143766164779663,
      "learning_rate": 0.00029317010309278354,
      "loss": 0.1778,
      "step": 643
    },
    {
      "epoch": 0.10375800539734965,
      "grad_norm": 2.2006239891052246,
      "learning_rate": 0.0002928479381443299,
      "loss": 0.2561,
      "step": 644
    },
    {
      "epoch": 0.10391912031256294,
      "grad_norm": 1.443662166595459,
      "learning_rate": 0.0002925257731958763,
      "loss": 0.1782,
      "step": 645
    },
    {
      "epoch": 0.10408023522777621,
      "grad_norm": 0.9648902416229248,
      "learning_rate": 0.0002922036082474227,
      "loss": 0.1296,
      "step": 646
    },
    {
      "epoch": 0.10424135014298949,
      "grad_norm": 1.7848708629608154,
      "learning_rate": 0.0002918814432989691,
      "loss": 0.2365,
      "step": 647
    },
    {
      "epoch": 0.10440246505820276,
      "grad_norm": 1.9437631368637085,
      "learning_rate": 0.0002915592783505155,
      "loss": 0.1517,
      "step": 648
    },
    {
      "epoch": 0.10456357997341603,
      "grad_norm": 2.1303040981292725,
      "learning_rate": 0.0002912371134020619,
      "loss": 0.1989,
      "step": 649
    },
    {
      "epoch": 0.10472469488862932,
      "grad_norm": 1.3605412244796753,
      "learning_rate": 0.00029091494845360827,
      "loss": 0.1736,
      "step": 650
    },
    {
      "epoch": 0.10488580980384259,
      "grad_norm": 1.8294728994369507,
      "learning_rate": 0.0002905927835051547,
      "loss": 0.1609,
      "step": 651
    },
    {
      "epoch": 0.10504692471905587,
      "grad_norm": 2.4485604763031006,
      "learning_rate": 0.00029027061855670106,
      "loss": 0.2265,
      "step": 652
    },
    {
      "epoch": 0.10520803963426914,
      "grad_norm": 1.3071930408477783,
      "learning_rate": 0.0002899484536082475,
      "loss": 0.1058,
      "step": 653
    },
    {
      "epoch": 0.10536915454948242,
      "grad_norm": 2.2583556175231934,
      "learning_rate": 0.00028962628865979384,
      "loss": 0.1619,
      "step": 654
    },
    {
      "epoch": 0.1055302694646957,
      "grad_norm": 2.836775064468384,
      "learning_rate": 0.00028930412371134026,
      "loss": 0.2278,
      "step": 655
    },
    {
      "epoch": 0.10569138437990896,
      "grad_norm": 3.038562536239624,
      "learning_rate": 0.0002889819587628866,
      "loss": 0.1549,
      "step": 656
    },
    {
      "epoch": 0.10585249929512225,
      "grad_norm": 2.51444149017334,
      "learning_rate": 0.00028865979381443294,
      "loss": 0.1302,
      "step": 657
    },
    {
      "epoch": 0.10601361421033552,
      "grad_norm": 3.082709789276123,
      "learning_rate": 0.00028833762886597937,
      "loss": 0.2077,
      "step": 658
    },
    {
      "epoch": 0.1061747291255488,
      "grad_norm": 2.596506118774414,
      "learning_rate": 0.00028801546391752573,
      "loss": 0.145,
      "step": 659
    },
    {
      "epoch": 0.10633584404076207,
      "grad_norm": 2.389549970626831,
      "learning_rate": 0.00028769329896907215,
      "loss": 0.1823,
      "step": 660
    },
    {
      "epoch": 0.10649695895597534,
      "grad_norm": 1.909435510635376,
      "learning_rate": 0.0002873711340206185,
      "loss": 0.1532,
      "step": 661
    },
    {
      "epoch": 0.10665807387118863,
      "grad_norm": 2.06390118598938,
      "learning_rate": 0.00028704896907216494,
      "loss": 0.1421,
      "step": 662
    },
    {
      "epoch": 0.1068191887864019,
      "grad_norm": 2.4880712032318115,
      "learning_rate": 0.0002867268041237113,
      "loss": 0.2302,
      "step": 663
    },
    {
      "epoch": 0.10698030370161518,
      "grad_norm": 2.1386404037475586,
      "learning_rate": 0.00028640463917525773,
      "loss": 0.1999,
      "step": 664
    },
    {
      "epoch": 0.10714141861682845,
      "grad_norm": 1.5982022285461426,
      "learning_rate": 0.0002860824742268041,
      "loss": 0.1579,
      "step": 665
    },
    {
      "epoch": 0.10730253353204174,
      "grad_norm": 1.8376573324203491,
      "learning_rate": 0.0002857603092783505,
      "loss": 0.1969,
      "step": 666
    },
    {
      "epoch": 0.107463648447255,
      "grad_norm": 3.611924648284912,
      "learning_rate": 0.0002854381443298969,
      "loss": 0.2469,
      "step": 667
    },
    {
      "epoch": 0.10762476336246828,
      "grad_norm": 1.2234561443328857,
      "learning_rate": 0.0002851159793814433,
      "loss": 0.1214,
      "step": 668
    },
    {
      "epoch": 0.10778587827768156,
      "grad_norm": 1.3551911115646362,
      "learning_rate": 0.00028479381443298967,
      "loss": 0.1405,
      "step": 669
    },
    {
      "epoch": 0.10794699319289483,
      "grad_norm": 2.627370595932007,
      "learning_rate": 0.0002844716494845361,
      "loss": 0.204,
      "step": 670
    },
    {
      "epoch": 0.10810810810810811,
      "grad_norm": 2.5594050884246826,
      "learning_rate": 0.00028414948453608246,
      "loss": 0.2089,
      "step": 671
    },
    {
      "epoch": 0.10826922302332138,
      "grad_norm": 2.0376017093658447,
      "learning_rate": 0.0002838273195876289,
      "loss": 0.1994,
      "step": 672
    },
    {
      "epoch": 0.10843033793853465,
      "grad_norm": 1.1219006776809692,
      "learning_rate": 0.00028350515463917525,
      "loss": 0.1268,
      "step": 673
    },
    {
      "epoch": 0.10859145285374794,
      "grad_norm": 2.10079288482666,
      "learning_rate": 0.00028318298969072167,
      "loss": 0.1776,
      "step": 674
    },
    {
      "epoch": 0.10875256776896121,
      "grad_norm": 1.4791629314422607,
      "learning_rate": 0.00028286082474226804,
      "loss": 0.1723,
      "step": 675
    },
    {
      "epoch": 0.10891368268417449,
      "grad_norm": 2.9015378952026367,
      "learning_rate": 0.00028253865979381446,
      "loss": 0.2158,
      "step": 676
    },
    {
      "epoch": 0.10907479759938776,
      "grad_norm": 1.8102740049362183,
      "learning_rate": 0.0002822164948453608,
      "loss": 0.1524,
      "step": 677
    },
    {
      "epoch": 0.10923591251460103,
      "grad_norm": 1.9519481658935547,
      "learning_rate": 0.00028189432989690724,
      "loss": 0.1552,
      "step": 678
    },
    {
      "epoch": 0.10939702742981432,
      "grad_norm": 1.7042193412780762,
      "learning_rate": 0.0002815721649484536,
      "loss": 0.1312,
      "step": 679
    },
    {
      "epoch": 0.10955814234502759,
      "grad_norm": 1.1441712379455566,
      "learning_rate": 0.00028125000000000003,
      "loss": 0.0923,
      "step": 680
    },
    {
      "epoch": 0.10971925726024087,
      "grad_norm": 1.3011009693145752,
      "learning_rate": 0.0002809278350515464,
      "loss": 0.1319,
      "step": 681
    },
    {
      "epoch": 0.10988037217545414,
      "grad_norm": 1.8286412954330444,
      "learning_rate": 0.00028060567010309277,
      "loss": 0.1672,
      "step": 682
    },
    {
      "epoch": 0.11004148709066743,
      "grad_norm": 2.1469533443450928,
      "learning_rate": 0.0002802835051546392,
      "loss": 0.1077,
      "step": 683
    },
    {
      "epoch": 0.1102026020058807,
      "grad_norm": 1.6251410245895386,
      "learning_rate": 0.00027996134020618555,
      "loss": 0.1421,
      "step": 684
    },
    {
      "epoch": 0.11036371692109397,
      "grad_norm": 2.4273762702941895,
      "learning_rate": 0.000279639175257732,
      "loss": 0.1874,
      "step": 685
    },
    {
      "epoch": 0.11052483183630725,
      "grad_norm": 2.2741923332214355,
      "learning_rate": 0.00027931701030927834,
      "loss": 0.2091,
      "step": 686
    },
    {
      "epoch": 0.11068594675152052,
      "grad_norm": 1.3897019624710083,
      "learning_rate": 0.00027899484536082476,
      "loss": 0.1178,
      "step": 687
    },
    {
      "epoch": 0.1108470616667338,
      "grad_norm": 1.5176284313201904,
      "learning_rate": 0.00027867268041237113,
      "loss": 0.1254,
      "step": 688
    },
    {
      "epoch": 0.11100817658194707,
      "grad_norm": 2.3698201179504395,
      "learning_rate": 0.00027835051546391755,
      "loss": 0.1482,
      "step": 689
    },
    {
      "epoch": 0.11116929149716034,
      "grad_norm": 2.1371676921844482,
      "learning_rate": 0.0002780283505154639,
      "loss": 0.1928,
      "step": 690
    },
    {
      "epoch": 0.11133040641237363,
      "grad_norm": 1.9171843528747559,
      "learning_rate": 0.00027770618556701034,
      "loss": 0.1741,
      "step": 691
    },
    {
      "epoch": 0.1114915213275869,
      "grad_norm": 1.966902256011963,
      "learning_rate": 0.0002773840206185567,
      "loss": 0.135,
      "step": 692
    },
    {
      "epoch": 0.11165263624280018,
      "grad_norm": 3.101173162460327,
      "learning_rate": 0.00027706185567010313,
      "loss": 0.2227,
      "step": 693
    },
    {
      "epoch": 0.11181375115801345,
      "grad_norm": 2.265672206878662,
      "learning_rate": 0.0002767396907216495,
      "loss": 0.1816,
      "step": 694
    },
    {
      "epoch": 0.11197486607322672,
      "grad_norm": 1.2850161790847778,
      "learning_rate": 0.0002764175257731959,
      "loss": 0.1161,
      "step": 695
    },
    {
      "epoch": 0.11213598098844,
      "grad_norm": 2.2734031677246094,
      "learning_rate": 0.0002760953608247423,
      "loss": 0.2389,
      "step": 696
    },
    {
      "epoch": 0.11229709590365328,
      "grad_norm": 4.862405776977539,
      "learning_rate": 0.0002757731958762887,
      "loss": 0.1969,
      "step": 697
    },
    {
      "epoch": 0.11245821081886656,
      "grad_norm": 1.8362352848052979,
      "learning_rate": 0.00027545103092783507,
      "loss": 0.1464,
      "step": 698
    },
    {
      "epoch": 0.11261932573407983,
      "grad_norm": 1.3163079023361206,
      "learning_rate": 0.0002751288659793815,
      "loss": 0.1467,
      "step": 699
    },
    {
      "epoch": 0.11278044064929311,
      "grad_norm": 1.9871023893356323,
      "learning_rate": 0.00027480670103092786,
      "loss": 0.1512,
      "step": 700
    },
    {
      "epoch": 0.11294155556450639,
      "grad_norm": 2.48968243598938,
      "learning_rate": 0.0002744845360824743,
      "loss": 0.2338,
      "step": 701
    },
    {
      "epoch": 0.11310267047971966,
      "grad_norm": 1.7719963788986206,
      "learning_rate": 0.00027416237113402065,
      "loss": 0.1326,
      "step": 702
    },
    {
      "epoch": 0.11326378539493294,
      "grad_norm": 1.6836256980895996,
      "learning_rate": 0.00027384020618556707,
      "loss": 0.1755,
      "step": 703
    },
    {
      "epoch": 0.11342490031014621,
      "grad_norm": 1.7413806915283203,
      "learning_rate": 0.00027351804123711343,
      "loss": 0.1765,
      "step": 704
    },
    {
      "epoch": 0.1135860152253595,
      "grad_norm": 1.4200917482376099,
      "learning_rate": 0.00027319587628865975,
      "loss": 0.1307,
      "step": 705
    },
    {
      "epoch": 0.11374713014057276,
      "grad_norm": 1.491337776184082,
      "learning_rate": 0.00027287371134020617,
      "loss": 0.1409,
      "step": 706
    },
    {
      "epoch": 0.11390824505578603,
      "grad_norm": 1.4282504320144653,
      "learning_rate": 0.00027255154639175253,
      "loss": 0.1886,
      "step": 707
    },
    {
      "epoch": 0.11406935997099932,
      "grad_norm": 1.5727230310440063,
      "learning_rate": 0.00027222938144329896,
      "loss": 0.1363,
      "step": 708
    },
    {
      "epoch": 0.11423047488621259,
      "grad_norm": 1.8581652641296387,
      "learning_rate": 0.0002719072164948453,
      "loss": 0.221,
      "step": 709
    },
    {
      "epoch": 0.11439158980142587,
      "grad_norm": 2.443208694458008,
      "learning_rate": 0.00027158505154639174,
      "loss": 0.1703,
      "step": 710
    },
    {
      "epoch": 0.11455270471663914,
      "grad_norm": 2.1062140464782715,
      "learning_rate": 0.0002712628865979381,
      "loss": 0.1533,
      "step": 711
    },
    {
      "epoch": 0.11471381963185241,
      "grad_norm": 1.416291356086731,
      "learning_rate": 0.00027094072164948453,
      "loss": 0.1258,
      "step": 712
    },
    {
      "epoch": 0.1148749345470657,
      "grad_norm": 2.375917673110962,
      "learning_rate": 0.0002706185567010309,
      "loss": 0.1339,
      "step": 713
    },
    {
      "epoch": 0.11503604946227897,
      "grad_norm": 2.2833499908447266,
      "learning_rate": 0.0002702963917525773,
      "loss": 0.2405,
      "step": 714
    },
    {
      "epoch": 0.11519716437749225,
      "grad_norm": 3.8151283264160156,
      "learning_rate": 0.0002699742268041237,
      "loss": 0.2883,
      "step": 715
    },
    {
      "epoch": 0.11535827929270552,
      "grad_norm": 1.6052703857421875,
      "learning_rate": 0.0002696520618556701,
      "loss": 0.1237,
      "step": 716
    },
    {
      "epoch": 0.1155193942079188,
      "grad_norm": 2.8441598415374756,
      "learning_rate": 0.0002693298969072165,
      "loss": 0.2376,
      "step": 717
    },
    {
      "epoch": 0.11568050912313207,
      "grad_norm": 1.3254661560058594,
      "learning_rate": 0.0002690077319587629,
      "loss": 0.1184,
      "step": 718
    },
    {
      "epoch": 0.11584162403834534,
      "grad_norm": 2.069483995437622,
      "learning_rate": 0.00026868556701030926,
      "loss": 0.1636,
      "step": 719
    },
    {
      "epoch": 0.11600273895355863,
      "grad_norm": 2.466902494430542,
      "learning_rate": 0.0002683634020618557,
      "loss": 0.1856,
      "step": 720
    },
    {
      "epoch": 0.1161638538687719,
      "grad_norm": 2.0698273181915283,
      "learning_rate": 0.00026804123711340205,
      "loss": 0.1398,
      "step": 721
    },
    {
      "epoch": 0.11632496878398518,
      "grad_norm": 1.2166014909744263,
      "learning_rate": 0.00026771907216494847,
      "loss": 0.1619,
      "step": 722
    },
    {
      "epoch": 0.11648608369919845,
      "grad_norm": 1.7274175882339478,
      "learning_rate": 0.00026739690721649484,
      "loss": 0.119,
      "step": 723
    },
    {
      "epoch": 0.11664719861441172,
      "grad_norm": 1.4100984334945679,
      "learning_rate": 0.00026707474226804126,
      "loss": 0.1416,
      "step": 724
    },
    {
      "epoch": 0.11680831352962501,
      "grad_norm": 2.224874258041382,
      "learning_rate": 0.0002667525773195876,
      "loss": 0.1152,
      "step": 725
    },
    {
      "epoch": 0.11696942844483828,
      "grad_norm": 1.951649785041809,
      "learning_rate": 0.00026643041237113405,
      "loss": 0.1525,
      "step": 726
    },
    {
      "epoch": 0.11713054336005156,
      "grad_norm": 2.8321938514709473,
      "learning_rate": 0.0002661082474226804,
      "loss": 0.1911,
      "step": 727
    },
    {
      "epoch": 0.11729165827526483,
      "grad_norm": 2.2436654567718506,
      "learning_rate": 0.00026578608247422684,
      "loss": 0.1809,
      "step": 728
    },
    {
      "epoch": 0.1174527731904781,
      "grad_norm": 2.27705717086792,
      "learning_rate": 0.0002654639175257732,
      "loss": 0.1956,
      "step": 729
    },
    {
      "epoch": 0.11761388810569139,
      "grad_norm": 2.606848955154419,
      "learning_rate": 0.00026514175257731957,
      "loss": 0.1498,
      "step": 730
    },
    {
      "epoch": 0.11777500302090466,
      "grad_norm": 2.221219539642334,
      "learning_rate": 0.000264819587628866,
      "loss": 0.1655,
      "step": 731
    },
    {
      "epoch": 0.11793611793611794,
      "grad_norm": 1.8663915395736694,
      "learning_rate": 0.00026449742268041236,
      "loss": 0.1679,
      "step": 732
    },
    {
      "epoch": 0.11809723285133121,
      "grad_norm": 1.5755195617675781,
      "learning_rate": 0.0002641752577319588,
      "loss": 0.1505,
      "step": 733
    },
    {
      "epoch": 0.1182583477665445,
      "grad_norm": 3.645608901977539,
      "learning_rate": 0.00026385309278350515,
      "loss": 0.0919,
      "step": 734
    },
    {
      "epoch": 0.11841946268175776,
      "grad_norm": 2.083085536956787,
      "learning_rate": 0.00026353092783505157,
      "loss": 0.1071,
      "step": 735
    },
    {
      "epoch": 0.11858057759697103,
      "grad_norm": 2.1221799850463867,
      "learning_rate": 0.00026320876288659793,
      "loss": 0.1504,
      "step": 736
    },
    {
      "epoch": 0.11874169251218432,
      "grad_norm": 2.6867873668670654,
      "learning_rate": 0.00026288659793814435,
      "loss": 0.1617,
      "step": 737
    },
    {
      "epoch": 0.11890280742739759,
      "grad_norm": 2.0653128623962402,
      "learning_rate": 0.0002625644329896907,
      "loss": 0.1894,
      "step": 738
    },
    {
      "epoch": 0.11906392234261087,
      "grad_norm": 2.4207308292388916,
      "learning_rate": 0.00026224226804123714,
      "loss": 0.2417,
      "step": 739
    },
    {
      "epoch": 0.11922503725782414,
      "grad_norm": 1.1078102588653564,
      "learning_rate": 0.0002619201030927835,
      "loss": 0.076,
      "step": 740
    },
    {
      "epoch": 0.11938615217303741,
      "grad_norm": 1.6296087503433228,
      "learning_rate": 0.00026159793814432993,
      "loss": 0.1252,
      "step": 741
    },
    {
      "epoch": 0.1195472670882507,
      "grad_norm": 1.8639334440231323,
      "learning_rate": 0.0002612757731958763,
      "loss": 0.1792,
      "step": 742
    },
    {
      "epoch": 0.11970838200346397,
      "grad_norm": 1.3746330738067627,
      "learning_rate": 0.0002609536082474227,
      "loss": 0.1645,
      "step": 743
    },
    {
      "epoch": 0.11986949691867725,
      "grad_norm": 1.3605178594589233,
      "learning_rate": 0.0002606314432989691,
      "loss": 0.1598,
      "step": 744
    },
    {
      "epoch": 0.12003061183389052,
      "grad_norm": 2.110149383544922,
      "learning_rate": 0.0002603092783505155,
      "loss": 0.1664,
      "step": 745
    },
    {
      "epoch": 0.12019172674910379,
      "grad_norm": 2.3310654163360596,
      "learning_rate": 0.0002599871134020619,
      "loss": 0.1322,
      "step": 746
    },
    {
      "epoch": 0.12035284166431708,
      "grad_norm": 2.53648042678833,
      "learning_rate": 0.0002596649484536083,
      "loss": 0.167,
      "step": 747
    },
    {
      "epoch": 0.12051395657953035,
      "grad_norm": 1.7218080759048462,
      "learning_rate": 0.00025934278350515466,
      "loss": 0.0875,
      "step": 748
    },
    {
      "epoch": 0.12067507149474363,
      "grad_norm": 1.6675161123275757,
      "learning_rate": 0.0002590206185567011,
      "loss": 0.1364,
      "step": 749
    },
    {
      "epoch": 0.1208361864099569,
      "grad_norm": 2.099011182785034,
      "learning_rate": 0.00025869845360824745,
      "loss": 0.1362,
      "step": 750
    },
    {
      "epoch": 0.12099730132517018,
      "grad_norm": 1.6289544105529785,
      "learning_rate": 0.00025837628865979387,
      "loss": 0.107,
      "step": 751
    },
    {
      "epoch": 0.12115841624038345,
      "grad_norm": 1.3011757135391235,
      "learning_rate": 0.00025805412371134024,
      "loss": 0.1087,
      "step": 752
    },
    {
      "epoch": 0.12131953115559672,
      "grad_norm": 2.128843307495117,
      "learning_rate": 0.00025773195876288655,
      "loss": 0.1614,
      "step": 753
    },
    {
      "epoch": 0.12148064607081001,
      "grad_norm": 2.060102701187134,
      "learning_rate": 0.00025740979381443297,
      "loss": 0.1329,
      "step": 754
    },
    {
      "epoch": 0.12164176098602328,
      "grad_norm": 1.4563082456588745,
      "learning_rate": 0.00025708762886597934,
      "loss": 0.1133,
      "step": 755
    },
    {
      "epoch": 0.12180287590123656,
      "grad_norm": 1.9002412557601929,
      "learning_rate": 0.00025676546391752576,
      "loss": 0.1254,
      "step": 756
    },
    {
      "epoch": 0.12196399081644983,
      "grad_norm": 3.2043237686157227,
      "learning_rate": 0.0002564432989690721,
      "loss": 0.2719,
      "step": 757
    },
    {
      "epoch": 0.1221251057316631,
      "grad_norm": 1.9266937971115112,
      "learning_rate": 0.00025612113402061855,
      "loss": 0.1122,
      "step": 758
    },
    {
      "epoch": 0.12228622064687639,
      "grad_norm": 1.2697731256484985,
      "learning_rate": 0.0002557989690721649,
      "loss": 0.0794,
      "step": 759
    },
    {
      "epoch": 0.12244733556208966,
      "grad_norm": 3.02462100982666,
      "learning_rate": 0.00025547680412371133,
      "loss": 0.123,
      "step": 760
    },
    {
      "epoch": 0.12260845047730294,
      "grad_norm": 2.543078899383545,
      "learning_rate": 0.0002551546391752577,
      "loss": 0.1468,
      "step": 761
    },
    {
      "epoch": 0.12276956539251621,
      "grad_norm": 2.519401788711548,
      "learning_rate": 0.0002548324742268041,
      "loss": 0.2134,
      "step": 762
    },
    {
      "epoch": 0.12293068030772948,
      "grad_norm": 2.5990915298461914,
      "learning_rate": 0.0002545103092783505,
      "loss": 0.2114,
      "step": 763
    },
    {
      "epoch": 0.12309179522294277,
      "grad_norm": 1.9964754581451416,
      "learning_rate": 0.0002541881443298969,
      "loss": 0.17,
      "step": 764
    },
    {
      "epoch": 0.12325291013815604,
      "grad_norm": 1.9156948328018188,
      "learning_rate": 0.0002538659793814433,
      "loss": 0.2037,
      "step": 765
    },
    {
      "epoch": 0.12341402505336932,
      "grad_norm": 1.9389487504959106,
      "learning_rate": 0.0002535438144329897,
      "loss": 0.161,
      "step": 766
    },
    {
      "epoch": 0.12357513996858259,
      "grad_norm": 1.3444401025772095,
      "learning_rate": 0.00025322164948453607,
      "loss": 0.1106,
      "step": 767
    },
    {
      "epoch": 0.12373625488379587,
      "grad_norm": 1.8416941165924072,
      "learning_rate": 0.0002528994845360825,
      "loss": 0.1381,
      "step": 768
    },
    {
      "epoch": 0.12389736979900914,
      "grad_norm": 1.3388251066207886,
      "learning_rate": 0.00025257731958762885,
      "loss": 0.2059,
      "step": 769
    },
    {
      "epoch": 0.12405848471422241,
      "grad_norm": 1.8742824792861938,
      "learning_rate": 0.0002522551546391753,
      "loss": 0.1695,
      "step": 770
    },
    {
      "epoch": 0.1242195996294357,
      "grad_norm": 1.4773002862930298,
      "learning_rate": 0.00025193298969072164,
      "loss": 0.1289,
      "step": 771
    },
    {
      "epoch": 0.12438071454464897,
      "grad_norm": 1.933445692062378,
      "learning_rate": 0.00025161082474226806,
      "loss": 0.1714,
      "step": 772
    },
    {
      "epoch": 0.12454182945986225,
      "grad_norm": 1.7411714792251587,
      "learning_rate": 0.00025128865979381443,
      "loss": 0.1303,
      "step": 773
    },
    {
      "epoch": 0.12470294437507552,
      "grad_norm": 1.9021544456481934,
      "learning_rate": 0.00025096649484536085,
      "loss": 0.1461,
      "step": 774
    },
    {
      "epoch": 0.12486405929028879,
      "grad_norm": 1.434914231300354,
      "learning_rate": 0.0002506443298969072,
      "loss": 0.1014,
      "step": 775
    },
    {
      "epoch": 0.12502517420550208,
      "grad_norm": 1.9347573518753052,
      "learning_rate": 0.00025032216494845364,
      "loss": 0.1082,
      "step": 776
    },
    {
      "epoch": 0.12518628912071536,
      "grad_norm": 2.513331651687622,
      "learning_rate": 0.00025,
      "loss": 0.2202,
      "step": 777
    },
    {
      "epoch": 0.12534740403592862,
      "grad_norm": 2.9721696376800537,
      "learning_rate": 0.00024967783505154637,
      "loss": 0.1792,
      "step": 778
    },
    {
      "epoch": 0.1255085189511419,
      "grad_norm": 2.032595157623291,
      "learning_rate": 0.0002493556701030928,
      "loss": 0.166,
      "step": 779
    },
    {
      "epoch": 0.12566963386635518,
      "grad_norm": 2.4145846366882324,
      "learning_rate": 0.00024903350515463916,
      "loss": 0.1804,
      "step": 780
    },
    {
      "epoch": 0.12583074878156844,
      "grad_norm": 2.585853338241577,
      "learning_rate": 0.0002487113402061856,
      "loss": 0.2185,
      "step": 781
    },
    {
      "epoch": 0.12599186369678173,
      "grad_norm": 1.9509526491165161,
      "learning_rate": 0.00024838917525773195,
      "loss": 0.0753,
      "step": 782
    },
    {
      "epoch": 0.126152978611995,
      "grad_norm": 1.7335056066513062,
      "learning_rate": 0.00024806701030927837,
      "loss": 0.0998,
      "step": 783
    },
    {
      "epoch": 0.1263140935272083,
      "grad_norm": 0.8579148054122925,
      "learning_rate": 0.00024774484536082474,
      "loss": 0.0875,
      "step": 784
    },
    {
      "epoch": 0.12647520844242155,
      "grad_norm": 2.2412586212158203,
      "learning_rate": 0.00024742268041237116,
      "loss": 0.1499,
      "step": 785
    },
    {
      "epoch": 0.12663632335763483,
      "grad_norm": 1.3183956146240234,
      "learning_rate": 0.0002471005154639175,
      "loss": 0.1379,
      "step": 786
    },
    {
      "epoch": 0.12679743827284812,
      "grad_norm": 1.7982814311981201,
      "learning_rate": 0.00024677835051546395,
      "loss": 0.1793,
      "step": 787
    },
    {
      "epoch": 0.12695855318806137,
      "grad_norm": 2.0053656101226807,
      "learning_rate": 0.0002464561855670103,
      "loss": 0.1822,
      "step": 788
    },
    {
      "epoch": 0.12711966810327466,
      "grad_norm": 1.6615681648254395,
      "learning_rate": 0.00024613402061855673,
      "loss": 0.1314,
      "step": 789
    },
    {
      "epoch": 0.12728078301848794,
      "grad_norm": 2.8802425861358643,
      "learning_rate": 0.0002458118556701031,
      "loss": 0.2103,
      "step": 790
    },
    {
      "epoch": 0.12744189793370123,
      "grad_norm": 1.9571069478988647,
      "learning_rate": 0.00024548969072164947,
      "loss": 0.1421,
      "step": 791
    },
    {
      "epoch": 0.12760301284891448,
      "grad_norm": 1.184903621673584,
      "learning_rate": 0.0002451675257731959,
      "loss": 0.0762,
      "step": 792
    },
    {
      "epoch": 0.12776412776412777,
      "grad_norm": 1.1590759754180908,
      "learning_rate": 0.00024484536082474225,
      "loss": 0.106,
      "step": 793
    },
    {
      "epoch": 0.12792524267934105,
      "grad_norm": 2.1445963382720947,
      "learning_rate": 0.0002445231958762887,
      "loss": 0.1503,
      "step": 794
    },
    {
      "epoch": 0.1280863575945543,
      "grad_norm": 1.770812749862671,
      "learning_rate": 0.00024420103092783504,
      "loss": 0.1918,
      "step": 795
    },
    {
      "epoch": 0.1282474725097676,
      "grad_norm": 1.709776759147644,
      "learning_rate": 0.00024387886597938144,
      "loss": 0.1175,
      "step": 796
    },
    {
      "epoch": 0.12840858742498087,
      "grad_norm": 1.613508701324463,
      "learning_rate": 0.00024355670103092783,
      "loss": 0.1572,
      "step": 797
    },
    {
      "epoch": 0.12856970234019413,
      "grad_norm": 1.620627999305725,
      "learning_rate": 0.00024323453608247422,
      "loss": 0.1118,
      "step": 798
    },
    {
      "epoch": 0.12873081725540741,
      "grad_norm": 3.156733512878418,
      "learning_rate": 0.00024291237113402062,
      "loss": 0.1841,
      "step": 799
    },
    {
      "epoch": 0.1288919321706207,
      "grad_norm": 1.8287280797958374,
      "learning_rate": 0.000242590206185567,
      "loss": 0.2483,
      "step": 800
    },
    {
      "epoch": 0.12905304708583398,
      "grad_norm": 1.7103444337844849,
      "learning_rate": 0.0002422680412371134,
      "loss": 0.1431,
      "step": 801
    },
    {
      "epoch": 0.12921416200104724,
      "grad_norm": 3.1917200088500977,
      "learning_rate": 0.0002419458762886598,
      "loss": 0.1421,
      "step": 802
    },
    {
      "epoch": 0.12937527691626052,
      "grad_norm": 2.4589438438415527,
      "learning_rate": 0.0002416237113402062,
      "loss": 0.1485,
      "step": 803
    },
    {
      "epoch": 0.1295363918314738,
      "grad_norm": 2.2397077083587646,
      "learning_rate": 0.0002413015463917526,
      "loss": 0.1167,
      "step": 804
    },
    {
      "epoch": 0.12969750674668706,
      "grad_norm": 2.06144380569458,
      "learning_rate": 0.00024097938144329898,
      "loss": 0.1703,
      "step": 805
    },
    {
      "epoch": 0.12985862166190035,
      "grad_norm": 1.992344856262207,
      "learning_rate": 0.00024065721649484538,
      "loss": 0.1813,
      "step": 806
    },
    {
      "epoch": 0.13001973657711363,
      "grad_norm": 2.2672464847564697,
      "learning_rate": 0.00024033505154639177,
      "loss": 0.1595,
      "step": 807
    },
    {
      "epoch": 0.13018085149232692,
      "grad_norm": 1.7087643146514893,
      "learning_rate": 0.00024001288659793816,
      "loss": 0.14,
      "step": 808
    },
    {
      "epoch": 0.13034196640754017,
      "grad_norm": 1.7062804698944092,
      "learning_rate": 0.00023969072164948456,
      "loss": 0.1714,
      "step": 809
    },
    {
      "epoch": 0.13050308132275346,
      "grad_norm": 2.009647846221924,
      "learning_rate": 0.00023936855670103095,
      "loss": 0.1604,
      "step": 810
    },
    {
      "epoch": 0.13066419623796674,
      "grad_norm": 1.7815383672714233,
      "learning_rate": 0.00023904639175257735,
      "loss": 0.1204,
      "step": 811
    },
    {
      "epoch": 0.13082531115318,
      "grad_norm": 2.0465869903564453,
      "learning_rate": 0.00023872422680412374,
      "loss": 0.1464,
      "step": 812
    },
    {
      "epoch": 0.13098642606839328,
      "grad_norm": 1.8748458623886108,
      "learning_rate": 0.00023840206185567013,
      "loss": 0.1155,
      "step": 813
    },
    {
      "epoch": 0.13114754098360656,
      "grad_norm": 1.6960129737854004,
      "learning_rate": 0.00023807989690721647,
      "loss": 0.1099,
      "step": 814
    },
    {
      "epoch": 0.13130865589881982,
      "grad_norm": 1.782641887664795,
      "learning_rate": 0.00023775773195876287,
      "loss": 0.1438,
      "step": 815
    },
    {
      "epoch": 0.1314697708140331,
      "grad_norm": 3.244131326675415,
      "learning_rate": 0.00023743556701030926,
      "loss": 0.1965,
      "step": 816
    },
    {
      "epoch": 0.1316308857292464,
      "grad_norm": 1.7400540113449097,
      "learning_rate": 0.00023711340206185566,
      "loss": 0.1081,
      "step": 817
    },
    {
      "epoch": 0.13179200064445967,
      "grad_norm": 1.4789224863052368,
      "learning_rate": 0.00023679123711340205,
      "loss": 0.1186,
      "step": 818
    },
    {
      "epoch": 0.13195311555967293,
      "grad_norm": 1.415061354637146,
      "learning_rate": 0.00023646907216494844,
      "loss": 0.1337,
      "step": 819
    },
    {
      "epoch": 0.1321142304748862,
      "grad_norm": 1.0569838285446167,
      "learning_rate": 0.00023614690721649484,
      "loss": 0.0664,
      "step": 820
    },
    {
      "epoch": 0.1322753453900995,
      "grad_norm": 1.7499175071716309,
      "learning_rate": 0.00023582474226804123,
      "loss": 0.1582,
      "step": 821
    },
    {
      "epoch": 0.13243646030531275,
      "grad_norm": 1.533321499824524,
      "learning_rate": 0.00023550257731958763,
      "loss": 0.1242,
      "step": 822
    },
    {
      "epoch": 0.13259757522052604,
      "grad_norm": 2.2150094509124756,
      "learning_rate": 0.00023518041237113402,
      "loss": 0.1748,
      "step": 823
    },
    {
      "epoch": 0.13275869013573932,
      "grad_norm": 2.734438896179199,
      "learning_rate": 0.00023485824742268041,
      "loss": 0.1288,
      "step": 824
    },
    {
      "epoch": 0.1329198050509526,
      "grad_norm": 3.355961322784424,
      "learning_rate": 0.0002345360824742268,
      "loss": 0.2573,
      "step": 825
    },
    {
      "epoch": 0.13308091996616586,
      "grad_norm": 1.8711096048355103,
      "learning_rate": 0.0002342139175257732,
      "loss": 0.1623,
      "step": 826
    },
    {
      "epoch": 0.13324203488137915,
      "grad_norm": 1.637799620628357,
      "learning_rate": 0.0002338917525773196,
      "loss": 0.1435,
      "step": 827
    },
    {
      "epoch": 0.13340314979659243,
      "grad_norm": 1.5036208629608154,
      "learning_rate": 0.000233569587628866,
      "loss": 0.1103,
      "step": 828
    },
    {
      "epoch": 0.13356426471180569,
      "grad_norm": 1.6976414918899536,
      "learning_rate": 0.00023324742268041238,
      "loss": 0.1454,
      "step": 829
    },
    {
      "epoch": 0.13372537962701897,
      "grad_norm": 1.3802409172058105,
      "learning_rate": 0.00023292525773195878,
      "loss": 0.1414,
      "step": 830
    },
    {
      "epoch": 0.13388649454223225,
      "grad_norm": 1.5983562469482422,
      "learning_rate": 0.00023260309278350517,
      "loss": 0.0977,
      "step": 831
    },
    {
      "epoch": 0.1340476094574455,
      "grad_norm": 1.6430600881576538,
      "learning_rate": 0.00023228092783505157,
      "loss": 0.1238,
      "step": 832
    },
    {
      "epoch": 0.1342087243726588,
      "grad_norm": 2.196575403213501,
      "learning_rate": 0.00023195876288659796,
      "loss": 0.1307,
      "step": 833
    },
    {
      "epoch": 0.13436983928787208,
      "grad_norm": 1.7936656475067139,
      "learning_rate": 0.00023163659793814435,
      "loss": 0.1787,
      "step": 834
    },
    {
      "epoch": 0.13453095420308536,
      "grad_norm": 1.3441658020019531,
      "learning_rate": 0.00023131443298969075,
      "loss": 0.0936,
      "step": 835
    },
    {
      "epoch": 0.13469206911829862,
      "grad_norm": 2.394791841506958,
      "learning_rate": 0.00023099226804123714,
      "loss": 0.1139,
      "step": 836
    },
    {
      "epoch": 0.1348531840335119,
      "grad_norm": 1.6803717613220215,
      "learning_rate": 0.00023067010309278354,
      "loss": 0.1408,
      "step": 837
    },
    {
      "epoch": 0.1350142989487252,
      "grad_norm": 1.916568398475647,
      "learning_rate": 0.00023034793814432988,
      "loss": 0.1508,
      "step": 838
    },
    {
      "epoch": 0.13517541386393844,
      "grad_norm": 1.9556739330291748,
      "learning_rate": 0.00023002577319587627,
      "loss": 0.0966,
      "step": 839
    },
    {
      "epoch": 0.13533652877915173,
      "grad_norm": 2.9043076038360596,
      "learning_rate": 0.00022970360824742266,
      "loss": 0.1608,
      "step": 840
    },
    {
      "epoch": 0.135497643694365,
      "grad_norm": 2.312708854675293,
      "learning_rate": 0.00022938144329896906,
      "loss": 0.2001,
      "step": 841
    },
    {
      "epoch": 0.1356587586095783,
      "grad_norm": 2.966313600540161,
      "learning_rate": 0.00022905927835051545,
      "loss": 0.2339,
      "step": 842
    },
    {
      "epoch": 0.13581987352479155,
      "grad_norm": 2.0800273418426514,
      "learning_rate": 0.00022873711340206185,
      "loss": 0.1607,
      "step": 843
    },
    {
      "epoch": 0.13598098844000484,
      "grad_norm": 1.8609236478805542,
      "learning_rate": 0.00022841494845360824,
      "loss": 0.159,
      "step": 844
    },
    {
      "epoch": 0.13614210335521812,
      "grad_norm": 2.276252508163452,
      "learning_rate": 0.00022809278350515463,
      "loss": 0.1462,
      "step": 845
    },
    {
      "epoch": 0.13630321827043138,
      "grad_norm": 1.1777981519699097,
      "learning_rate": 0.00022777061855670103,
      "loss": 0.1561,
      "step": 846
    },
    {
      "epoch": 0.13646433318564466,
      "grad_norm": 1.212530255317688,
      "learning_rate": 0.00022744845360824742,
      "loss": 0.1165,
      "step": 847
    },
    {
      "epoch": 0.13662544810085794,
      "grad_norm": 1.2903705835342407,
      "learning_rate": 0.00022712628865979382,
      "loss": 0.1042,
      "step": 848
    },
    {
      "epoch": 0.1367865630160712,
      "grad_norm": 2.3084230422973633,
      "learning_rate": 0.0002268041237113402,
      "loss": 0.2334,
      "step": 849
    },
    {
      "epoch": 0.13694767793128448,
      "grad_norm": 1.8732017278671265,
      "learning_rate": 0.0002264819587628866,
      "loss": 0.1121,
      "step": 850
    },
    {
      "epoch": 0.13710879284649777,
      "grad_norm": 1.7462939023971558,
      "learning_rate": 0.000226159793814433,
      "loss": 0.1611,
      "step": 851
    },
    {
      "epoch": 0.13726990776171105,
      "grad_norm": 2.5752406120300293,
      "learning_rate": 0.0002258376288659794,
      "loss": 0.1654,
      "step": 852
    },
    {
      "epoch": 0.1374310226769243,
      "grad_norm": 2.572725772857666,
      "learning_rate": 0.00022551546391752579,
      "loss": 0.1442,
      "step": 853
    },
    {
      "epoch": 0.1375921375921376,
      "grad_norm": 2.033348560333252,
      "learning_rate": 0.00022519329896907218,
      "loss": 0.1021,
      "step": 854
    },
    {
      "epoch": 0.13775325250735088,
      "grad_norm": 2.5029919147491455,
      "learning_rate": 0.00022487113402061857,
      "loss": 0.125,
      "step": 855
    },
    {
      "epoch": 0.13791436742256413,
      "grad_norm": 1.6739435195922852,
      "learning_rate": 0.00022454896907216497,
      "loss": 0.1411,
      "step": 856
    },
    {
      "epoch": 0.13807548233777742,
      "grad_norm": 1.56549072265625,
      "learning_rate": 0.00022422680412371136,
      "loss": 0.1459,
      "step": 857
    },
    {
      "epoch": 0.1382365972529907,
      "grad_norm": 2.3510191440582275,
      "learning_rate": 0.00022390463917525776,
      "loss": 0.1345,
      "step": 858
    },
    {
      "epoch": 0.13839771216820398,
      "grad_norm": 2.4032480716705322,
      "learning_rate": 0.00022358247422680415,
      "loss": 0.0982,
      "step": 859
    },
    {
      "epoch": 0.13855882708341724,
      "grad_norm": 0.9738886952400208,
      "learning_rate": 0.00022326030927835054,
      "loss": 0.0979,
      "step": 860
    },
    {
      "epoch": 0.13871994199863052,
      "grad_norm": 1.405638575553894,
      "learning_rate": 0.00022293814432989694,
      "loss": 0.1,
      "step": 861
    },
    {
      "epoch": 0.1388810569138438,
      "grad_norm": 1.9160175323486328,
      "learning_rate": 0.00022261597938144328,
      "loss": 0.1998,
      "step": 862
    },
    {
      "epoch": 0.13904217182905707,
      "grad_norm": 1.64534592628479,
      "learning_rate": 0.00022229381443298967,
      "loss": 0.1561,
      "step": 863
    },
    {
      "epoch": 0.13920328674427035,
      "grad_norm": 2.293637275695801,
      "learning_rate": 0.00022197164948453606,
      "loss": 0.1225,
      "step": 864
    },
    {
      "epoch": 0.13936440165948363,
      "grad_norm": 1.1159061193466187,
      "learning_rate": 0.00022164948453608246,
      "loss": 0.0946,
      "step": 865
    },
    {
      "epoch": 0.1395255165746969,
      "grad_norm": 1.6142464876174927,
      "learning_rate": 0.00022132731958762885,
      "loss": 0.0908,
      "step": 866
    },
    {
      "epoch": 0.13968663148991017,
      "grad_norm": 2.28985595703125,
      "learning_rate": 0.00022100515463917525,
      "loss": 0.1588,
      "step": 867
    },
    {
      "epoch": 0.13984774640512346,
      "grad_norm": 1.3495581150054932,
      "learning_rate": 0.00022068298969072164,
      "loss": 0.0837,
      "step": 868
    },
    {
      "epoch": 0.14000886132033674,
      "grad_norm": 2.1083920001983643,
      "learning_rate": 0.00022036082474226803,
      "loss": 0.0969,
      "step": 869
    },
    {
      "epoch": 0.14016997623555,
      "grad_norm": 1.737335205078125,
      "learning_rate": 0.00022003865979381443,
      "loss": 0.1785,
      "step": 870
    },
    {
      "epoch": 0.14033109115076328,
      "grad_norm": 1.4190566539764404,
      "learning_rate": 0.00021971649484536082,
      "loss": 0.1417,
      "step": 871
    },
    {
      "epoch": 0.14049220606597657,
      "grad_norm": 1.245225429534912,
      "learning_rate": 0.00021939432989690722,
      "loss": 0.1137,
      "step": 872
    },
    {
      "epoch": 0.14065332098118982,
      "grad_norm": 2.080735445022583,
      "learning_rate": 0.0002190721649484536,
      "loss": 0.1749,
      "step": 873
    },
    {
      "epoch": 0.1408144358964031,
      "grad_norm": 2.424078941345215,
      "learning_rate": 0.00021875,
      "loss": 0.1499,
      "step": 874
    },
    {
      "epoch": 0.1409755508116164,
      "grad_norm": 1.2441693544387817,
      "learning_rate": 0.0002184278350515464,
      "loss": 0.0977,
      "step": 875
    },
    {
      "epoch": 0.14113666572682967,
      "grad_norm": 1.4444559812545776,
      "learning_rate": 0.0002181056701030928,
      "loss": 0.1602,
      "step": 876
    },
    {
      "epoch": 0.14129778064204293,
      "grad_norm": 1.467107892036438,
      "learning_rate": 0.0002177835051546392,
      "loss": 0.0914,
      "step": 877
    },
    {
      "epoch": 0.14145889555725621,
      "grad_norm": 2.2512149810791016,
      "learning_rate": 0.00021746134020618558,
      "loss": 0.089,
      "step": 878
    },
    {
      "epoch": 0.1416200104724695,
      "grad_norm": 1.171021819114685,
      "learning_rate": 0.00021713917525773197,
      "loss": 0.0961,
      "step": 879
    },
    {
      "epoch": 0.14178112538768275,
      "grad_norm": 2.4924232959747314,
      "learning_rate": 0.00021681701030927837,
      "loss": 0.1067,
      "step": 880
    },
    {
      "epoch": 0.14194224030289604,
      "grad_norm": 2.976240873336792,
      "learning_rate": 0.00021649484536082476,
      "loss": 0.219,
      "step": 881
    },
    {
      "epoch": 0.14210335521810932,
      "grad_norm": 1.7929127216339111,
      "learning_rate": 0.00021617268041237116,
      "loss": 0.13,
      "step": 882
    },
    {
      "epoch": 0.14226447013332258,
      "grad_norm": 1.83296799659729,
      "learning_rate": 0.00021585051546391755,
      "loss": 0.0789,
      "step": 883
    },
    {
      "epoch": 0.14242558504853586,
      "grad_norm": 1.3968784809112549,
      "learning_rate": 0.00021552835051546394,
      "loss": 0.0725,
      "step": 884
    },
    {
      "epoch": 0.14258669996374915,
      "grad_norm": 1.3780434131622314,
      "learning_rate": 0.00021520618556701034,
      "loss": 0.1146,
      "step": 885
    },
    {
      "epoch": 0.14274781487896243,
      "grad_norm": 2.1176576614379883,
      "learning_rate": 0.00021488402061855673,
      "loss": 0.2118,
      "step": 886
    },
    {
      "epoch": 0.1429089297941757,
      "grad_norm": 2.4698243141174316,
      "learning_rate": 0.00021456185567010307,
      "loss": 0.25,
      "step": 887
    },
    {
      "epoch": 0.14307004470938897,
      "grad_norm": 1.685522437095642,
      "learning_rate": 0.00021423969072164947,
      "loss": 0.1433,
      "step": 888
    },
    {
      "epoch": 0.14323115962460226,
      "grad_norm": 1.7868047952651978,
      "learning_rate": 0.00021391752577319586,
      "loss": 0.1938,
      "step": 889
    },
    {
      "epoch": 0.1433922745398155,
      "grad_norm": 1.5655566453933716,
      "learning_rate": 0.00021359536082474225,
      "loss": 0.0927,
      "step": 890
    },
    {
      "epoch": 0.1435533894550288,
      "grad_norm": 1.7419627904891968,
      "learning_rate": 0.00021327319587628865,
      "loss": 0.1332,
      "step": 891
    },
    {
      "epoch": 0.14371450437024208,
      "grad_norm": 2.1293227672576904,
      "learning_rate": 0.00021295103092783504,
      "loss": 0.1569,
      "step": 892
    },
    {
      "epoch": 0.14387561928545536,
      "grad_norm": 1.8256921768188477,
      "learning_rate": 0.00021262886597938144,
      "loss": 0.1135,
      "step": 893
    },
    {
      "epoch": 0.14403673420066862,
      "grad_norm": 2.2023394107818604,
      "learning_rate": 0.00021230670103092783,
      "loss": 0.1426,
      "step": 894
    },
    {
      "epoch": 0.1441978491158819,
      "grad_norm": 2.137573480606079,
      "learning_rate": 0.00021198453608247422,
      "loss": 0.0989,
      "step": 895
    },
    {
      "epoch": 0.1443589640310952,
      "grad_norm": 1.733633279800415,
      "learning_rate": 0.00021166237113402062,
      "loss": 0.1245,
      "step": 896
    },
    {
      "epoch": 0.14452007894630844,
      "grad_norm": 1.8653674125671387,
      "learning_rate": 0.000211340206185567,
      "loss": 0.1461,
      "step": 897
    },
    {
      "epoch": 0.14468119386152173,
      "grad_norm": 1.6503171920776367,
      "learning_rate": 0.0002110180412371134,
      "loss": 0.1038,
      "step": 898
    },
    {
      "epoch": 0.144842308776735,
      "grad_norm": 1.5938527584075928,
      "learning_rate": 0.0002106958762886598,
      "loss": 0.0851,
      "step": 899
    },
    {
      "epoch": 0.14500342369194827,
      "grad_norm": 2.5007266998291016,
      "learning_rate": 0.0002103737113402062,
      "loss": 0.1712,
      "step": 900
    },
    {
      "epoch": 0.14516453860716155,
      "grad_norm": 1.7114955186843872,
      "learning_rate": 0.0002100515463917526,
      "loss": 0.1335,
      "step": 901
    },
    {
      "epoch": 0.14532565352237484,
      "grad_norm": 1.8453534841537476,
      "learning_rate": 0.00020972938144329898,
      "loss": 0.1034,
      "step": 902
    },
    {
      "epoch": 0.14548676843758812,
      "grad_norm": 1.5941051244735718,
      "learning_rate": 0.00020940721649484538,
      "loss": 0.1338,
      "step": 903
    },
    {
      "epoch": 0.14564788335280138,
      "grad_norm": 2.148385524749756,
      "learning_rate": 0.00020908505154639177,
      "loss": 0.1442,
      "step": 904
    },
    {
      "epoch": 0.14580899826801466,
      "grad_norm": 1.6432502269744873,
      "learning_rate": 0.00020876288659793816,
      "loss": 0.0995,
      "step": 905
    },
    {
      "epoch": 0.14597011318322795,
      "grad_norm": 0.9052758812904358,
      "learning_rate": 0.00020844072164948456,
      "loss": 0.0765,
      "step": 906
    },
    {
      "epoch": 0.1461312280984412,
      "grad_norm": 0.8630697131156921,
      "learning_rate": 0.00020811855670103095,
      "loss": 0.081,
      "step": 907
    },
    {
      "epoch": 0.14629234301365449,
      "grad_norm": 1.7264975309371948,
      "learning_rate": 0.00020779639175257735,
      "loss": 0.1067,
      "step": 908
    },
    {
      "epoch": 0.14645345792886777,
      "grad_norm": 1.6727919578552246,
      "learning_rate": 0.00020747422680412374,
      "loss": 0.1617,
      "step": 909
    },
    {
      "epoch": 0.14661457284408105,
      "grad_norm": 1.7053501605987549,
      "learning_rate": 0.00020715206185567013,
      "loss": 0.1412,
      "step": 910
    },
    {
      "epoch": 0.1467756877592943,
      "grad_norm": 2.8225972652435303,
      "learning_rate": 0.00020682989690721647,
      "loss": 0.2385,
      "step": 911
    },
    {
      "epoch": 0.1469368026745076,
      "grad_norm": 1.196656346321106,
      "learning_rate": 0.00020650773195876287,
      "loss": 0.0888,
      "step": 912
    },
    {
      "epoch": 0.14709791758972088,
      "grad_norm": 1.4081782102584839,
      "learning_rate": 0.00020618556701030926,
      "loss": 0.0978,
      "step": 913
    },
    {
      "epoch": 0.14725903250493413,
      "grad_norm": 2.1169650554656982,
      "learning_rate": 0.00020586340206185566,
      "loss": 0.1934,
      "step": 914
    },
    {
      "epoch": 0.14742014742014742,
      "grad_norm": 1.827533483505249,
      "learning_rate": 0.00020554123711340205,
      "loss": 0.2099,
      "step": 915
    },
    {
      "epoch": 0.1475812623353607,
      "grad_norm": 3.48864483833313,
      "learning_rate": 0.00020521907216494844,
      "loss": 0.1987,
      "step": 916
    },
    {
      "epoch": 0.14774237725057396,
      "grad_norm": 1.4235296249389648,
      "learning_rate": 0.00020489690721649484,
      "loss": 0.1043,
      "step": 917
    },
    {
      "epoch": 0.14790349216578724,
      "grad_norm": 1.7662880420684814,
      "learning_rate": 0.00020457474226804123,
      "loss": 0.1206,
      "step": 918
    },
    {
      "epoch": 0.14806460708100053,
      "grad_norm": 1.9639897346496582,
      "learning_rate": 0.00020425257731958763,
      "loss": 0.1244,
      "step": 919
    },
    {
      "epoch": 0.1482257219962138,
      "grad_norm": 1.5764206647872925,
      "learning_rate": 0.00020393041237113402,
      "loss": 0.1372,
      "step": 920
    },
    {
      "epoch": 0.14838683691142707,
      "grad_norm": 1.61930513381958,
      "learning_rate": 0.0002036082474226804,
      "loss": 0.1233,
      "step": 921
    },
    {
      "epoch": 0.14854795182664035,
      "grad_norm": 1.0198001861572266,
      "learning_rate": 0.0002032860824742268,
      "loss": 0.0739,
      "step": 922
    },
    {
      "epoch": 0.14870906674185363,
      "grad_norm": 1.4359426498413086,
      "learning_rate": 0.0002029639175257732,
      "loss": 0.089,
      "step": 923
    },
    {
      "epoch": 0.1488701816570669,
      "grad_norm": 1.7530847787857056,
      "learning_rate": 0.0002026417525773196,
      "loss": 0.153,
      "step": 924
    },
    {
      "epoch": 0.14903129657228018,
      "grad_norm": 1.78721284866333,
      "learning_rate": 0.000202319587628866,
      "loss": 0.1084,
      "step": 925
    },
    {
      "epoch": 0.14919241148749346,
      "grad_norm": 1.930497169494629,
      "learning_rate": 0.00020199742268041238,
      "loss": 0.1213,
      "step": 926
    },
    {
      "epoch": 0.14935352640270674,
      "grad_norm": 1.3684757947921753,
      "learning_rate": 0.00020167525773195878,
      "loss": 0.1355,
      "step": 927
    },
    {
      "epoch": 0.14951464131792,
      "grad_norm": 2.30216121673584,
      "learning_rate": 0.00020135309278350517,
      "loss": 0.1649,
      "step": 928
    },
    {
      "epoch": 0.14967575623313328,
      "grad_norm": 1.2949128150939941,
      "learning_rate": 0.00020103092783505157,
      "loss": 0.0777,
      "step": 929
    },
    {
      "epoch": 0.14983687114834657,
      "grad_norm": 2.3587872982025146,
      "learning_rate": 0.00020070876288659796,
      "loss": 0.2114,
      "step": 930
    },
    {
      "epoch": 0.14999798606355982,
      "grad_norm": 2.1059679985046387,
      "learning_rate": 0.00020038659793814435,
      "loss": 0.0932,
      "step": 931
    },
    {
      "epoch": 0.1501591009787731,
      "grad_norm": 1.081067442893982,
      "learning_rate": 0.00020006443298969075,
      "loss": 0.1336,
      "step": 932
    },
    {
      "epoch": 0.1503202158939864,
      "grad_norm": 1.6256107091903687,
      "learning_rate": 0.00019974226804123714,
      "loss": 0.122,
      "step": 933
    },
    {
      "epoch": 0.15048133080919965,
      "grad_norm": 0.8975231647491455,
      "learning_rate": 0.00019942010309278354,
      "loss": 0.0788,
      "step": 934
    },
    {
      "epoch": 0.15064244572441293,
      "grad_norm": 3.1858978271484375,
      "learning_rate": 0.00019909793814432987,
      "loss": 0.1628,
      "step": 935
    },
    {
      "epoch": 0.15080356063962622,
      "grad_norm": 0.8925537467002869,
      "learning_rate": 0.00019877577319587627,
      "loss": 0.0739,
      "step": 936
    },
    {
      "epoch": 0.1509646755548395,
      "grad_norm": 1.2304127216339111,
      "learning_rate": 0.00019845360824742266,
      "loss": 0.0913,
      "step": 937
    },
    {
      "epoch": 0.15112579047005276,
      "grad_norm": 1.8190088272094727,
      "learning_rate": 0.00019813144329896906,
      "loss": 0.15,
      "step": 938
    },
    {
      "epoch": 0.15128690538526604,
      "grad_norm": 1.3795825242996216,
      "learning_rate": 0.00019780927835051545,
      "loss": 0.1109,
      "step": 939
    },
    {
      "epoch": 0.15144802030047932,
      "grad_norm": 1.8369886875152588,
      "learning_rate": 0.00019748711340206184,
      "loss": 0.1403,
      "step": 940
    },
    {
      "epoch": 0.15160913521569258,
      "grad_norm": 1.7034525871276855,
      "learning_rate": 0.00019716494845360824,
      "loss": 0.1199,
      "step": 941
    },
    {
      "epoch": 0.15177025013090586,
      "grad_norm": 1.579166054725647,
      "learning_rate": 0.00019684278350515463,
      "loss": 0.1381,
      "step": 942
    },
    {
      "epoch": 0.15193136504611915,
      "grad_norm": 2.7142996788024902,
      "learning_rate": 0.00019652061855670103,
      "loss": 0.1884,
      "step": 943
    },
    {
      "epoch": 0.15209247996133243,
      "grad_norm": 1.9781032800674438,
      "learning_rate": 0.00019619845360824742,
      "loss": 0.0563,
      "step": 944
    },
    {
      "epoch": 0.1522535948765457,
      "grad_norm": 1.2932920455932617,
      "learning_rate": 0.00019587628865979381,
      "loss": 0.0971,
      "step": 945
    },
    {
      "epoch": 0.15241470979175897,
      "grad_norm": 1.5274370908737183,
      "learning_rate": 0.0001955541237113402,
      "loss": 0.163,
      "step": 946
    },
    {
      "epoch": 0.15257582470697226,
      "grad_norm": 1.9057679176330566,
      "learning_rate": 0.0001952319587628866,
      "loss": 0.121,
      "step": 947
    },
    {
      "epoch": 0.1527369396221855,
      "grad_norm": 1.1201491355895996,
      "learning_rate": 0.000194909793814433,
      "loss": 0.1204,
      "step": 948
    },
    {
      "epoch": 0.1528980545373988,
      "grad_norm": 1.7204896211624146,
      "learning_rate": 0.0001945876288659794,
      "loss": 0.2113,
      "step": 949
    },
    {
      "epoch": 0.15305916945261208,
      "grad_norm": 2.0189480781555176,
      "learning_rate": 0.00019426546391752578,
      "loss": 0.1291,
      "step": 950
    },
    {
      "epoch": 0.15322028436782534,
      "grad_norm": 1.994733214378357,
      "learning_rate": 0.00019394329896907218,
      "loss": 0.1584,
      "step": 951
    },
    {
      "epoch": 0.15338139928303862,
      "grad_norm": 2.935614585876465,
      "learning_rate": 0.00019362113402061857,
      "loss": 0.1496,
      "step": 952
    },
    {
      "epoch": 0.1535425141982519,
      "grad_norm": 1.0693110227584839,
      "learning_rate": 0.00019329896907216497,
      "loss": 0.0646,
      "step": 953
    },
    {
      "epoch": 0.1537036291134652,
      "grad_norm": 1.9896252155303955,
      "learning_rate": 0.00019297680412371136,
      "loss": 0.1318,
      "step": 954
    },
    {
      "epoch": 0.15386474402867845,
      "grad_norm": 1.7445577383041382,
      "learning_rate": 0.00019265463917525775,
      "loss": 0.2259,
      "step": 955
    },
    {
      "epoch": 0.15402585894389173,
      "grad_norm": 1.4432685375213623,
      "learning_rate": 0.00019233247422680415,
      "loss": 0.0921,
      "step": 956
    },
    {
      "epoch": 0.15418697385910501,
      "grad_norm": 1.931213140487671,
      "learning_rate": 0.00019201030927835054,
      "loss": 0.2377,
      "step": 957
    },
    {
      "epoch": 0.15434808877431827,
      "grad_norm": 1.255178451538086,
      "learning_rate": 0.00019168814432989694,
      "loss": 0.105,
      "step": 958
    },
    {
      "epoch": 0.15450920368953155,
      "grad_norm": 1.7266089916229248,
      "learning_rate": 0.00019136597938144328,
      "loss": 0.1833,
      "step": 959
    },
    {
      "epoch": 0.15467031860474484,
      "grad_norm": 2.3070905208587646,
      "learning_rate": 0.00019104381443298967,
      "loss": 0.2103,
      "step": 960
    },
    {
      "epoch": 0.15483143351995812,
      "grad_norm": 2.261488914489746,
      "learning_rate": 0.00019072164948453606,
      "loss": 0.1276,
      "step": 961
    },
    {
      "epoch": 0.15499254843517138,
      "grad_norm": 1.826197862625122,
      "learning_rate": 0.00019039948453608246,
      "loss": 0.1186,
      "step": 962
    },
    {
      "epoch": 0.15515366335038466,
      "grad_norm": 2.009351968765259,
      "learning_rate": 0.00019007731958762885,
      "loss": 0.1371,
      "step": 963
    },
    {
      "epoch": 0.15531477826559795,
      "grad_norm": 2.213059663772583,
      "learning_rate": 0.00018975515463917525,
      "loss": 0.1313,
      "step": 964
    },
    {
      "epoch": 0.1554758931808112,
      "grad_norm": 2.2001631259918213,
      "learning_rate": 0.00018943298969072164,
      "loss": 0.2211,
      "step": 965
    },
    {
      "epoch": 0.1556370080960245,
      "grad_norm": 1.7286280393600464,
      "learning_rate": 0.00018911082474226803,
      "loss": 0.0888,
      "step": 966
    },
    {
      "epoch": 0.15579812301123777,
      "grad_norm": 2.162631034851074,
      "learning_rate": 0.00018878865979381443,
      "loss": 0.1674,
      "step": 967
    },
    {
      "epoch": 0.15595923792645103,
      "grad_norm": 1.8636306524276733,
      "learning_rate": 0.00018846649484536082,
      "loss": 0.1388,
      "step": 968
    },
    {
      "epoch": 0.1561203528416643,
      "grad_norm": 2.5636062622070312,
      "learning_rate": 0.00018814432989690722,
      "loss": 0.14,
      "step": 969
    },
    {
      "epoch": 0.1562814677568776,
      "grad_norm": 2.650369882583618,
      "learning_rate": 0.0001878221649484536,
      "loss": 0.1228,
      "step": 970
    },
    {
      "epoch": 0.15644258267209088,
      "grad_norm": 1.8414753675460815,
      "learning_rate": 0.0001875,
      "loss": 0.0994,
      "step": 971
    },
    {
      "epoch": 0.15660369758730414,
      "grad_norm": 1.1310827732086182,
      "learning_rate": 0.0001871778350515464,
      "loss": 0.1062,
      "step": 972
    },
    {
      "epoch": 0.15676481250251742,
      "grad_norm": 1.4958826303482056,
      "learning_rate": 0.0001868556701030928,
      "loss": 0.1825,
      "step": 973
    },
    {
      "epoch": 0.1569259274177307,
      "grad_norm": 2.36222505569458,
      "learning_rate": 0.00018653350515463919,
      "loss": 0.1591,
      "step": 974
    },
    {
      "epoch": 0.15708704233294396,
      "grad_norm": 1.4015518426895142,
      "learning_rate": 0.00018621134020618558,
      "loss": 0.1229,
      "step": 975
    },
    {
      "epoch": 0.15724815724815724,
      "grad_norm": 2.3001372814178467,
      "learning_rate": 0.00018588917525773197,
      "loss": 0.189,
      "step": 976
    },
    {
      "epoch": 0.15740927216337053,
      "grad_norm": 0.8892154693603516,
      "learning_rate": 0.00018556701030927837,
      "loss": 0.0696,
      "step": 977
    },
    {
      "epoch": 0.1575703870785838,
      "grad_norm": 1.6188021898269653,
      "learning_rate": 0.00018524484536082476,
      "loss": 0.1233,
      "step": 978
    },
    {
      "epoch": 0.15773150199379707,
      "grad_norm": 2.181300163269043,
      "learning_rate": 0.00018492268041237116,
      "loss": 0.161,
      "step": 979
    },
    {
      "epoch": 0.15789261690901035,
      "grad_norm": 1.5301238298416138,
      "learning_rate": 0.00018460051546391755,
      "loss": 0.1772,
      "step": 980
    },
    {
      "epoch": 0.15805373182422364,
      "grad_norm": 1.6285884380340576,
      "learning_rate": 0.00018427835051546394,
      "loss": 0.1485,
      "step": 981
    },
    {
      "epoch": 0.1582148467394369,
      "grad_norm": 2.355179786682129,
      "learning_rate": 0.00018395618556701034,
      "loss": 0.1528,
      "step": 982
    },
    {
      "epoch": 0.15837596165465018,
      "grad_norm": 1.4924250841140747,
      "learning_rate": 0.00018363402061855673,
      "loss": 0.1361,
      "step": 983
    },
    {
      "epoch": 0.15853707656986346,
      "grad_norm": 1.7952061891555786,
      "learning_rate": 0.00018331185567010307,
      "loss": 0.2211,
      "step": 984
    },
    {
      "epoch": 0.15869819148507672,
      "grad_norm": 1.601462721824646,
      "learning_rate": 0.00018298969072164947,
      "loss": 0.0981,
      "step": 985
    },
    {
      "epoch": 0.15885930640029,
      "grad_norm": 2.7662971019744873,
      "learning_rate": 0.00018266752577319586,
      "loss": 0.1487,
      "step": 986
    },
    {
      "epoch": 0.15902042131550329,
      "grad_norm": 2.1921238899230957,
      "learning_rate": 0.00018234536082474225,
      "loss": 0.142,
      "step": 987
    },
    {
      "epoch": 0.15918153623071657,
      "grad_norm": 1.8970216512680054,
      "learning_rate": 0.00018202319587628865,
      "loss": 0.1734,
      "step": 988
    },
    {
      "epoch": 0.15934265114592983,
      "grad_norm": 1.8144289255142212,
      "learning_rate": 0.00018170103092783504,
      "loss": 0.1408,
      "step": 989
    },
    {
      "epoch": 0.1595037660611431,
      "grad_norm": 1.4562737941741943,
      "learning_rate": 0.00018137886597938144,
      "loss": 0.127,
      "step": 990
    },
    {
      "epoch": 0.1596648809763564,
      "grad_norm": 1.8071978092193604,
      "learning_rate": 0.00018105670103092783,
      "loss": 0.2088,
      "step": 991
    },
    {
      "epoch": 0.15982599589156965,
      "grad_norm": 1.0988795757293701,
      "learning_rate": 0.00018073453608247422,
      "loss": 0.0966,
      "step": 992
    },
    {
      "epoch": 0.15998711080678293,
      "grad_norm": 1.3888148069381714,
      "learning_rate": 0.00018041237113402062,
      "loss": 0.1208,
      "step": 993
    },
    {
      "epoch": 0.16014822572199622,
      "grad_norm": 2.36626935005188,
      "learning_rate": 0.000180090206185567,
      "loss": 0.1078,
      "step": 994
    },
    {
      "epoch": 0.1603093406372095,
      "grad_norm": 0.9779294729232788,
      "learning_rate": 0.0001797680412371134,
      "loss": 0.0804,
      "step": 995
    },
    {
      "epoch": 0.16047045555242276,
      "grad_norm": 1.0486360788345337,
      "learning_rate": 0.0001794458762886598,
      "loss": 0.0943,
      "step": 996
    },
    {
      "epoch": 0.16063157046763604,
      "grad_norm": 1.6178592443466187,
      "learning_rate": 0.0001791237113402062,
      "loss": 0.1479,
      "step": 997
    },
    {
      "epoch": 0.16079268538284933,
      "grad_norm": 1.5385267734527588,
      "learning_rate": 0.0001788015463917526,
      "loss": 0.1248,
      "step": 998
    },
    {
      "epoch": 0.16095380029806258,
      "grad_norm": 2.3869149684906006,
      "learning_rate": 0.00017847938144329898,
      "loss": 0.0958,
      "step": 999
    },
    {
      "epoch": 0.16111491521327587,
      "grad_norm": 0.8867502808570862,
      "learning_rate": 0.00017815721649484538,
      "loss": 0.0966,
      "step": 1000
    },
    {
      "epoch": 0.16127603012848915,
      "grad_norm": 1.2863503694534302,
      "learning_rate": 0.00017783505154639177,
      "loss": 0.0935,
      "step": 1001
    },
    {
      "epoch": 0.1614371450437024,
      "grad_norm": 2.6100800037384033,
      "learning_rate": 0.00017751288659793816,
      "loss": 0.177,
      "step": 1002
    },
    {
      "epoch": 0.1615982599589157,
      "grad_norm": 2.1011502742767334,
      "learning_rate": 0.00017719072164948456,
      "loss": 0.1695,
      "step": 1003
    },
    {
      "epoch": 0.16175937487412897,
      "grad_norm": 2.3291728496551514,
      "learning_rate": 0.00017686855670103095,
      "loss": 0.1605,
      "step": 1004
    },
    {
      "epoch": 0.16192048978934226,
      "grad_norm": 1.8329334259033203,
      "learning_rate": 0.00017654639175257735,
      "loss": 0.1434,
      "step": 1005
    },
    {
      "epoch": 0.16208160470455552,
      "grad_norm": 2.1877307891845703,
      "learning_rate": 0.00017622422680412374,
      "loss": 0.1453,
      "step": 1006
    },
    {
      "epoch": 0.1622427196197688,
      "grad_norm": 1.175995111465454,
      "learning_rate": 0.00017590206185567013,
      "loss": 0.125,
      "step": 1007
    },
    {
      "epoch": 0.16240383453498208,
      "grad_norm": 1.135883092880249,
      "learning_rate": 0.00017557989690721647,
      "loss": 0.1317,
      "step": 1008
    },
    {
      "epoch": 0.16256494945019534,
      "grad_norm": 1.7554659843444824,
      "learning_rate": 0.00017525773195876287,
      "loss": 0.1913,
      "step": 1009
    },
    {
      "epoch": 0.16272606436540862,
      "grad_norm": 1.4541213512420654,
      "learning_rate": 0.00017493556701030926,
      "loss": 0.1537,
      "step": 1010
    },
    {
      "epoch": 0.1628871792806219,
      "grad_norm": 1.3459954261779785,
      "learning_rate": 0.00017461340206185565,
      "loss": 0.1237,
      "step": 1011
    },
    {
      "epoch": 0.1630482941958352,
      "grad_norm": 1.1905057430267334,
      "learning_rate": 0.00017429123711340205,
      "loss": 0.0681,
      "step": 1012
    },
    {
      "epoch": 0.16320940911104845,
      "grad_norm": 1.0371813774108887,
      "learning_rate": 0.00017396907216494844,
      "loss": 0.0642,
      "step": 1013
    },
    {
      "epoch": 0.16337052402626173,
      "grad_norm": 2.110051393508911,
      "learning_rate": 0.00017364690721649484,
      "loss": 0.14,
      "step": 1014
    },
    {
      "epoch": 0.16353163894147502,
      "grad_norm": 1.9787474870681763,
      "learning_rate": 0.00017332474226804123,
      "loss": 0.154,
      "step": 1015
    },
    {
      "epoch": 0.16369275385668827,
      "grad_norm": 1.3783557415008545,
      "learning_rate": 0.00017300257731958762,
      "loss": 0.127,
      "step": 1016
    },
    {
      "epoch": 0.16385386877190156,
      "grad_norm": 1.7015249729156494,
      "learning_rate": 0.00017268041237113402,
      "loss": 0.1288,
      "step": 1017
    },
    {
      "epoch": 0.16401498368711484,
      "grad_norm": 1.44057297706604,
      "learning_rate": 0.0001723582474226804,
      "loss": 0.101,
      "step": 1018
    },
    {
      "epoch": 0.16417609860232812,
      "grad_norm": 2.4824397563934326,
      "learning_rate": 0.0001720360824742268,
      "loss": 0.1145,
      "step": 1019
    },
    {
      "epoch": 0.16433721351754138,
      "grad_norm": 1.038699746131897,
      "learning_rate": 0.0001717139175257732,
      "loss": 0.076,
      "step": 1020
    },
    {
      "epoch": 0.16449832843275466,
      "grad_norm": 1.5466841459274292,
      "learning_rate": 0.0001713917525773196,
      "loss": 0.0581,
      "step": 1021
    },
    {
      "epoch": 0.16465944334796795,
      "grad_norm": 2.003443717956543,
      "learning_rate": 0.000171069587628866,
      "loss": 0.1539,
      "step": 1022
    },
    {
      "epoch": 0.1648205582631812,
      "grad_norm": 1.6774306297302246,
      "learning_rate": 0.00017074742268041238,
      "loss": 0.1063,
      "step": 1023
    },
    {
      "epoch": 0.1649816731783945,
      "grad_norm": 1.991719126701355,
      "learning_rate": 0.00017042525773195878,
      "loss": 0.1109,
      "step": 1024
    },
    {
      "epoch": 0.16514278809360777,
      "grad_norm": 2.160628318786621,
      "learning_rate": 0.00017010309278350517,
      "loss": 0.0571,
      "step": 1025
    },
    {
      "epoch": 0.16530390300882103,
      "grad_norm": 1.714500904083252,
      "learning_rate": 0.00016978092783505156,
      "loss": 0.1057,
      "step": 1026
    },
    {
      "epoch": 0.1654650179240343,
      "grad_norm": 1.3987971544265747,
      "learning_rate": 0.00016945876288659796,
      "loss": 0.0693,
      "step": 1027
    },
    {
      "epoch": 0.1656261328392476,
      "grad_norm": 2.290548801422119,
      "learning_rate": 0.00016913659793814435,
      "loss": 0.1478,
      "step": 1028
    },
    {
      "epoch": 0.16578724775446088,
      "grad_norm": 0.9777520895004272,
      "learning_rate": 0.00016881443298969075,
      "loss": 0.0753,
      "step": 1029
    },
    {
      "epoch": 0.16594836266967414,
      "grad_norm": 2.22090220451355,
      "learning_rate": 0.00016849226804123714,
      "loss": 0.1457,
      "step": 1030
    },
    {
      "epoch": 0.16610947758488742,
      "grad_norm": 1.9461872577667236,
      "learning_rate": 0.00016817010309278353,
      "loss": 0.1233,
      "step": 1031
    },
    {
      "epoch": 0.1662705925001007,
      "grad_norm": 2.112393856048584,
      "learning_rate": 0.00016784793814432987,
      "loss": 0.1543,
      "step": 1032
    },
    {
      "epoch": 0.16643170741531396,
      "grad_norm": 2.0959579944610596,
      "learning_rate": 0.00016752577319587627,
      "loss": 0.1014,
      "step": 1033
    },
    {
      "epoch": 0.16659282233052725,
      "grad_norm": 1.7031292915344238,
      "learning_rate": 0.00016720360824742266,
      "loss": 0.1061,
      "step": 1034
    },
    {
      "epoch": 0.16675393724574053,
      "grad_norm": 1.5706950426101685,
      "learning_rate": 0.00016688144329896906,
      "loss": 0.1114,
      "step": 1035
    },
    {
      "epoch": 0.16691505216095381,
      "grad_norm": 1.856347680091858,
      "learning_rate": 0.00016655927835051545,
      "loss": 0.1034,
      "step": 1036
    },
    {
      "epoch": 0.16707616707616707,
      "grad_norm": 1.0278291702270508,
      "learning_rate": 0.00016623711340206184,
      "loss": 0.1007,
      "step": 1037
    },
    {
      "epoch": 0.16723728199138035,
      "grad_norm": 1.3258639574050903,
      "learning_rate": 0.00016591494845360824,
      "loss": 0.0911,
      "step": 1038
    },
    {
      "epoch": 0.16739839690659364,
      "grad_norm": 1.149976372718811,
      "learning_rate": 0.00016559278350515463,
      "loss": 0.1114,
      "step": 1039
    },
    {
      "epoch": 0.1675595118218069,
      "grad_norm": 0.8217151165008545,
      "learning_rate": 0.00016527061855670103,
      "loss": 0.0902,
      "step": 1040
    },
    {
      "epoch": 0.16772062673702018,
      "grad_norm": 1.6496399641036987,
      "learning_rate": 0.00016494845360824742,
      "loss": 0.1302,
      "step": 1041
    },
    {
      "epoch": 0.16788174165223346,
      "grad_norm": 1.4446779489517212,
      "learning_rate": 0.00016462628865979381,
      "loss": 0.0707,
      "step": 1042
    },
    {
      "epoch": 0.16804285656744672,
      "grad_norm": 0.8480768799781799,
      "learning_rate": 0.0001643041237113402,
      "loss": 0.0787,
      "step": 1043
    },
    {
      "epoch": 0.16820397148266,
      "grad_norm": 1.9849597215652466,
      "learning_rate": 0.0001639819587628866,
      "loss": 0.1302,
      "step": 1044
    },
    {
      "epoch": 0.1683650863978733,
      "grad_norm": 3.7543952465057373,
      "learning_rate": 0.000163659793814433,
      "loss": 0.1186,
      "step": 1045
    },
    {
      "epoch": 0.16852620131308657,
      "grad_norm": 1.6352490186691284,
      "learning_rate": 0.0001633376288659794,
      "loss": 0.0986,
      "step": 1046
    },
    {
      "epoch": 0.16868731622829983,
      "grad_norm": 2.230403184890747,
      "learning_rate": 0.00016301546391752578,
      "loss": 0.1423,
      "step": 1047
    },
    {
      "epoch": 0.1688484311435131,
      "grad_norm": 2.180901527404785,
      "learning_rate": 0.00016269329896907218,
      "loss": 0.0606,
      "step": 1048
    },
    {
      "epoch": 0.1690095460587264,
      "grad_norm": 1.4846347570419312,
      "learning_rate": 0.00016237113402061857,
      "loss": 0.0985,
      "step": 1049
    },
    {
      "epoch": 0.16917066097393965,
      "grad_norm": 1.8343836069107056,
      "learning_rate": 0.00016204896907216497,
      "loss": 0.122,
      "step": 1050
    },
    {
      "epoch": 0.16933177588915294,
      "grad_norm": 2.3206045627593994,
      "learning_rate": 0.00016172680412371136,
      "loss": 0.1136,
      "step": 1051
    },
    {
      "epoch": 0.16949289080436622,
      "grad_norm": 1.5385918617248535,
      "learning_rate": 0.00016140463917525775,
      "loss": 0.0903,
      "step": 1052
    },
    {
      "epoch": 0.1696540057195795,
      "grad_norm": 1.5983911752700806,
      "learning_rate": 0.00016108247422680415,
      "loss": 0.1267,
      "step": 1053
    },
    {
      "epoch": 0.16981512063479276,
      "grad_norm": 1.1360759735107422,
      "learning_rate": 0.00016076030927835054,
      "loss": 0.1032,
      "step": 1054
    },
    {
      "epoch": 0.16997623555000604,
      "grad_norm": 1.9500285387039185,
      "learning_rate": 0.00016043814432989694,
      "loss": 0.1093,
      "step": 1055
    },
    {
      "epoch": 0.17013735046521933,
      "grad_norm": 1.1106252670288086,
      "learning_rate": 0.00016011597938144328,
      "loss": 0.0839,
      "step": 1056
    },
    {
      "epoch": 0.17029846538043258,
      "grad_norm": 1.3593262434005737,
      "learning_rate": 0.00015979381443298967,
      "loss": 0.0782,
      "step": 1057
    },
    {
      "epoch": 0.17045958029564587,
      "grad_norm": 2.4170799255371094,
      "learning_rate": 0.00015947164948453606,
      "loss": 0.1151,
      "step": 1058
    },
    {
      "epoch": 0.17062069521085915,
      "grad_norm": 1.7429819107055664,
      "learning_rate": 0.00015914948453608246,
      "loss": 0.0965,
      "step": 1059
    },
    {
      "epoch": 0.1707818101260724,
      "grad_norm": 1.9749207496643066,
      "learning_rate": 0.00015882731958762885,
      "loss": 0.1817,
      "step": 1060
    },
    {
      "epoch": 0.1709429250412857,
      "grad_norm": 2.8226635456085205,
      "learning_rate": 0.00015850515463917525,
      "loss": 0.2176,
      "step": 1061
    },
    {
      "epoch": 0.17110403995649898,
      "grad_norm": 1.4699021577835083,
      "learning_rate": 0.00015818298969072164,
      "loss": 0.1195,
      "step": 1062
    },
    {
      "epoch": 0.17126515487171226,
      "grad_norm": 2.5465304851531982,
      "learning_rate": 0.00015786082474226803,
      "loss": 0.1874,
      "step": 1063
    },
    {
      "epoch": 0.17142626978692552,
      "grad_norm": 1.165540099143982,
      "learning_rate": 0.00015753865979381443,
      "loss": 0.1042,
      "step": 1064
    },
    {
      "epoch": 0.1715873847021388,
      "grad_norm": 1.913743495941162,
      "learning_rate": 0.00015721649484536082,
      "loss": 0.1305,
      "step": 1065
    },
    {
      "epoch": 0.17174849961735209,
      "grad_norm": 1.1052515506744385,
      "learning_rate": 0.00015689432989690722,
      "loss": 0.1105,
      "step": 1066
    },
    {
      "epoch": 0.17190961453256534,
      "grad_norm": 2.024351119995117,
      "learning_rate": 0.0001565721649484536,
      "loss": 0.1722,
      "step": 1067
    },
    {
      "epoch": 0.17207072944777863,
      "grad_norm": 2.2033088207244873,
      "learning_rate": 0.00015625,
      "loss": 0.1697,
      "step": 1068
    },
    {
      "epoch": 0.1722318443629919,
      "grad_norm": 0.8601778149604797,
      "learning_rate": 0.0001559278350515464,
      "loss": 0.0728,
      "step": 1069
    },
    {
      "epoch": 0.1723929592782052,
      "grad_norm": 1.1859601736068726,
      "learning_rate": 0.0001556056701030928,
      "loss": 0.1357,
      "step": 1070
    },
    {
      "epoch": 0.17255407419341845,
      "grad_norm": 1.4155457019805908,
      "learning_rate": 0.00015528350515463919,
      "loss": 0.095,
      "step": 1071
    },
    {
      "epoch": 0.17271518910863173,
      "grad_norm": 1.3505480289459229,
      "learning_rate": 0.00015496134020618558,
      "loss": 0.1167,
      "step": 1072
    },
    {
      "epoch": 0.17287630402384502,
      "grad_norm": 2.2968220710754395,
      "learning_rate": 0.00015463917525773197,
      "loss": 0.1181,
      "step": 1073
    },
    {
      "epoch": 0.17303741893905827,
      "grad_norm": 1.447118878364563,
      "learning_rate": 0.00015431701030927837,
      "loss": 0.146,
      "step": 1074
    },
    {
      "epoch": 0.17319853385427156,
      "grad_norm": 1.3639330863952637,
      "learning_rate": 0.00015399484536082476,
      "loss": 0.0514,
      "step": 1075
    },
    {
      "epoch": 0.17335964876948484,
      "grad_norm": 0.6956433057785034,
      "learning_rate": 0.00015367268041237116,
      "loss": 0.0646,
      "step": 1076
    },
    {
      "epoch": 0.1735207636846981,
      "grad_norm": 1.8842881917953491,
      "learning_rate": 0.00015335051546391755,
      "loss": 0.094,
      "step": 1077
    },
    {
      "epoch": 0.17368187859991138,
      "grad_norm": 1.7287228107452393,
      "learning_rate": 0.00015302835051546394,
      "loss": 0.08,
      "step": 1078
    },
    {
      "epoch": 0.17384299351512467,
      "grad_norm": 1.6793845891952515,
      "learning_rate": 0.00015270618556701034,
      "loss": 0.1365,
      "step": 1079
    },
    {
      "epoch": 0.17400410843033795,
      "grad_norm": 1.1963051557540894,
      "learning_rate": 0.00015238402061855673,
      "loss": 0.1143,
      "step": 1080
    },
    {
      "epoch": 0.1741652233455512,
      "grad_norm": 1.7680226564407349,
      "learning_rate": 0.00015206185567010307,
      "loss": 0.1402,
      "step": 1081
    },
    {
      "epoch": 0.1743263382607645,
      "grad_norm": 1.1964975595474243,
      "learning_rate": 0.00015173969072164946,
      "loss": 0.0996,
      "step": 1082
    },
    {
      "epoch": 0.17448745317597777,
      "grad_norm": 1.9621307849884033,
      "learning_rate": 0.00015141752577319586,
      "loss": 0.1373,
      "step": 1083
    },
    {
      "epoch": 0.17464856809119103,
      "grad_norm": 1.3759455680847168,
      "learning_rate": 0.00015109536082474225,
      "loss": 0.126,
      "step": 1084
    },
    {
      "epoch": 0.17480968300640432,
      "grad_norm": 1.0755535364151,
      "learning_rate": 0.00015077319587628865,
      "loss": 0.1145,
      "step": 1085
    },
    {
      "epoch": 0.1749707979216176,
      "grad_norm": 1.478171944618225,
      "learning_rate": 0.00015045103092783504,
      "loss": 0.1277,
      "step": 1086
    },
    {
      "epoch": 0.17513191283683088,
      "grad_norm": 1.4187849760055542,
      "learning_rate": 0.00015012886597938143,
      "loss": 0.1453,
      "step": 1087
    },
    {
      "epoch": 0.17529302775204414,
      "grad_norm": 1.631824016571045,
      "learning_rate": 0.00014980670103092783,
      "loss": 0.1174,
      "step": 1088
    },
    {
      "epoch": 0.17545414266725742,
      "grad_norm": 1.1297341585159302,
      "learning_rate": 0.00014948453608247422,
      "loss": 0.0926,
      "step": 1089
    },
    {
      "epoch": 0.1756152575824707,
      "grad_norm": 1.6425440311431885,
      "learning_rate": 0.00014916237113402062,
      "loss": 0.167,
      "step": 1090
    },
    {
      "epoch": 0.17577637249768396,
      "grad_norm": 1.1875303983688354,
      "learning_rate": 0.000148840206185567,
      "loss": 0.068,
      "step": 1091
    },
    {
      "epoch": 0.17593748741289725,
      "grad_norm": 1.5908201932907104,
      "learning_rate": 0.0001485180412371134,
      "loss": 0.1755,
      "step": 1092
    },
    {
      "epoch": 0.17609860232811053,
      "grad_norm": 2.5501668453216553,
      "learning_rate": 0.0001481958762886598,
      "loss": 0.1868,
      "step": 1093
    },
    {
      "epoch": 0.1762597172433238,
      "grad_norm": 1.7736979722976685,
      "learning_rate": 0.0001478737113402062,
      "loss": 0.1833,
      "step": 1094
    },
    {
      "epoch": 0.17642083215853707,
      "grad_norm": 2.33677077293396,
      "learning_rate": 0.0001475515463917526,
      "loss": 0.1812,
      "step": 1095
    },
    {
      "epoch": 0.17658194707375036,
      "grad_norm": 1.4693583250045776,
      "learning_rate": 0.00014722938144329898,
      "loss": 0.082,
      "step": 1096
    },
    {
      "epoch": 0.17674306198896364,
      "grad_norm": 1.3421028852462769,
      "learning_rate": 0.00014690721649484537,
      "loss": 0.118,
      "step": 1097
    },
    {
      "epoch": 0.1769041769041769,
      "grad_norm": 1.1907861232757568,
      "learning_rate": 0.00014658505154639177,
      "loss": 0.142,
      "step": 1098
    },
    {
      "epoch": 0.17706529181939018,
      "grad_norm": 0.8273949027061462,
      "learning_rate": 0.00014626288659793816,
      "loss": 0.0554,
      "step": 1099
    },
    {
      "epoch": 0.17722640673460346,
      "grad_norm": 1.9871296882629395,
      "learning_rate": 0.00014594072164948456,
      "loss": 0.1706,
      "step": 1100
    },
    {
      "epoch": 0.17738752164981672,
      "grad_norm": 1.0222907066345215,
      "learning_rate": 0.00014561855670103095,
      "loss": 0.117,
      "step": 1101
    },
    {
      "epoch": 0.17754863656503,
      "grad_norm": 2.7180850505828857,
      "learning_rate": 0.00014529639175257734,
      "loss": 0.1814,
      "step": 1102
    },
    {
      "epoch": 0.1777097514802433,
      "grad_norm": 2.073579788208008,
      "learning_rate": 0.00014497422680412374,
      "loss": 0.1749,
      "step": 1103
    },
    {
      "epoch": 0.17787086639545657,
      "grad_norm": 1.5971155166625977,
      "learning_rate": 0.00014465206185567013,
      "loss": 0.1074,
      "step": 1104
    },
    {
      "epoch": 0.17803198131066983,
      "grad_norm": 1.0852323770523071,
      "learning_rate": 0.00014432989690721647,
      "loss": 0.108,
      "step": 1105
    },
    {
      "epoch": 0.1781930962258831,
      "grad_norm": 1.2151988744735718,
      "learning_rate": 0.00014400773195876287,
      "loss": 0.1361,
      "step": 1106
    },
    {
      "epoch": 0.1783542111410964,
      "grad_norm": 1.0843853950500488,
      "learning_rate": 0.00014368556701030926,
      "loss": 0.112,
      "step": 1107
    },
    {
      "epoch": 0.17851532605630965,
      "grad_norm": 1.7547365427017212,
      "learning_rate": 0.00014336340206185565,
      "loss": 0.1571,
      "step": 1108
    },
    {
      "epoch": 0.17867644097152294,
      "grad_norm": 1.2463161945343018,
      "learning_rate": 0.00014304123711340205,
      "loss": 0.0752,
      "step": 1109
    },
    {
      "epoch": 0.17883755588673622,
      "grad_norm": 2.4198455810546875,
      "learning_rate": 0.00014271907216494844,
      "loss": 0.1751,
      "step": 1110
    },
    {
      "epoch": 0.17899867080194948,
      "grad_norm": 1.4024772644042969,
      "learning_rate": 0.00014239690721649484,
      "loss": 0.0977,
      "step": 1111
    },
    {
      "epoch": 0.17915978571716276,
      "grad_norm": 1.818110704421997,
      "learning_rate": 0.00014207474226804123,
      "loss": 0.0893,
      "step": 1112
    },
    {
      "epoch": 0.17932090063237605,
      "grad_norm": 1.1924163103103638,
      "learning_rate": 0.00014175257731958762,
      "loss": 0.1097,
      "step": 1113
    },
    {
      "epoch": 0.17948201554758933,
      "grad_norm": 2.0196492671966553,
      "learning_rate": 0.00014143041237113402,
      "loss": 0.1732,
      "step": 1114
    },
    {
      "epoch": 0.17964313046280259,
      "grad_norm": 2.618464231491089,
      "learning_rate": 0.0001411082474226804,
      "loss": 0.2032,
      "step": 1115
    },
    {
      "epoch": 0.17980424537801587,
      "grad_norm": 1.3398287296295166,
      "learning_rate": 0.0001407860824742268,
      "loss": 0.1128,
      "step": 1116
    },
    {
      "epoch": 0.17996536029322915,
      "grad_norm": 1.2396403551101685,
      "learning_rate": 0.0001404639175257732,
      "loss": 0.1109,
      "step": 1117
    },
    {
      "epoch": 0.1801264752084424,
      "grad_norm": 2.0397369861602783,
      "learning_rate": 0.0001401417525773196,
      "loss": 0.0893,
      "step": 1118
    },
    {
      "epoch": 0.1802875901236557,
      "grad_norm": 1.2057300806045532,
      "learning_rate": 0.000139819587628866,
      "loss": 0.1289,
      "step": 1119
    },
    {
      "epoch": 0.18044870503886898,
      "grad_norm": 0.9958335757255554,
      "learning_rate": 0.00013949742268041238,
      "loss": 0.0759,
      "step": 1120
    },
    {
      "epoch": 0.18060981995408226,
      "grad_norm": 1.3511848449707031,
      "learning_rate": 0.00013917525773195878,
      "loss": 0.1163,
      "step": 1121
    },
    {
      "epoch": 0.18077093486929552,
      "grad_norm": 1.060467004776001,
      "learning_rate": 0.00013885309278350517,
      "loss": 0.1007,
      "step": 1122
    },
    {
      "epoch": 0.1809320497845088,
      "grad_norm": 2.7832412719726562,
      "learning_rate": 0.00013853092783505156,
      "loss": 0.1831,
      "step": 1123
    },
    {
      "epoch": 0.1810931646997221,
      "grad_norm": 0.8624967336654663,
      "learning_rate": 0.00013820876288659796,
      "loss": 0.0642,
      "step": 1124
    },
    {
      "epoch": 0.18125427961493534,
      "grad_norm": 1.5762783288955688,
      "learning_rate": 0.00013788659793814435,
      "loss": 0.0847,
      "step": 1125
    },
    {
      "epoch": 0.18141539453014863,
      "grad_norm": 1.4854943752288818,
      "learning_rate": 0.00013756443298969075,
      "loss": 0.1288,
      "step": 1126
    },
    {
      "epoch": 0.1815765094453619,
      "grad_norm": 1.4521400928497314,
      "learning_rate": 0.00013724226804123714,
      "loss": 0.0614,
      "step": 1127
    },
    {
      "epoch": 0.18173762436057517,
      "grad_norm": 1.1918244361877441,
      "learning_rate": 0.00013692010309278353,
      "loss": 0.1036,
      "step": 1128
    },
    {
      "epoch": 0.18189873927578845,
      "grad_norm": 1.0130890607833862,
      "learning_rate": 0.00013659793814432987,
      "loss": 0.1083,
      "step": 1129
    },
    {
      "epoch": 0.18205985419100174,
      "grad_norm": 1.4030665159225464,
      "learning_rate": 0.00013627577319587627,
      "loss": 0.1487,
      "step": 1130
    },
    {
      "epoch": 0.18222096910621502,
      "grad_norm": 1.333832859992981,
      "learning_rate": 0.00013595360824742266,
      "loss": 0.0972,
      "step": 1131
    },
    {
      "epoch": 0.18238208402142828,
      "grad_norm": 1.807895541191101,
      "learning_rate": 0.00013563144329896906,
      "loss": 0.0976,
      "step": 1132
    },
    {
      "epoch": 0.18254319893664156,
      "grad_norm": 1.0457146167755127,
      "learning_rate": 0.00013530927835051545,
      "loss": 0.1279,
      "step": 1133
    },
    {
      "epoch": 0.18270431385185484,
      "grad_norm": 2.1121530532836914,
      "learning_rate": 0.00013498711340206184,
      "loss": 0.113,
      "step": 1134
    },
    {
      "epoch": 0.1828654287670681,
      "grad_norm": 1.585458517074585,
      "learning_rate": 0.00013466494845360824,
      "loss": 0.1247,
      "step": 1135
    },
    {
      "epoch": 0.18302654368228138,
      "grad_norm": 2.194699287414551,
      "learning_rate": 0.00013434278350515463,
      "loss": 0.1425,
      "step": 1136
    },
    {
      "epoch": 0.18318765859749467,
      "grad_norm": 1.634712815284729,
      "learning_rate": 0.00013402061855670103,
      "loss": 0.164,
      "step": 1137
    },
    {
      "epoch": 0.18334877351270795,
      "grad_norm": 1.1522430181503296,
      "learning_rate": 0.00013369845360824742,
      "loss": 0.0375,
      "step": 1138
    },
    {
      "epoch": 0.1835098884279212,
      "grad_norm": 2.901299238204956,
      "learning_rate": 0.0001333762886597938,
      "loss": 0.2016,
      "step": 1139
    },
    {
      "epoch": 0.1836710033431345,
      "grad_norm": 1.3083667755126953,
      "learning_rate": 0.0001330541237113402,
      "loss": 0.0843,
      "step": 1140
    },
    {
      "epoch": 0.18383211825834778,
      "grad_norm": 1.523768663406372,
      "learning_rate": 0.0001327319587628866,
      "loss": 0.1304,
      "step": 1141
    },
    {
      "epoch": 0.18399323317356103,
      "grad_norm": 0.9226101636886597,
      "learning_rate": 0.000132409793814433,
      "loss": 0.0678,
      "step": 1142
    },
    {
      "epoch": 0.18415434808877432,
      "grad_norm": 1.6037040948867798,
      "learning_rate": 0.0001320876288659794,
      "loss": 0.1732,
      "step": 1143
    },
    {
      "epoch": 0.1843154630039876,
      "grad_norm": 1.4732897281646729,
      "learning_rate": 0.00013176546391752578,
      "loss": 0.1298,
      "step": 1144
    },
    {
      "epoch": 0.18447657791920086,
      "grad_norm": 2.451267719268799,
      "learning_rate": 0.00013144329896907218,
      "loss": 0.1475,
      "step": 1145
    },
    {
      "epoch": 0.18463769283441414,
      "grad_norm": 1.3075212240219116,
      "learning_rate": 0.00013112113402061857,
      "loss": 0.1033,
      "step": 1146
    },
    {
      "epoch": 0.18479880774962743,
      "grad_norm": 0.7134339809417725,
      "learning_rate": 0.00013079896907216497,
      "loss": 0.074,
      "step": 1147
    },
    {
      "epoch": 0.1849599226648407,
      "grad_norm": 1.8439600467681885,
      "learning_rate": 0.00013047680412371136,
      "loss": 0.0857,
      "step": 1148
    },
    {
      "epoch": 0.18512103758005397,
      "grad_norm": 1.118169903755188,
      "learning_rate": 0.00013015463917525775,
      "loss": 0.0892,
      "step": 1149
    },
    {
      "epoch": 0.18528215249526725,
      "grad_norm": 1.1401704549789429,
      "learning_rate": 0.00012983247422680415,
      "loss": 0.102,
      "step": 1150
    },
    {
      "epoch": 0.18544326741048053,
      "grad_norm": 1.8983139991760254,
      "learning_rate": 0.00012951030927835054,
      "loss": 0.1362,
      "step": 1151
    },
    {
      "epoch": 0.1856043823256938,
      "grad_norm": 1.2006512880325317,
      "learning_rate": 0.00012918814432989694,
      "loss": 0.078,
      "step": 1152
    },
    {
      "epoch": 0.18576549724090707,
      "grad_norm": 0.6632665395736694,
      "learning_rate": 0.00012886597938144327,
      "loss": 0.0672,
      "step": 1153
    },
    {
      "epoch": 0.18592661215612036,
      "grad_norm": 3.376854419708252,
      "learning_rate": 0.00012854381443298967,
      "loss": 0.0973,
      "step": 1154
    },
    {
      "epoch": 0.18608772707133364,
      "grad_norm": 2.021061420440674,
      "learning_rate": 0.00012822164948453606,
      "loss": 0.0972,
      "step": 1155
    },
    {
      "epoch": 0.1862488419865469,
      "grad_norm": 1.3357889652252197,
      "learning_rate": 0.00012789948453608246,
      "loss": 0.1136,
      "step": 1156
    },
    {
      "epoch": 0.18640995690176018,
      "grad_norm": 1.9958645105361938,
      "learning_rate": 0.00012757731958762885,
      "loss": 0.0954,
      "step": 1157
    },
    {
      "epoch": 0.18657107181697347,
      "grad_norm": 2.119908094406128,
      "learning_rate": 0.00012725515463917524,
      "loss": 0.1854,
      "step": 1158
    },
    {
      "epoch": 0.18673218673218672,
      "grad_norm": 1.4917018413543701,
      "learning_rate": 0.00012693298969072164,
      "loss": 0.1811,
      "step": 1159
    },
    {
      "epoch": 0.1868933016474,
      "grad_norm": 1.9476219415664673,
      "learning_rate": 0.00012661082474226803,
      "loss": 0.1238,
      "step": 1160
    },
    {
      "epoch": 0.1870544165626133,
      "grad_norm": 1.4649250507354736,
      "learning_rate": 0.00012628865979381443,
      "loss": 0.1685,
      "step": 1161
    },
    {
      "epoch": 0.18721553147782655,
      "grad_norm": 1.0501775741577148,
      "learning_rate": 0.00012596649484536082,
      "loss": 0.0865,
      "step": 1162
    },
    {
      "epoch": 0.18737664639303983,
      "grad_norm": 1.196028470993042,
      "learning_rate": 0.00012564432989690721,
      "loss": 0.0536,
      "step": 1163
    },
    {
      "epoch": 0.18753776130825311,
      "grad_norm": 1.0076303482055664,
      "learning_rate": 0.0001253221649484536,
      "loss": 0.1097,
      "step": 1164
    },
    {
      "epoch": 0.1876988762234664,
      "grad_norm": 0.9476607441902161,
      "learning_rate": 0.000125,
      "loss": 0.0922,
      "step": 1165
    },
    {
      "epoch": 0.18785999113867966,
      "grad_norm": 1.040099859237671,
      "learning_rate": 0.0001246778350515464,
      "loss": 0.1052,
      "step": 1166
    },
    {
      "epoch": 0.18802110605389294,
      "grad_norm": 1.3266496658325195,
      "learning_rate": 0.0001243556701030928,
      "loss": 0.132,
      "step": 1167
    },
    {
      "epoch": 0.18818222096910622,
      "grad_norm": 1.2828449010849,
      "learning_rate": 0.00012403350515463918,
      "loss": 0.1316,
      "step": 1168
    },
    {
      "epoch": 0.18834333588431948,
      "grad_norm": 1.494953989982605,
      "learning_rate": 0.00012371134020618558,
      "loss": 0.0965,
      "step": 1169
    },
    {
      "epoch": 0.18850445079953276,
      "grad_norm": 1.4862585067749023,
      "learning_rate": 0.00012338917525773197,
      "loss": 0.1215,
      "step": 1170
    },
    {
      "epoch": 0.18866556571474605,
      "grad_norm": 0.7023438811302185,
      "learning_rate": 0.00012306701030927837,
      "loss": 0.0872,
      "step": 1171
    },
    {
      "epoch": 0.18882668062995933,
      "grad_norm": 4.724209308624268,
      "learning_rate": 0.00012274484536082473,
      "loss": 0.1133,
      "step": 1172
    },
    {
      "epoch": 0.1889877955451726,
      "grad_norm": 0.7391184568405151,
      "learning_rate": 0.00012242268041237113,
      "loss": 0.0782,
      "step": 1173
    },
    {
      "epoch": 0.18914891046038587,
      "grad_norm": 1.625010371208191,
      "learning_rate": 0.00012210051546391752,
      "loss": 0.0956,
      "step": 1174
    },
    {
      "epoch": 0.18931002537559916,
      "grad_norm": 1.7777466773986816,
      "learning_rate": 0.00012177835051546392,
      "loss": 0.0818,
      "step": 1175
    },
    {
      "epoch": 0.1894711402908124,
      "grad_norm": 1.7725671529769897,
      "learning_rate": 0.00012145618556701031,
      "loss": 0.1136,
      "step": 1176
    },
    {
      "epoch": 0.1896322552060257,
      "grad_norm": 0.8946359753608704,
      "learning_rate": 0.0001211340206185567,
      "loss": 0.072,
      "step": 1177
    },
    {
      "epoch": 0.18979337012123898,
      "grad_norm": 1.3540090322494507,
      "learning_rate": 0.0001208118556701031,
      "loss": 0.0762,
      "step": 1178
    },
    {
      "epoch": 0.18995448503645224,
      "grad_norm": 2.455338954925537,
      "learning_rate": 0.00012048969072164949,
      "loss": 0.0656,
      "step": 1179
    },
    {
      "epoch": 0.19011559995166552,
      "grad_norm": 1.6908141374588013,
      "learning_rate": 0.00012016752577319589,
      "loss": 0.1048,
      "step": 1180
    },
    {
      "epoch": 0.1902767148668788,
      "grad_norm": 2.035856008529663,
      "learning_rate": 0.00011984536082474228,
      "loss": 0.0865,
      "step": 1181
    },
    {
      "epoch": 0.1904378297820921,
      "grad_norm": 1.2050164937973022,
      "learning_rate": 0.00011952319587628867,
      "loss": 0.0885,
      "step": 1182
    },
    {
      "epoch": 0.19059894469730534,
      "grad_norm": 1.6831268072128296,
      "learning_rate": 0.00011920103092783507,
      "loss": 0.0475,
      "step": 1183
    },
    {
      "epoch": 0.19076005961251863,
      "grad_norm": 1.5490323305130005,
      "learning_rate": 0.00011887886597938143,
      "loss": 0.0814,
      "step": 1184
    },
    {
      "epoch": 0.1909211745277319,
      "grad_norm": 1.6029313802719116,
      "learning_rate": 0.00011855670103092783,
      "loss": 0.0673,
      "step": 1185
    },
    {
      "epoch": 0.19108228944294517,
      "grad_norm": 1.4827467203140259,
      "learning_rate": 0.00011823453608247422,
      "loss": 0.0895,
      "step": 1186
    },
    {
      "epoch": 0.19124340435815845,
      "grad_norm": 1.9541535377502441,
      "learning_rate": 0.00011791237113402062,
      "loss": 0.1566,
      "step": 1187
    },
    {
      "epoch": 0.19140451927337174,
      "grad_norm": 1.7928861379623413,
      "learning_rate": 0.00011759020618556701,
      "loss": 0.1309,
      "step": 1188
    },
    {
      "epoch": 0.19156563418858502,
      "grad_norm": 2.145496368408203,
      "learning_rate": 0.0001172680412371134,
      "loss": 0.1161,
      "step": 1189
    },
    {
      "epoch": 0.19172674910379828,
      "grad_norm": 1.082406759262085,
      "learning_rate": 0.0001169458762886598,
      "loss": 0.0897,
      "step": 1190
    },
    {
      "epoch": 0.19188786401901156,
      "grad_norm": 2.080838203430176,
      "learning_rate": 0.00011662371134020619,
      "loss": 0.1043,
      "step": 1191
    },
    {
      "epoch": 0.19204897893422485,
      "grad_norm": 1.5050617456436157,
      "learning_rate": 0.00011630154639175259,
      "loss": 0.1179,
      "step": 1192
    },
    {
      "epoch": 0.1922100938494381,
      "grad_norm": 2.0797550678253174,
      "learning_rate": 0.00011597938144329898,
      "loss": 0.1802,
      "step": 1193
    },
    {
      "epoch": 0.19237120876465139,
      "grad_norm": 1.9665038585662842,
      "learning_rate": 0.00011565721649484537,
      "loss": 0.1087,
      "step": 1194
    },
    {
      "epoch": 0.19253232367986467,
      "grad_norm": 1.6247611045837402,
      "learning_rate": 0.00011533505154639177,
      "loss": 0.0936,
      "step": 1195
    },
    {
      "epoch": 0.19269343859507793,
      "grad_norm": 1.075154185295105,
      "learning_rate": 0.00011501288659793813,
      "loss": 0.068,
      "step": 1196
    },
    {
      "epoch": 0.1928545535102912,
      "grad_norm": 1.1025331020355225,
      "learning_rate": 0.00011469072164948453,
      "loss": 0.1092,
      "step": 1197
    },
    {
      "epoch": 0.1930156684255045,
      "grad_norm": 1.4660395383834839,
      "learning_rate": 0.00011436855670103092,
      "loss": 0.1268,
      "step": 1198
    },
    {
      "epoch": 0.19317678334071778,
      "grad_norm": 2.216503381729126,
      "learning_rate": 0.00011404639175257732,
      "loss": 0.1056,
      "step": 1199
    },
    {
      "epoch": 0.19333789825593103,
      "grad_norm": 1.2174019813537598,
      "learning_rate": 0.00011372422680412371,
      "loss": 0.1141,
      "step": 1200
    },
    {
      "epoch": 0.19349901317114432,
      "grad_norm": 2.134097099304199,
      "learning_rate": 0.0001134020618556701,
      "loss": 0.137,
      "step": 1201
    },
    {
      "epoch": 0.1936601280863576,
      "grad_norm": 2.405399799346924,
      "learning_rate": 0.0001130798969072165,
      "loss": 0.1336,
      "step": 1202
    },
    {
      "epoch": 0.19382124300157086,
      "grad_norm": 2.6413207054138184,
      "learning_rate": 0.00011275773195876289,
      "loss": 0.1757,
      "step": 1203
    },
    {
      "epoch": 0.19398235791678414,
      "grad_norm": 1.8104884624481201,
      "learning_rate": 0.00011243556701030929,
      "loss": 0.1267,
      "step": 1204
    },
    {
      "epoch": 0.19414347283199743,
      "grad_norm": 1.4826141595840454,
      "learning_rate": 0.00011211340206185568,
      "loss": 0.1167,
      "step": 1205
    },
    {
      "epoch": 0.1943045877472107,
      "grad_norm": 1.54149329662323,
      "learning_rate": 0.00011179123711340207,
      "loss": 0.155,
      "step": 1206
    },
    {
      "epoch": 0.19446570266242397,
      "grad_norm": 1.5108827352523804,
      "learning_rate": 0.00011146907216494847,
      "loss": 0.1145,
      "step": 1207
    },
    {
      "epoch": 0.19462681757763725,
      "grad_norm": 1.2161829471588135,
      "learning_rate": 0.00011114690721649484,
      "loss": 0.0697,
      "step": 1208
    },
    {
      "epoch": 0.19478793249285054,
      "grad_norm": 0.8232661485671997,
      "learning_rate": 0.00011082474226804123,
      "loss": 0.0676,
      "step": 1209
    },
    {
      "epoch": 0.1949490474080638,
      "grad_norm": 1.18253493309021,
      "learning_rate": 0.00011050257731958762,
      "loss": 0.1181,
      "step": 1210
    },
    {
      "epoch": 0.19511016232327708,
      "grad_norm": 1.4034899473190308,
      "learning_rate": 0.00011018041237113402,
      "loss": 0.0904,
      "step": 1211
    },
    {
      "epoch": 0.19527127723849036,
      "grad_norm": 1.1093800067901611,
      "learning_rate": 0.00010985824742268041,
      "loss": 0.0527,
      "step": 1212
    },
    {
      "epoch": 0.19543239215370362,
      "grad_norm": 1.9002379179000854,
      "learning_rate": 0.0001095360824742268,
      "loss": 0.1125,
      "step": 1213
    },
    {
      "epoch": 0.1955935070689169,
      "grad_norm": 1.8065893650054932,
      "learning_rate": 0.0001092139175257732,
      "loss": 0.1156,
      "step": 1214
    },
    {
      "epoch": 0.19575462198413018,
      "grad_norm": 0.9339481592178345,
      "learning_rate": 0.0001088917525773196,
      "loss": 0.1047,
      "step": 1215
    },
    {
      "epoch": 0.19591573689934347,
      "grad_norm": 1.5927692651748657,
      "learning_rate": 0.00010856958762886599,
      "loss": 0.0641,
      "step": 1216
    },
    {
      "epoch": 0.19607685181455672,
      "grad_norm": 1.0775188207626343,
      "learning_rate": 0.00010824742268041238,
      "loss": 0.0947,
      "step": 1217
    },
    {
      "epoch": 0.19623796672977,
      "grad_norm": 1.6761566400527954,
      "learning_rate": 0.00010792525773195878,
      "loss": 0.1053,
      "step": 1218
    },
    {
      "epoch": 0.1963990816449833,
      "grad_norm": 1.9179422855377197,
      "learning_rate": 0.00010760309278350517,
      "loss": 0.1043,
      "step": 1219
    },
    {
      "epoch": 0.19656019656019655,
      "grad_norm": 1.3604763746261597,
      "learning_rate": 0.00010728092783505154,
      "loss": 0.1073,
      "step": 1220
    },
    {
      "epoch": 0.19672131147540983,
      "grad_norm": 1.936958909034729,
      "learning_rate": 0.00010695876288659793,
      "loss": 0.1511,
      "step": 1221
    },
    {
      "epoch": 0.19688242639062312,
      "grad_norm": 2.160853624343872,
      "learning_rate": 0.00010663659793814432,
      "loss": 0.1888,
      "step": 1222
    },
    {
      "epoch": 0.1970435413058364,
      "grad_norm": 2.330183267593384,
      "learning_rate": 0.00010631443298969072,
      "loss": 0.1757,
      "step": 1223
    },
    {
      "epoch": 0.19720465622104966,
      "grad_norm": 1.4093385934829712,
      "learning_rate": 0.00010599226804123711,
      "loss": 0.1429,
      "step": 1224
    },
    {
      "epoch": 0.19736577113626294,
      "grad_norm": 1.4386628866195679,
      "learning_rate": 0.0001056701030927835,
      "loss": 0.0963,
      "step": 1225
    },
    {
      "epoch": 0.19752688605147622,
      "grad_norm": 1.055576205253601,
      "learning_rate": 0.0001053479381443299,
      "loss": 0.0737,
      "step": 1226
    },
    {
      "epoch": 0.19768800096668948,
      "grad_norm": 1.0231611728668213,
      "learning_rate": 0.0001050257731958763,
      "loss": 0.1517,
      "step": 1227
    },
    {
      "epoch": 0.19784911588190277,
      "grad_norm": 1.7937582731246948,
      "learning_rate": 0.00010470360824742269,
      "loss": 0.1337,
      "step": 1228
    },
    {
      "epoch": 0.19801023079711605,
      "grad_norm": 1.3434969186782837,
      "learning_rate": 0.00010438144329896908,
      "loss": 0.1054,
      "step": 1229
    },
    {
      "epoch": 0.1981713457123293,
      "grad_norm": 0.8999637961387634,
      "learning_rate": 0.00010405927835051548,
      "loss": 0.1049,
      "step": 1230
    },
    {
      "epoch": 0.1983324606275426,
      "grad_norm": 1.5277312994003296,
      "learning_rate": 0.00010373711340206187,
      "loss": 0.1494,
      "step": 1231
    },
    {
      "epoch": 0.19849357554275587,
      "grad_norm": 0.7477571368217468,
      "learning_rate": 0.00010341494845360824,
      "loss": 0.0874,
      "step": 1232
    },
    {
      "epoch": 0.19865469045796916,
      "grad_norm": 1.2816247940063477,
      "learning_rate": 0.00010309278350515463,
      "loss": 0.1153,
      "step": 1233
    },
    {
      "epoch": 0.1988158053731824,
      "grad_norm": 1.2357499599456787,
      "learning_rate": 0.00010277061855670102,
      "loss": 0.0881,
      "step": 1234
    },
    {
      "epoch": 0.1989769202883957,
      "grad_norm": 1.2732360363006592,
      "learning_rate": 0.00010244845360824742,
      "loss": 0.1066,
      "step": 1235
    },
    {
      "epoch": 0.19913803520360898,
      "grad_norm": 1.4757672548294067,
      "learning_rate": 0.00010212628865979381,
      "loss": 0.1276,
      "step": 1236
    },
    {
      "epoch": 0.19929915011882224,
      "grad_norm": 0.5221022963523865,
      "learning_rate": 0.0001018041237113402,
      "loss": 0.0612,
      "step": 1237
    },
    {
      "epoch": 0.19946026503403552,
      "grad_norm": 1.1699919700622559,
      "learning_rate": 0.0001014819587628866,
      "loss": 0.1536,
      "step": 1238
    },
    {
      "epoch": 0.1996213799492488,
      "grad_norm": 0.7811644673347473,
      "learning_rate": 0.000101159793814433,
      "loss": 0.1051,
      "step": 1239
    },
    {
      "epoch": 0.1997824948644621,
      "grad_norm": 1.0991976261138916,
      "learning_rate": 0.00010083762886597939,
      "loss": 0.0876,
      "step": 1240
    },
    {
      "epoch": 0.19994360977967535,
      "grad_norm": 1.6495908498764038,
      "learning_rate": 0.00010051546391752578,
      "loss": 0.1437,
      "step": 1241
    },
    {
      "epoch": 0.20010472469488863,
      "grad_norm": 0.7276721596717834,
      "learning_rate": 0.00010019329896907218,
      "loss": 0.0735,
      "step": 1242
    },
    {
      "epoch": 0.20026583961010191,
      "grad_norm": 1.3608781099319458,
      "learning_rate": 9.987113402061857e-05,
      "loss": 0.125,
      "step": 1243
    },
    {
      "epoch": 0.20042695452531517,
      "grad_norm": 1.9312427043914795,
      "learning_rate": 9.954896907216494e-05,
      "loss": 0.1495,
      "step": 1244
    },
    {
      "epoch": 0.20058806944052845,
      "grad_norm": 1.7482335567474365,
      "learning_rate": 9.922680412371133e-05,
      "loss": 0.117,
      "step": 1245
    },
    {
      "epoch": 0.20074918435574174,
      "grad_norm": 0.9314818978309631,
      "learning_rate": 9.890463917525773e-05,
      "loss": 0.0666,
      "step": 1246
    },
    {
      "epoch": 0.200910299270955,
      "grad_norm": 1.2768440246582031,
      "learning_rate": 9.858247422680412e-05,
      "loss": 0.0762,
      "step": 1247
    },
    {
      "epoch": 0.20107141418616828,
      "grad_norm": 1.6510554552078247,
      "learning_rate": 9.826030927835051e-05,
      "loss": 0.1541,
      "step": 1248
    },
    {
      "epoch": 0.20123252910138156,
      "grad_norm": 1.2252110242843628,
      "learning_rate": 9.793814432989691e-05,
      "loss": 0.1085,
      "step": 1249
    },
    {
      "epoch": 0.20139364401659485,
      "grad_norm": 0.8189984560012817,
      "learning_rate": 9.76159793814433e-05,
      "loss": 0.0763,
      "step": 1250
    },
    {
      "epoch": 0.2015547589318081,
      "grad_norm": 0.773398756980896,
      "learning_rate": 9.72938144329897e-05,
      "loss": 0.0974,
      "step": 1251
    },
    {
      "epoch": 0.2017158738470214,
      "grad_norm": 0.7924113273620605,
      "learning_rate": 9.697164948453609e-05,
      "loss": 0.0676,
      "step": 1252
    },
    {
      "epoch": 0.20187698876223467,
      "grad_norm": 1.5841161012649536,
      "learning_rate": 9.664948453608248e-05,
      "loss": 0.0793,
      "step": 1253
    },
    {
      "epoch": 0.20203810367744793,
      "grad_norm": 0.7812061309814453,
      "learning_rate": 9.632731958762888e-05,
      "loss": 0.0306,
      "step": 1254
    },
    {
      "epoch": 0.2021992185926612,
      "grad_norm": 0.6219812035560608,
      "learning_rate": 9.600515463917527e-05,
      "loss": 0.0414,
      "step": 1255
    },
    {
      "epoch": 0.2023603335078745,
      "grad_norm": 1.4173091650009155,
      "learning_rate": 9.568298969072164e-05,
      "loss": 0.1429,
      "step": 1256
    },
    {
      "epoch": 0.20252144842308778,
      "grad_norm": 0.968349277973175,
      "learning_rate": 9.536082474226803e-05,
      "loss": 0.068,
      "step": 1257
    },
    {
      "epoch": 0.20268256333830104,
      "grad_norm": 2.2210025787353516,
      "learning_rate": 9.503865979381443e-05,
      "loss": 0.1406,
      "step": 1258
    },
    {
      "epoch": 0.20284367825351432,
      "grad_norm": 0.9064059853553772,
      "learning_rate": 9.471649484536082e-05,
      "loss": 0.0758,
      "step": 1259
    },
    {
      "epoch": 0.2030047931687276,
      "grad_norm": 1.4857538938522339,
      "learning_rate": 9.439432989690721e-05,
      "loss": 0.1501,
      "step": 1260
    },
    {
      "epoch": 0.20316590808394086,
      "grad_norm": 2.0497965812683105,
      "learning_rate": 9.407216494845361e-05,
      "loss": 0.1208,
      "step": 1261
    },
    {
      "epoch": 0.20332702299915414,
      "grad_norm": 5.179530143737793,
      "learning_rate": 9.375e-05,
      "loss": 0.1806,
      "step": 1262
    },
    {
      "epoch": 0.20348813791436743,
      "grad_norm": 1.4094103574752808,
      "learning_rate": 9.34278350515464e-05,
      "loss": 0.1168,
      "step": 1263
    },
    {
      "epoch": 0.20364925282958068,
      "grad_norm": 1.4691162109375,
      "learning_rate": 9.310567010309279e-05,
      "loss": 0.0889,
      "step": 1264
    },
    {
      "epoch": 0.20381036774479397,
      "grad_norm": 4.122340679168701,
      "learning_rate": 9.278350515463918e-05,
      "loss": 0.1905,
      "step": 1265
    },
    {
      "epoch": 0.20397148266000725,
      "grad_norm": 1.1261425018310547,
      "learning_rate": 9.246134020618558e-05,
      "loss": 0.063,
      "step": 1266
    },
    {
      "epoch": 0.20413259757522054,
      "grad_norm": 1.2539135217666626,
      "learning_rate": 9.213917525773197e-05,
      "loss": 0.08,
      "step": 1267
    },
    {
      "epoch": 0.2042937124904338,
      "grad_norm": 1.1007914543151855,
      "learning_rate": 9.181701030927837e-05,
      "loss": 0.1165,
      "step": 1268
    },
    {
      "epoch": 0.20445482740564708,
      "grad_norm": 1.5066187381744385,
      "learning_rate": 9.149484536082473e-05,
      "loss": 0.1444,
      "step": 1269
    },
    {
      "epoch": 0.20461594232086036,
      "grad_norm": 1.3277161121368408,
      "learning_rate": 9.117268041237113e-05,
      "loss": 0.0896,
      "step": 1270
    },
    {
      "epoch": 0.20477705723607362,
      "grad_norm": 2.210069417953491,
      "learning_rate": 9.085051546391752e-05,
      "loss": 0.1127,
      "step": 1271
    },
    {
      "epoch": 0.2049381721512869,
      "grad_norm": 0.9345569014549255,
      "learning_rate": 9.052835051546391e-05,
      "loss": 0.0803,
      "step": 1272
    },
    {
      "epoch": 0.20509928706650019,
      "grad_norm": 2.4022960662841797,
      "learning_rate": 9.020618556701031e-05,
      "loss": 0.1942,
      "step": 1273
    },
    {
      "epoch": 0.20526040198171347,
      "grad_norm": 1.0855474472045898,
      "learning_rate": 8.98840206185567e-05,
      "loss": 0.0595,
      "step": 1274
    },
    {
      "epoch": 0.20542151689692673,
      "grad_norm": 2.1726176738739014,
      "learning_rate": 8.95618556701031e-05,
      "loss": 0.1771,
      "step": 1275
    },
    {
      "epoch": 0.20558263181214,
      "grad_norm": 1.3134000301361084,
      "learning_rate": 8.923969072164949e-05,
      "loss": 0.1192,
      "step": 1276
    },
    {
      "epoch": 0.2057437467273533,
      "grad_norm": 1.6545305252075195,
      "learning_rate": 8.891752577319588e-05,
      "loss": 0.1297,
      "step": 1277
    },
    {
      "epoch": 0.20590486164256655,
      "grad_norm": 1.484601616859436,
      "learning_rate": 8.859536082474228e-05,
      "loss": 0.1139,
      "step": 1278
    },
    {
      "epoch": 0.20606597655777983,
      "grad_norm": 1.4535547494888306,
      "learning_rate": 8.827319587628867e-05,
      "loss": 0.1195,
      "step": 1279
    },
    {
      "epoch": 0.20622709147299312,
      "grad_norm": 0.838226318359375,
      "learning_rate": 8.795103092783507e-05,
      "loss": 0.0516,
      "step": 1280
    },
    {
      "epoch": 0.20638820638820637,
      "grad_norm": 0.9190845489501953,
      "learning_rate": 8.762886597938143e-05,
      "loss": 0.1009,
      "step": 1281
    },
    {
      "epoch": 0.20654932130341966,
      "grad_norm": 1.5050009489059448,
      "learning_rate": 8.730670103092783e-05,
      "loss": 0.1166,
      "step": 1282
    },
    {
      "epoch": 0.20671043621863294,
      "grad_norm": 1.142283320426941,
      "learning_rate": 8.698453608247422e-05,
      "loss": 0.0954,
      "step": 1283
    },
    {
      "epoch": 0.20687155113384623,
      "grad_norm": 1.0591480731964111,
      "learning_rate": 8.666237113402062e-05,
      "loss": 0.0938,
      "step": 1284
    },
    {
      "epoch": 0.20703266604905948,
      "grad_norm": 1.0544824600219727,
      "learning_rate": 8.634020618556701e-05,
      "loss": 0.1157,
      "step": 1285
    },
    {
      "epoch": 0.20719378096427277,
      "grad_norm": 1.571374535560608,
      "learning_rate": 8.60180412371134e-05,
      "loss": 0.0854,
      "step": 1286
    },
    {
      "epoch": 0.20735489587948605,
      "grad_norm": 0.89726722240448,
      "learning_rate": 8.56958762886598e-05,
      "loss": 0.0842,
      "step": 1287
    },
    {
      "epoch": 0.2075160107946993,
      "grad_norm": 1.0074641704559326,
      "learning_rate": 8.537371134020619e-05,
      "loss": 0.1038,
      "step": 1288
    },
    {
      "epoch": 0.2076771257099126,
      "grad_norm": 0.9524397850036621,
      "learning_rate": 8.505154639175259e-05,
      "loss": 0.1084,
      "step": 1289
    },
    {
      "epoch": 0.20783824062512588,
      "grad_norm": 0.7382520437240601,
      "learning_rate": 8.472938144329898e-05,
      "loss": 0.0375,
      "step": 1290
    },
    {
      "epoch": 0.20799935554033916,
      "grad_norm": 0.5818484425544739,
      "learning_rate": 8.440721649484537e-05,
      "loss": 0.0729,
      "step": 1291
    },
    {
      "epoch": 0.20816047045555242,
      "grad_norm": 0.7437809705734253,
      "learning_rate": 8.408505154639177e-05,
      "loss": 0.078,
      "step": 1292
    },
    {
      "epoch": 0.2083215853707657,
      "grad_norm": 1.3435156345367432,
      "learning_rate": 8.376288659793813e-05,
      "loss": 0.1195,
      "step": 1293
    },
    {
      "epoch": 0.20848270028597898,
      "grad_norm": 1.9216854572296143,
      "learning_rate": 8.344072164948453e-05,
      "loss": 0.1657,
      "step": 1294
    },
    {
      "epoch": 0.20864381520119224,
      "grad_norm": 0.6254622340202332,
      "learning_rate": 8.311855670103092e-05,
      "loss": 0.0457,
      "step": 1295
    },
    {
      "epoch": 0.20880493011640552,
      "grad_norm": 1.036331295967102,
      "learning_rate": 8.279639175257732e-05,
      "loss": 0.0904,
      "step": 1296
    },
    {
      "epoch": 0.2089660450316188,
      "grad_norm": 1.310715913772583,
      "learning_rate": 8.247422680412371e-05,
      "loss": 0.1135,
      "step": 1297
    },
    {
      "epoch": 0.20912715994683206,
      "grad_norm": 1.1445015668869019,
      "learning_rate": 8.21520618556701e-05,
      "loss": 0.0732,
      "step": 1298
    },
    {
      "epoch": 0.20928827486204535,
      "grad_norm": 1.7031941413879395,
      "learning_rate": 8.18298969072165e-05,
      "loss": 0.1478,
      "step": 1299
    },
    {
      "epoch": 0.20944938977725863,
      "grad_norm": 2.4998183250427246,
      "learning_rate": 8.150773195876289e-05,
      "loss": 0.1817,
      "step": 1300
    },
    {
      "epoch": 0.20961050469247192,
      "grad_norm": 0.8842570185661316,
      "learning_rate": 8.118556701030929e-05,
      "loss": 0.0721,
      "step": 1301
    },
    {
      "epoch": 0.20977161960768517,
      "grad_norm": 1.0540207624435425,
      "learning_rate": 8.086340206185568e-05,
      "loss": 0.0823,
      "step": 1302
    },
    {
      "epoch": 0.20993273452289846,
      "grad_norm": 1.168084740638733,
      "learning_rate": 8.054123711340207e-05,
      "loss": 0.0752,
      "step": 1303
    },
    {
      "epoch": 0.21009384943811174,
      "grad_norm": 1.516684889793396,
      "learning_rate": 8.021907216494847e-05,
      "loss": 0.1179,
      "step": 1304
    },
    {
      "epoch": 0.210254964353325,
      "grad_norm": 1.2570980787277222,
      "learning_rate": 7.989690721649483e-05,
      "loss": 0.0802,
      "step": 1305
    },
    {
      "epoch": 0.21041607926853828,
      "grad_norm": 2.933147430419922,
      "learning_rate": 7.957474226804123e-05,
      "loss": 0.1367,
      "step": 1306
    },
    {
      "epoch": 0.21057719418375156,
      "grad_norm": 1.7508783340454102,
      "learning_rate": 7.925257731958762e-05,
      "loss": 0.1143,
      "step": 1307
    },
    {
      "epoch": 0.21073830909896485,
      "grad_norm": 1.4127546548843384,
      "learning_rate": 7.893041237113402e-05,
      "loss": 0.1304,
      "step": 1308
    },
    {
      "epoch": 0.2108994240141781,
      "grad_norm": 1.01311194896698,
      "learning_rate": 7.860824742268041e-05,
      "loss": 0.1011,
      "step": 1309
    },
    {
      "epoch": 0.2110605389293914,
      "grad_norm": 1.2249627113342285,
      "learning_rate": 7.82860824742268e-05,
      "loss": 0.0454,
      "step": 1310
    },
    {
      "epoch": 0.21122165384460467,
      "grad_norm": 1.0981523990631104,
      "learning_rate": 7.79639175257732e-05,
      "loss": 0.0998,
      "step": 1311
    },
    {
      "epoch": 0.21138276875981793,
      "grad_norm": 1.6209189891815186,
      "learning_rate": 7.764175257731959e-05,
      "loss": 0.154,
      "step": 1312
    },
    {
      "epoch": 0.2115438836750312,
      "grad_norm": 2.491689920425415,
      "learning_rate": 7.731958762886599e-05,
      "loss": 0.0938,
      "step": 1313
    },
    {
      "epoch": 0.2117049985902445,
      "grad_norm": 1.8028759956359863,
      "learning_rate": 7.699742268041238e-05,
      "loss": 0.1089,
      "step": 1314
    },
    {
      "epoch": 0.21186611350545778,
      "grad_norm": 1.0093297958374023,
      "learning_rate": 7.667525773195877e-05,
      "loss": 0.1025,
      "step": 1315
    },
    {
      "epoch": 0.21202722842067104,
      "grad_norm": 1.7297568321228027,
      "learning_rate": 7.635309278350517e-05,
      "loss": 0.1497,
      "step": 1316
    },
    {
      "epoch": 0.21218834333588432,
      "grad_norm": 1.1744576692581177,
      "learning_rate": 7.603092783505154e-05,
      "loss": 0.0988,
      "step": 1317
    },
    {
      "epoch": 0.2123494582510976,
      "grad_norm": 1.4174529314041138,
      "learning_rate": 7.570876288659793e-05,
      "loss": 0.115,
      "step": 1318
    },
    {
      "epoch": 0.21251057316631086,
      "grad_norm": 1.7385945320129395,
      "learning_rate": 7.538659793814432e-05,
      "loss": 0.0795,
      "step": 1319
    },
    {
      "epoch": 0.21267168808152415,
      "grad_norm": 1.3883329629898071,
      "learning_rate": 7.506443298969072e-05,
      "loss": 0.1014,
      "step": 1320
    },
    {
      "epoch": 0.21283280299673743,
      "grad_norm": 1.0587406158447266,
      "learning_rate": 7.474226804123711e-05,
      "loss": 0.0992,
      "step": 1321
    },
    {
      "epoch": 0.2129939179119507,
      "grad_norm": 0.8918877840042114,
      "learning_rate": 7.44201030927835e-05,
      "loss": 0.1267,
      "step": 1322
    },
    {
      "epoch": 0.21315503282716397,
      "grad_norm": 1.1072787046432495,
      "learning_rate": 7.40979381443299e-05,
      "loss": 0.0672,
      "step": 1323
    },
    {
      "epoch": 0.21331614774237725,
      "grad_norm": 1.437516689300537,
      "learning_rate": 7.37757731958763e-05,
      "loss": 0.1457,
      "step": 1324
    },
    {
      "epoch": 0.21347726265759054,
      "grad_norm": 1.4996899366378784,
      "learning_rate": 7.345360824742269e-05,
      "loss": 0.1106,
      "step": 1325
    },
    {
      "epoch": 0.2136383775728038,
      "grad_norm": 1.3288182020187378,
      "learning_rate": 7.313144329896908e-05,
      "loss": 0.0862,
      "step": 1326
    },
    {
      "epoch": 0.21379949248801708,
      "grad_norm": 0.9653350710868835,
      "learning_rate": 7.280927835051548e-05,
      "loss": 0.0676,
      "step": 1327
    },
    {
      "epoch": 0.21396060740323036,
      "grad_norm": 1.6030346155166626,
      "learning_rate": 7.248711340206187e-05,
      "loss": 0.0762,
      "step": 1328
    },
    {
      "epoch": 0.21412172231844362,
      "grad_norm": 1.705291509628296,
      "learning_rate": 7.216494845360824e-05,
      "loss": 0.1114,
      "step": 1329
    },
    {
      "epoch": 0.2142828372336569,
      "grad_norm": 1.0117682218551636,
      "learning_rate": 7.184278350515463e-05,
      "loss": 0.0632,
      "step": 1330
    },
    {
      "epoch": 0.2144439521488702,
      "grad_norm": 0.8954210877418518,
      "learning_rate": 7.152061855670102e-05,
      "loss": 0.0757,
      "step": 1331
    },
    {
      "epoch": 0.21460506706408347,
      "grad_norm": 0.9875961542129517,
      "learning_rate": 7.119845360824742e-05,
      "loss": 0.0596,
      "step": 1332
    },
    {
      "epoch": 0.21476618197929673,
      "grad_norm": 2.5678529739379883,
      "learning_rate": 7.087628865979381e-05,
      "loss": 0.0928,
      "step": 1333
    },
    {
      "epoch": 0.21492729689451,
      "grad_norm": 1.0542572736740112,
      "learning_rate": 7.05541237113402e-05,
      "loss": 0.0697,
      "step": 1334
    },
    {
      "epoch": 0.2150884118097233,
      "grad_norm": 1.3597948551177979,
      "learning_rate": 7.02319587628866e-05,
      "loss": 0.0927,
      "step": 1335
    },
    {
      "epoch": 0.21524952672493655,
      "grad_norm": 1.1759637594223022,
      "learning_rate": 6.9909793814433e-05,
      "loss": 0.0788,
      "step": 1336
    },
    {
      "epoch": 0.21541064164014984,
      "grad_norm": 0.8626535534858704,
      "learning_rate": 6.958762886597939e-05,
      "loss": 0.0793,
      "step": 1337
    },
    {
      "epoch": 0.21557175655536312,
      "grad_norm": 1.223752498626709,
      "learning_rate": 6.926546391752578e-05,
      "loss": 0.093,
      "step": 1338
    },
    {
      "epoch": 0.21573287147057638,
      "grad_norm": 1.8439058065414429,
      "learning_rate": 6.894329896907218e-05,
      "loss": 0.134,
      "step": 1339
    },
    {
      "epoch": 0.21589398638578966,
      "grad_norm": 1.378114938735962,
      "learning_rate": 6.862113402061857e-05,
      "loss": 0.1192,
      "step": 1340
    },
    {
      "epoch": 0.21605510130100294,
      "grad_norm": 2.2069692611694336,
      "learning_rate": 6.829896907216494e-05,
      "loss": 0.173,
      "step": 1341
    },
    {
      "epoch": 0.21621621621621623,
      "grad_norm": 1.393305778503418,
      "learning_rate": 6.797680412371133e-05,
      "loss": 0.1745,
      "step": 1342
    },
    {
      "epoch": 0.21637733113142948,
      "grad_norm": 0.6083109378814697,
      "learning_rate": 6.765463917525772e-05,
      "loss": 0.0681,
      "step": 1343
    },
    {
      "epoch": 0.21653844604664277,
      "grad_norm": 1.5548537969589233,
      "learning_rate": 6.733247422680412e-05,
      "loss": 0.0945,
      "step": 1344
    },
    {
      "epoch": 0.21669956096185605,
      "grad_norm": 0.9144071340560913,
      "learning_rate": 6.701030927835051e-05,
      "loss": 0.0722,
      "step": 1345
    },
    {
      "epoch": 0.2168606758770693,
      "grad_norm": 1.046682596206665,
      "learning_rate": 6.66881443298969e-05,
      "loss": 0.1052,
      "step": 1346
    },
    {
      "epoch": 0.2170217907922826,
      "grad_norm": 1.078810214996338,
      "learning_rate": 6.63659793814433e-05,
      "loss": 0.1212,
      "step": 1347
    },
    {
      "epoch": 0.21718290570749588,
      "grad_norm": 1.7371898889541626,
      "learning_rate": 6.60438144329897e-05,
      "loss": 0.1046,
      "step": 1348
    },
    {
      "epoch": 0.21734402062270916,
      "grad_norm": 1.3082951307296753,
      "learning_rate": 6.572164948453609e-05,
      "loss": 0.1482,
      "step": 1349
    },
    {
      "epoch": 0.21750513553792242,
      "grad_norm": 1.6434746980667114,
      "learning_rate": 6.539948453608248e-05,
      "loss": 0.1282,
      "step": 1350
    },
    {
      "epoch": 0.2176662504531357,
      "grad_norm": 1.6144558191299438,
      "learning_rate": 6.507731958762888e-05,
      "loss": 0.223,
      "step": 1351
    },
    {
      "epoch": 0.21782736536834899,
      "grad_norm": 1.1893227100372314,
      "learning_rate": 6.475515463917527e-05,
      "loss": 0.104,
      "step": 1352
    },
    {
      "epoch": 0.21798848028356224,
      "grad_norm": 1.0459638833999634,
      "learning_rate": 6.443298969072164e-05,
      "loss": 0.0811,
      "step": 1353
    },
    {
      "epoch": 0.21814959519877553,
      "grad_norm": 0.6782276630401611,
      "learning_rate": 6.411082474226803e-05,
      "loss": 0.0674,
      "step": 1354
    },
    {
      "epoch": 0.2183107101139888,
      "grad_norm": 1.0459738969802856,
      "learning_rate": 6.378865979381443e-05,
      "loss": 0.0592,
      "step": 1355
    },
    {
      "epoch": 0.21847182502920207,
      "grad_norm": 1.008406162261963,
      "learning_rate": 6.346649484536082e-05,
      "loss": 0.0752,
      "step": 1356
    },
    {
      "epoch": 0.21863293994441535,
      "grad_norm": 0.7298954129219055,
      "learning_rate": 6.314432989690721e-05,
      "loss": 0.0736,
      "step": 1357
    },
    {
      "epoch": 0.21879405485962863,
      "grad_norm": 1.3547555208206177,
      "learning_rate": 6.282216494845361e-05,
      "loss": 0.1075,
      "step": 1358
    },
    {
      "epoch": 0.21895516977484192,
      "grad_norm": 0.7383569478988647,
      "learning_rate": 6.25e-05,
      "loss": 0.0751,
      "step": 1359
    },
    {
      "epoch": 0.21911628469005517,
      "grad_norm": 1.0284132957458496,
      "learning_rate": 6.21778350515464e-05,
      "loss": 0.0882,
      "step": 1360
    },
    {
      "epoch": 0.21927739960526846,
      "grad_norm": 0.6281552314758301,
      "learning_rate": 6.185567010309279e-05,
      "loss": 0.0342,
      "step": 1361
    },
    {
      "epoch": 0.21943851452048174,
      "grad_norm": 1.2455289363861084,
      "learning_rate": 6.153350515463918e-05,
      "loss": 0.1045,
      "step": 1362
    },
    {
      "epoch": 0.219599629435695,
      "grad_norm": 1.4466360807418823,
      "learning_rate": 6.121134020618556e-05,
      "loss": 0.1015,
      "step": 1363
    },
    {
      "epoch": 0.21976074435090828,
      "grad_norm": 0.8284788727760315,
      "learning_rate": 6.088917525773196e-05,
      "loss": 0.0778,
      "step": 1364
    },
    {
      "epoch": 0.21992185926612157,
      "grad_norm": 2.8529345989227295,
      "learning_rate": 6.056701030927835e-05,
      "loss": 0.1349,
      "step": 1365
    },
    {
      "epoch": 0.22008297418133485,
      "grad_norm": 1.4812769889831543,
      "learning_rate": 6.0244845360824746e-05,
      "loss": 0.1529,
      "step": 1366
    },
    {
      "epoch": 0.2202440890965481,
      "grad_norm": 1.053092122077942,
      "learning_rate": 5.992268041237114e-05,
      "loss": 0.0861,
      "step": 1367
    },
    {
      "epoch": 0.2204052040117614,
      "grad_norm": 1.8723012208938599,
      "learning_rate": 5.9600515463917534e-05,
      "loss": 0.118,
      "step": 1368
    },
    {
      "epoch": 0.22056631892697467,
      "grad_norm": 1.475460171699524,
      "learning_rate": 5.9278350515463914e-05,
      "loss": 0.1023,
      "step": 1369
    },
    {
      "epoch": 0.22072743384218793,
      "grad_norm": 1.2081540822982788,
      "learning_rate": 5.895618556701031e-05,
      "loss": 0.1198,
      "step": 1370
    },
    {
      "epoch": 0.22088854875740122,
      "grad_norm": 0.863648533821106,
      "learning_rate": 5.86340206185567e-05,
      "loss": 0.0986,
      "step": 1371
    },
    {
      "epoch": 0.2210496636726145,
      "grad_norm": 1.5292925834655762,
      "learning_rate": 5.8311855670103096e-05,
      "loss": 0.1371,
      "step": 1372
    },
    {
      "epoch": 0.22121077858782776,
      "grad_norm": 2.3557913303375244,
      "learning_rate": 5.798969072164949e-05,
      "loss": 0.1309,
      "step": 1373
    },
    {
      "epoch": 0.22137189350304104,
      "grad_norm": 1.0361261367797852,
      "learning_rate": 5.7667525773195884e-05,
      "loss": 0.1321,
      "step": 1374
    },
    {
      "epoch": 0.22153300841825432,
      "grad_norm": 1.1686898469924927,
      "learning_rate": 5.7345360824742264e-05,
      "loss": 0.0786,
      "step": 1375
    },
    {
      "epoch": 0.2216941233334676,
      "grad_norm": 1.5880897045135498,
      "learning_rate": 5.702319587628866e-05,
      "loss": 0.101,
      "step": 1376
    },
    {
      "epoch": 0.22185523824868086,
      "grad_norm": 1.5808991193771362,
      "learning_rate": 5.670103092783505e-05,
      "loss": 0.117,
      "step": 1377
    },
    {
      "epoch": 0.22201635316389415,
      "grad_norm": 0.48662683367729187,
      "learning_rate": 5.6378865979381446e-05,
      "loss": 0.0361,
      "step": 1378
    },
    {
      "epoch": 0.22217746807910743,
      "grad_norm": 0.8820776343345642,
      "learning_rate": 5.605670103092784e-05,
      "loss": 0.1078,
      "step": 1379
    },
    {
      "epoch": 0.2223385829943207,
      "grad_norm": 1.4613794088363647,
      "learning_rate": 5.5734536082474234e-05,
      "loss": 0.1678,
      "step": 1380
    },
    {
      "epoch": 0.22249969790953397,
      "grad_norm": 1.6328387260437012,
      "learning_rate": 5.5412371134020615e-05,
      "loss": 0.1294,
      "step": 1381
    },
    {
      "epoch": 0.22266081282474726,
      "grad_norm": 0.9158191084861755,
      "learning_rate": 5.509020618556701e-05,
      "loss": 0.0832,
      "step": 1382
    },
    {
      "epoch": 0.22282192773996054,
      "grad_norm": 0.9453802704811096,
      "learning_rate": 5.47680412371134e-05,
      "loss": 0.1257,
      "step": 1383
    },
    {
      "epoch": 0.2229830426551738,
      "grad_norm": 0.6197601556777954,
      "learning_rate": 5.44458762886598e-05,
      "loss": 0.0631,
      "step": 1384
    },
    {
      "epoch": 0.22314415757038708,
      "grad_norm": 0.9744426608085632,
      "learning_rate": 5.412371134020619e-05,
      "loss": 0.0699,
      "step": 1385
    },
    {
      "epoch": 0.22330527248560036,
      "grad_norm": 0.880283772945404,
      "learning_rate": 5.3801546391752585e-05,
      "loss": 0.065,
      "step": 1386
    },
    {
      "epoch": 0.22346638740081362,
      "grad_norm": 1.2759803533554077,
      "learning_rate": 5.3479381443298965e-05,
      "loss": 0.1039,
      "step": 1387
    },
    {
      "epoch": 0.2236275023160269,
      "grad_norm": 1.1881649494171143,
      "learning_rate": 5.315721649484536e-05,
      "loss": 0.138,
      "step": 1388
    },
    {
      "epoch": 0.2237886172312402,
      "grad_norm": 1.1050467491149902,
      "learning_rate": 5.283505154639175e-05,
      "loss": 0.1158,
      "step": 1389
    },
    {
      "epoch": 0.22394973214645345,
      "grad_norm": 0.7620590329170227,
      "learning_rate": 5.251288659793815e-05,
      "loss": 0.0566,
      "step": 1390
    },
    {
      "epoch": 0.22411084706166673,
      "grad_norm": 1.6459379196166992,
      "learning_rate": 5.219072164948454e-05,
      "loss": 0.1491,
      "step": 1391
    },
    {
      "epoch": 0.22427196197688,
      "grad_norm": 1.583658218383789,
      "learning_rate": 5.1868556701030935e-05,
      "loss": 0.1523,
      "step": 1392
    },
    {
      "epoch": 0.2244330768920933,
      "grad_norm": 0.7494300603866577,
      "learning_rate": 5.1546391752577315e-05,
      "loss": 0.0831,
      "step": 1393
    },
    {
      "epoch": 0.22459419180730655,
      "grad_norm": 1.2998496294021606,
      "learning_rate": 5.122422680412371e-05,
      "loss": 0.1082,
      "step": 1394
    },
    {
      "epoch": 0.22475530672251984,
      "grad_norm": 1.8796932697296143,
      "learning_rate": 5.09020618556701e-05,
      "loss": 0.1706,
      "step": 1395
    },
    {
      "epoch": 0.22491642163773312,
      "grad_norm": 1.3170316219329834,
      "learning_rate": 5.05798969072165e-05,
      "loss": 0.1026,
      "step": 1396
    },
    {
      "epoch": 0.22507753655294638,
      "grad_norm": 1.5066263675689697,
      "learning_rate": 5.025773195876289e-05,
      "loss": 0.1139,
      "step": 1397
    },
    {
      "epoch": 0.22523865146815966,
      "grad_norm": 0.6126667261123657,
      "learning_rate": 4.9935567010309285e-05,
      "loss": 0.0328,
      "step": 1398
    },
    {
      "epoch": 0.22539976638337295,
      "grad_norm": 1.8107657432556152,
      "learning_rate": 4.9613402061855666e-05,
      "loss": 0.1301,
      "step": 1399
    },
    {
      "epoch": 0.22556088129858623,
      "grad_norm": 1.0907344818115234,
      "learning_rate": 4.929123711340206e-05,
      "loss": 0.0902,
      "step": 1400
    },
    {
      "epoch": 0.2257219962137995,
      "grad_norm": 0.7116109728813171,
      "learning_rate": 4.8969072164948454e-05,
      "loss": 0.0462,
      "step": 1401
    },
    {
      "epoch": 0.22588311112901277,
      "grad_norm": 1.0265849828720093,
      "learning_rate": 4.864690721649485e-05,
      "loss": 0.0834,
      "step": 1402
    },
    {
      "epoch": 0.22604422604422605,
      "grad_norm": 0.46384936571121216,
      "learning_rate": 4.832474226804124e-05,
      "loss": 0.0446,
      "step": 1403
    },
    {
      "epoch": 0.2262053409594393,
      "grad_norm": 0.7034428119659424,
      "learning_rate": 4.8002577319587636e-05,
      "loss": 0.0568,
      "step": 1404
    },
    {
      "epoch": 0.2263664558746526,
      "grad_norm": 1.3610819578170776,
      "learning_rate": 4.7680412371134016e-05,
      "loss": 0.095,
      "step": 1405
    },
    {
      "epoch": 0.22652757078986588,
      "grad_norm": 1.1854392290115356,
      "learning_rate": 4.735824742268041e-05,
      "loss": 0.1072,
      "step": 1406
    },
    {
      "epoch": 0.22668868570507913,
      "grad_norm": 1.2084473371505737,
      "learning_rate": 4.7036082474226804e-05,
      "loss": 0.0885,
      "step": 1407
    },
    {
      "epoch": 0.22684980062029242,
      "grad_norm": 1.2506027221679688,
      "learning_rate": 4.67139175257732e-05,
      "loss": 0.1184,
      "step": 1408
    },
    {
      "epoch": 0.2270109155355057,
      "grad_norm": 1.3890687227249146,
      "learning_rate": 4.639175257731959e-05,
      "loss": 0.0863,
      "step": 1409
    },
    {
      "epoch": 0.227172030450719,
      "grad_norm": 1.6172761917114258,
      "learning_rate": 4.6069587628865986e-05,
      "loss": 0.0918,
      "step": 1410
    },
    {
      "epoch": 0.22733314536593224,
      "grad_norm": 0.6877086758613586,
      "learning_rate": 4.5747422680412366e-05,
      "loss": 0.07,
      "step": 1411
    },
    {
      "epoch": 0.22749426028114553,
      "grad_norm": 1.4763164520263672,
      "learning_rate": 4.542525773195876e-05,
      "loss": 0.0716,
      "step": 1412
    },
    {
      "epoch": 0.2276553751963588,
      "grad_norm": 0.8782684206962585,
      "learning_rate": 4.5103092783505154e-05,
      "loss": 0.0812,
      "step": 1413
    },
    {
      "epoch": 0.22781649011157207,
      "grad_norm": 1.0436925888061523,
      "learning_rate": 4.478092783505155e-05,
      "loss": 0.0966,
      "step": 1414
    },
    {
      "epoch": 0.22797760502678535,
      "grad_norm": 0.5551279783248901,
      "learning_rate": 4.445876288659794e-05,
      "loss": 0.0403,
      "step": 1415
    },
    {
      "epoch": 0.22813871994199864,
      "grad_norm": 1.1792809963226318,
      "learning_rate": 4.4136597938144336e-05,
      "loss": 0.1047,
      "step": 1416
    },
    {
      "epoch": 0.22829983485721192,
      "grad_norm": 2.3763513565063477,
      "learning_rate": 4.381443298969072e-05,
      "loss": 0.1325,
      "step": 1417
    },
    {
      "epoch": 0.22846094977242518,
      "grad_norm": 1.4632943868637085,
      "learning_rate": 4.349226804123711e-05,
      "loss": 0.106,
      "step": 1418
    },
    {
      "epoch": 0.22862206468763846,
      "grad_norm": 1.550316572189331,
      "learning_rate": 4.3170103092783505e-05,
      "loss": 0.1058,
      "step": 1419
    },
    {
      "epoch": 0.22878317960285174,
      "grad_norm": 0.7667373418807983,
      "learning_rate": 4.28479381443299e-05,
      "loss": 0.073,
      "step": 1420
    },
    {
      "epoch": 0.228944294518065,
      "grad_norm": 0.8955627679824829,
      "learning_rate": 4.252577319587629e-05,
      "loss": 0.0427,
      "step": 1421
    },
    {
      "epoch": 0.22910540943327828,
      "grad_norm": 1.3616418838500977,
      "learning_rate": 4.2203608247422687e-05,
      "loss": 0.0822,
      "step": 1422
    },
    {
      "epoch": 0.22926652434849157,
      "grad_norm": 1.207964539527893,
      "learning_rate": 4.188144329896907e-05,
      "loss": 0.1131,
      "step": 1423
    },
    {
      "epoch": 0.22942763926370482,
      "grad_norm": 1.2083244323730469,
      "learning_rate": 4.155927835051546e-05,
      "loss": 0.0975,
      "step": 1424
    },
    {
      "epoch": 0.2295887541789181,
      "grad_norm": 1.0600011348724365,
      "learning_rate": 4.1237113402061855e-05,
      "loss": 0.0559,
      "step": 1425
    },
    {
      "epoch": 0.2297498690941314,
      "grad_norm": 0.8386572599411011,
      "learning_rate": 4.091494845360825e-05,
      "loss": 0.0887,
      "step": 1426
    },
    {
      "epoch": 0.22991098400934468,
      "grad_norm": 1.224962830543518,
      "learning_rate": 4.059278350515464e-05,
      "loss": 0.1278,
      "step": 1427
    },
    {
      "epoch": 0.23007209892455793,
      "grad_norm": 2.0781235694885254,
      "learning_rate": 4.027061855670104e-05,
      "loss": 0.1774,
      "step": 1428
    },
    {
      "epoch": 0.23023321383977122,
      "grad_norm": 1.6144038438796997,
      "learning_rate": 3.994845360824742e-05,
      "loss": 0.1483,
      "step": 1429
    },
    {
      "epoch": 0.2303943287549845,
      "grad_norm": 1.9319974184036255,
      "learning_rate": 3.962628865979381e-05,
      "loss": 0.1206,
      "step": 1430
    },
    {
      "epoch": 0.23055544367019776,
      "grad_norm": 1.4539518356323242,
      "learning_rate": 3.9304123711340205e-05,
      "loss": 0.0907,
      "step": 1431
    },
    {
      "epoch": 0.23071655858541104,
      "grad_norm": 0.9339103698730469,
      "learning_rate": 3.89819587628866e-05,
      "loss": 0.0957,
      "step": 1432
    },
    {
      "epoch": 0.23087767350062433,
      "grad_norm": 0.8105632066726685,
      "learning_rate": 3.865979381443299e-05,
      "loss": 0.0616,
      "step": 1433
    },
    {
      "epoch": 0.2310387884158376,
      "grad_norm": 4.045145511627197,
      "learning_rate": 3.833762886597939e-05,
      "loss": 0.077,
      "step": 1434
    },
    {
      "epoch": 0.23119990333105087,
      "grad_norm": 1.8192006349563599,
      "learning_rate": 3.801546391752577e-05,
      "loss": 0.0872,
      "step": 1435
    },
    {
      "epoch": 0.23136101824626415,
      "grad_norm": 0.7488523721694946,
      "learning_rate": 3.769329896907216e-05,
      "loss": 0.0697,
      "step": 1436
    },
    {
      "epoch": 0.23152213316147743,
      "grad_norm": 1.0170838832855225,
      "learning_rate": 3.7371134020618556e-05,
      "loss": 0.0747,
      "step": 1437
    },
    {
      "epoch": 0.2316832480766907,
      "grad_norm": 1.0464186668395996,
      "learning_rate": 3.704896907216495e-05,
      "loss": 0.1183,
      "step": 1438
    },
    {
      "epoch": 0.23184436299190397,
      "grad_norm": 1.025654673576355,
      "learning_rate": 3.6726804123711344e-05,
      "loss": 0.0557,
      "step": 1439
    },
    {
      "epoch": 0.23200547790711726,
      "grad_norm": 1.0756322145462036,
      "learning_rate": 3.640463917525774e-05,
      "loss": 0.0918,
      "step": 1440
    },
    {
      "epoch": 0.23216659282233051,
      "grad_norm": 1.0448802709579468,
      "learning_rate": 3.608247422680412e-05,
      "loss": 0.047,
      "step": 1441
    },
    {
      "epoch": 0.2323277077375438,
      "grad_norm": 0.745361864566803,
      "learning_rate": 3.576030927835051e-05,
      "loss": 0.0558,
      "step": 1442
    },
    {
      "epoch": 0.23248882265275708,
      "grad_norm": 1.1369694471359253,
      "learning_rate": 3.5438144329896906e-05,
      "loss": 0.1104,
      "step": 1443
    },
    {
      "epoch": 0.23264993756797037,
      "grad_norm": 1.3019462823867798,
      "learning_rate": 3.51159793814433e-05,
      "loss": 0.0722,
      "step": 1444
    },
    {
      "epoch": 0.23281105248318362,
      "grad_norm": 0.9838494658470154,
      "learning_rate": 3.4793814432989694e-05,
      "loss": 0.0937,
      "step": 1445
    },
    {
      "epoch": 0.2329721673983969,
      "grad_norm": 1.513726830482483,
      "learning_rate": 3.447164948453609e-05,
      "loss": 0.0873,
      "step": 1446
    },
    {
      "epoch": 0.2331332823136102,
      "grad_norm": 1.4574129581451416,
      "learning_rate": 3.414948453608247e-05,
      "loss": 0.1584,
      "step": 1447
    },
    {
      "epoch": 0.23329439722882345,
      "grad_norm": 1.5163297653198242,
      "learning_rate": 3.382731958762886e-05,
      "loss": 0.1348,
      "step": 1448
    },
    {
      "epoch": 0.23345551214403673,
      "grad_norm": 1.308436393737793,
      "learning_rate": 3.3505154639175256e-05,
      "loss": 0.0729,
      "step": 1449
    },
    {
      "epoch": 0.23361662705925001,
      "grad_norm": 1.6566795110702515,
      "learning_rate": 3.318298969072165e-05,
      "loss": 0.1424,
      "step": 1450
    },
    {
      "epoch": 0.2337777419744633,
      "grad_norm": 1.9239797592163086,
      "learning_rate": 3.2860824742268044e-05,
      "loss": 0.1184,
      "step": 1451
    },
    {
      "epoch": 0.23393885688967656,
      "grad_norm": 0.9623520374298096,
      "learning_rate": 3.253865979381444e-05,
      "loss": 0.0709,
      "step": 1452
    },
    {
      "epoch": 0.23409997180488984,
      "grad_norm": 1.1558138132095337,
      "learning_rate": 3.221649484536082e-05,
      "loss": 0.0951,
      "step": 1453
    },
    {
      "epoch": 0.23426108672010312,
      "grad_norm": 0.8816866874694824,
      "learning_rate": 3.189432989690721e-05,
      "loss": 0.0799,
      "step": 1454
    },
    {
      "epoch": 0.23442220163531638,
      "grad_norm": 1.1119064092636108,
      "learning_rate": 3.157216494845361e-05,
      "loss": 0.1082,
      "step": 1455
    },
    {
      "epoch": 0.23458331655052966,
      "grad_norm": 1.4108198881149292,
      "learning_rate": 3.125e-05,
      "loss": 0.0574,
      "step": 1456
    },
    {
      "epoch": 0.23474443146574295,
      "grad_norm": 1.1340919733047485,
      "learning_rate": 3.0927835051546395e-05,
      "loss": 0.0768,
      "step": 1457
    },
    {
      "epoch": 0.2349055463809562,
      "grad_norm": 0.767833411693573,
      "learning_rate": 3.060567010309278e-05,
      "loss": 0.0595,
      "step": 1458
    },
    {
      "epoch": 0.2350666612961695,
      "grad_norm": 1.0850296020507812,
      "learning_rate": 3.0283505154639176e-05,
      "loss": 0.0542,
      "step": 1459
    },
    {
      "epoch": 0.23522777621138277,
      "grad_norm": 1.041465401649475,
      "learning_rate": 2.996134020618557e-05,
      "loss": 0.1076,
      "step": 1460
    },
    {
      "epoch": 0.23538889112659606,
      "grad_norm": 1.088520884513855,
      "learning_rate": 2.9639175257731957e-05,
      "loss": 0.0987,
      "step": 1461
    },
    {
      "epoch": 0.2355500060418093,
      "grad_norm": 1.7412065267562866,
      "learning_rate": 2.931701030927835e-05,
      "loss": 0.1148,
      "step": 1462
    },
    {
      "epoch": 0.2357111209570226,
      "grad_norm": 0.6315349340438843,
      "learning_rate": 2.8994845360824745e-05,
      "loss": 0.0465,
      "step": 1463
    },
    {
      "epoch": 0.23587223587223588,
      "grad_norm": 1.0015218257904053,
      "learning_rate": 2.8672680412371132e-05,
      "loss": 0.0732,
      "step": 1464
    },
    {
      "epoch": 0.23603335078744914,
      "grad_norm": 1.0135010480880737,
      "learning_rate": 2.8350515463917526e-05,
      "loss": 0.0802,
      "step": 1465
    },
    {
      "epoch": 0.23619446570266242,
      "grad_norm": 0.9643453359603882,
      "learning_rate": 2.802835051546392e-05,
      "loss": 0.11,
      "step": 1466
    },
    {
      "epoch": 0.2363555806178757,
      "grad_norm": 1.0107061862945557,
      "learning_rate": 2.7706185567010307e-05,
      "loss": 0.0674,
      "step": 1467
    },
    {
      "epoch": 0.236516695533089,
      "grad_norm": 1.5476065874099731,
      "learning_rate": 2.73840206185567e-05,
      "loss": 0.1349,
      "step": 1468
    },
    {
      "epoch": 0.23667781044830224,
      "grad_norm": 0.9436169266700745,
      "learning_rate": 2.7061855670103095e-05,
      "loss": 0.0522,
      "step": 1469
    },
    {
      "epoch": 0.23683892536351553,
      "grad_norm": 1.1458531618118286,
      "learning_rate": 2.6739690721649483e-05,
      "loss": 0.0747,
      "step": 1470
    },
    {
      "epoch": 0.2370000402787288,
      "grad_norm": 2.0589640140533447,
      "learning_rate": 2.6417525773195876e-05,
      "loss": 0.1237,
      "step": 1471
    },
    {
      "epoch": 0.23716115519394207,
      "grad_norm": 1.4040879011154175,
      "learning_rate": 2.609536082474227e-05,
      "loss": 0.1071,
      "step": 1472
    },
    {
      "epoch": 0.23732227010915535,
      "grad_norm": 0.9237440228462219,
      "learning_rate": 2.5773195876288658e-05,
      "loss": 0.0804,
      "step": 1473
    },
    {
      "epoch": 0.23748338502436864,
      "grad_norm": 2.2145161628723145,
      "learning_rate": 2.545103092783505e-05,
      "loss": 0.1401,
      "step": 1474
    },
    {
      "epoch": 0.2376444999395819,
      "grad_norm": 1.0903226137161255,
      "learning_rate": 2.5128865979381446e-05,
      "loss": 0.1052,
      "step": 1475
    },
    {
      "epoch": 0.23780561485479518,
      "grad_norm": 0.7773158550262451,
      "learning_rate": 2.4806701030927833e-05,
      "loss": 0.0824,
      "step": 1476
    },
    {
      "epoch": 0.23796672977000846,
      "grad_norm": 0.9774087071418762,
      "learning_rate": 2.4484536082474227e-05,
      "loss": 0.0989,
      "step": 1477
    },
    {
      "epoch": 0.23812784468522175,
      "grad_norm": 1.2272827625274658,
      "learning_rate": 2.416237113402062e-05,
      "loss": 0.0556,
      "step": 1478
    },
    {
      "epoch": 0.238288959600435,
      "grad_norm": 0.5554375648498535,
      "learning_rate": 2.3840206185567008e-05,
      "loss": 0.0378,
      "step": 1479
    },
    {
      "epoch": 0.23845007451564829,
      "grad_norm": 0.6086856722831726,
      "learning_rate": 2.3518041237113402e-05,
      "loss": 0.0374,
      "step": 1480
    },
    {
      "epoch": 0.23861118943086157,
      "grad_norm": 1.1820995807647705,
      "learning_rate": 2.3195876288659796e-05,
      "loss": 0.1068,
      "step": 1481
    },
    {
      "epoch": 0.23877230434607483,
      "grad_norm": 1.1123602390289307,
      "learning_rate": 2.2873711340206183e-05,
      "loss": 0.0816,
      "step": 1482
    },
    {
      "epoch": 0.2389334192612881,
      "grad_norm": 1.2646170854568481,
      "learning_rate": 2.2551546391752577e-05,
      "loss": 0.0994,
      "step": 1483
    },
    {
      "epoch": 0.2390945341765014,
      "grad_norm": 1.4455879926681519,
      "learning_rate": 2.222938144329897e-05,
      "loss": 0.1371,
      "step": 1484
    },
    {
      "epoch": 0.23925564909171468,
      "grad_norm": 1.0216974020004272,
      "learning_rate": 2.190721649484536e-05,
      "loss": 0.0702,
      "step": 1485
    },
    {
      "epoch": 0.23941676400692793,
      "grad_norm": 0.6822994351387024,
      "learning_rate": 2.1585051546391752e-05,
      "loss": 0.0517,
      "step": 1486
    },
    {
      "epoch": 0.23957787892214122,
      "grad_norm": 1.494274616241455,
      "learning_rate": 2.1262886597938146e-05,
      "loss": 0.1176,
      "step": 1487
    },
    {
      "epoch": 0.2397389938373545,
      "grad_norm": 1.2852518558502197,
      "learning_rate": 2.0940721649484534e-05,
      "loss": 0.1202,
      "step": 1488
    },
    {
      "epoch": 0.23990010875256776,
      "grad_norm": 1.2811833620071411,
      "learning_rate": 2.0618556701030927e-05,
      "loss": 0.1303,
      "step": 1489
    },
    {
      "epoch": 0.24006122366778104,
      "grad_norm": 0.9083079695701599,
      "learning_rate": 2.029639175257732e-05,
      "loss": 0.0766,
      "step": 1490
    },
    {
      "epoch": 0.24022233858299433,
      "grad_norm": 1.4425337314605713,
      "learning_rate": 1.997422680412371e-05,
      "loss": 0.1263,
      "step": 1491
    },
    {
      "epoch": 0.24038345349820758,
      "grad_norm": 1.9075287580490112,
      "learning_rate": 1.9652061855670103e-05,
      "loss": 0.0968,
      "step": 1492
    },
    {
      "epoch": 0.24054456841342087,
      "grad_norm": 0.8525300025939941,
      "learning_rate": 1.9329896907216497e-05,
      "loss": 0.0536,
      "step": 1493
    },
    {
      "epoch": 0.24070568332863415,
      "grad_norm": 0.8756606578826904,
      "learning_rate": 1.9007731958762884e-05,
      "loss": 0.0359,
      "step": 1494
    },
    {
      "epoch": 0.24086679824384744,
      "grad_norm": 1.0167772769927979,
      "learning_rate": 1.8685567010309278e-05,
      "loss": 0.1003,
      "step": 1495
    },
    {
      "epoch": 0.2410279131590607,
      "grad_norm": 1.7952358722686768,
      "learning_rate": 1.8363402061855672e-05,
      "loss": 0.0406,
      "step": 1496
    },
    {
      "epoch": 0.24118902807427398,
      "grad_norm": 1.244053840637207,
      "learning_rate": 1.804123711340206e-05,
      "loss": 0.1693,
      "step": 1497
    },
    {
      "epoch": 0.24135014298948726,
      "grad_norm": 2.4146580696105957,
      "learning_rate": 1.7719072164948453e-05,
      "loss": 0.1225,
      "step": 1498
    },
    {
      "epoch": 0.24151125790470052,
      "grad_norm": 1.2574971914291382,
      "learning_rate": 1.7396907216494847e-05,
      "loss": 0.0759,
      "step": 1499
    },
    {
      "epoch": 0.2416723728199138,
      "grad_norm": 1.6101030111312866,
      "learning_rate": 1.7074742268041234e-05,
      "loss": 0.1474,
      "step": 1500
    },
    {
      "epoch": 0.24183348773512708,
      "grad_norm": 2.4263498783111572,
      "learning_rate": 1.6752577319587628e-05,
      "loss": 0.1357,
      "step": 1501
    },
    {
      "epoch": 0.24199460265034037,
      "grad_norm": 1.92099130153656,
      "learning_rate": 1.6430412371134022e-05,
      "loss": 0.0815,
      "step": 1502
    },
    {
      "epoch": 0.24215571756555362,
      "grad_norm": 1.3913979530334473,
      "learning_rate": 1.610824742268041e-05,
      "loss": 0.0478,
      "step": 1503
    },
    {
      "epoch": 0.2423168324807669,
      "grad_norm": 0.980656623840332,
      "learning_rate": 1.5786082474226803e-05,
      "loss": 0.087,
      "step": 1504
    },
    {
      "epoch": 0.2424779473959802,
      "grad_norm": 1.054806113243103,
      "learning_rate": 1.5463917525773197e-05,
      "loss": 0.0725,
      "step": 1505
    },
    {
      "epoch": 0.24263906231119345,
      "grad_norm": 1.286242127418518,
      "learning_rate": 1.5141752577319588e-05,
      "loss": 0.1343,
      "step": 1506
    },
    {
      "epoch": 0.24280017722640673,
      "grad_norm": 1.4401392936706543,
      "learning_rate": 1.4819587628865979e-05,
      "loss": 0.1596,
      "step": 1507
    },
    {
      "epoch": 0.24296129214162002,
      "grad_norm": 0.6223215460777283,
      "learning_rate": 1.4497422680412372e-05,
      "loss": 0.044,
      "step": 1508
    },
    {
      "epoch": 0.24312240705683327,
      "grad_norm": 1.6968927383422852,
      "learning_rate": 1.4175257731958763e-05,
      "loss": 0.1169,
      "step": 1509
    },
    {
      "epoch": 0.24328352197204656,
      "grad_norm": 0.504501223564148,
      "learning_rate": 1.3853092783505154e-05,
      "loss": 0.0633,
      "step": 1510
    },
    {
      "epoch": 0.24344463688725984,
      "grad_norm": 1.2615749835968018,
      "learning_rate": 1.3530927835051548e-05,
      "loss": 0.1009,
      "step": 1511
    },
    {
      "epoch": 0.24360575180247312,
      "grad_norm": 1.1487987041473389,
      "learning_rate": 1.3208762886597938e-05,
      "loss": 0.0759,
      "step": 1512
    },
    {
      "epoch": 0.24376686671768638,
      "grad_norm": 1.2025655508041382,
      "learning_rate": 1.2886597938144329e-05,
      "loss": 0.0935,
      "step": 1513
    },
    {
      "epoch": 0.24392798163289967,
      "grad_norm": 0.8999623656272888,
      "learning_rate": 1.2564432989690723e-05,
      "loss": 0.0828,
      "step": 1514
    },
    {
      "epoch": 0.24408909654811295,
      "grad_norm": 0.8504254221916199,
      "learning_rate": 1.2242268041237113e-05,
      "loss": 0.0719,
      "step": 1515
    },
    {
      "epoch": 0.2442502114633262,
      "grad_norm": 1.0178537368774414,
      "learning_rate": 1.1920103092783504e-05,
      "loss": 0.0635,
      "step": 1516
    },
    {
      "epoch": 0.2444113263785395,
      "grad_norm": 0.7066934704780579,
      "learning_rate": 1.1597938144329898e-05,
      "loss": 0.0842,
      "step": 1517
    },
    {
      "epoch": 0.24457244129375277,
      "grad_norm": 0.8995494842529297,
      "learning_rate": 1.1275773195876289e-05,
      "loss": 0.041,
      "step": 1518
    },
    {
      "epoch": 0.24473355620896606,
      "grad_norm": 0.603967547416687,
      "learning_rate": 1.095360824742268e-05,
      "loss": 0.0643,
      "step": 1519
    },
    {
      "epoch": 0.24489467112417931,
      "grad_norm": 0.7761855721473694,
      "learning_rate": 1.0631443298969073e-05,
      "loss": 0.0639,
      "step": 1520
    },
    {
      "epoch": 0.2450557860393926,
      "grad_norm": 1.3017266988754272,
      "learning_rate": 1.0309278350515464e-05,
      "loss": 0.0862,
      "step": 1521
    },
    {
      "epoch": 0.24521690095460588,
      "grad_norm": 1.7294347286224365,
      "learning_rate": 9.987113402061854e-06,
      "loss": 0.1575,
      "step": 1522
    },
    {
      "epoch": 0.24537801586981914,
      "grad_norm": 1.1872060298919678,
      "learning_rate": 9.664948453608248e-06,
      "loss": 0.0755,
      "step": 1523
    },
    {
      "epoch": 0.24553913078503242,
      "grad_norm": 0.7910639643669128,
      "learning_rate": 9.342783505154639e-06,
      "loss": 0.0687,
      "step": 1524
    },
    {
      "epoch": 0.2457002457002457,
      "grad_norm": 1.184722661972046,
      "learning_rate": 9.02061855670103e-06,
      "loss": 0.0988,
      "step": 1525
    },
    {
      "epoch": 0.24586136061545896,
      "grad_norm": 1.3014708757400513,
      "learning_rate": 8.698453608247423e-06,
      "loss": 0.1169,
      "step": 1526
    },
    {
      "epoch": 0.24602247553067225,
      "grad_norm": 0.7871626019477844,
      "learning_rate": 8.376288659793814e-06,
      "loss": 0.0738,
      "step": 1527
    },
    {
      "epoch": 0.24618359044588553,
      "grad_norm": 1.9334371089935303,
      "learning_rate": 8.054123711340205e-06,
      "loss": 0.1904,
      "step": 1528
    },
    {
      "epoch": 0.24634470536109881,
      "grad_norm": 0.9029141664505005,
      "learning_rate": 7.731958762886599e-06,
      "loss": 0.0854,
      "step": 1529
    },
    {
      "epoch": 0.24650582027631207,
      "grad_norm": 1.4193305969238281,
      "learning_rate": 7.409793814432989e-06,
      "loss": 0.0657,
      "step": 1530
    },
    {
      "epoch": 0.24666693519152535,
      "grad_norm": 1.159244179725647,
      "learning_rate": 7.0876288659793815e-06,
      "loss": 0.0917,
      "step": 1531
    },
    {
      "epoch": 0.24682805010673864,
      "grad_norm": 0.743968665599823,
      "learning_rate": 6.765463917525774e-06,
      "loss": 0.0762,
      "step": 1532
    },
    {
      "epoch": 0.2469891650219519,
      "grad_norm": 0.5763382911682129,
      "learning_rate": 6.443298969072164e-06,
      "loss": 0.0341,
      "step": 1533
    },
    {
      "epoch": 0.24715027993716518,
      "grad_norm": 0.5782284140586853,
      "learning_rate": 6.121134020618557e-06,
      "loss": 0.0744,
      "step": 1534
    },
    {
      "epoch": 0.24731139485237846,
      "grad_norm": 0.677263081073761,
      "learning_rate": 5.798969072164949e-06,
      "loss": 0.0621,
      "step": 1535
    },
    {
      "epoch": 0.24747250976759175,
      "grad_norm": 0.7201213240623474,
      "learning_rate": 5.47680412371134e-06,
      "loss": 0.084,
      "step": 1536
    },
    {
      "epoch": 0.247633624682805,
      "grad_norm": 0.9033128023147583,
      "learning_rate": 5.154639175257732e-06,
      "loss": 0.0399,
      "step": 1537
    },
    {
      "epoch": 0.2477947395980183,
      "grad_norm": 1.0809634923934937,
      "learning_rate": 4.832474226804124e-06,
      "loss": 0.0748,
      "step": 1538
    },
    {
      "epoch": 0.24795585451323157,
      "grad_norm": 0.8559706807136536,
      "learning_rate": 4.510309278350515e-06,
      "loss": 0.0627,
      "step": 1539
    },
    {
      "epoch": 0.24811696942844483,
      "grad_norm": 0.8022115230560303,
      "learning_rate": 4.188144329896907e-06,
      "loss": 0.0878,
      "step": 1540
    },
    {
      "epoch": 0.2482780843436581,
      "grad_norm": 1.100559115409851,
      "learning_rate": 3.865979381443299e-06,
      "loss": 0.0955,
      "step": 1541
    },
    {
      "epoch": 0.2484391992588714,
      "grad_norm": 0.7851197123527527,
      "learning_rate": 3.5438144329896908e-06,
      "loss": 0.0743,
      "step": 1542
    },
    {
      "epoch": 0.24860031417408465,
      "grad_norm": 1.2847042083740234,
      "learning_rate": 3.221649484536082e-06,
      "loss": 0.0594,
      "step": 1543
    },
    {
      "epoch": 0.24876142908929794,
      "grad_norm": 1.3674824237823486,
      "learning_rate": 2.8994845360824745e-06,
      "loss": 0.1092,
      "step": 1544
    },
    {
      "epoch": 0.24892254400451122,
      "grad_norm": 1.274596929550171,
      "learning_rate": 2.577319587628866e-06,
      "loss": 0.0915,
      "step": 1545
    },
    {
      "epoch": 0.2490836589197245,
      "grad_norm": 1.1092267036437988,
      "learning_rate": 2.2551546391752574e-06,
      "loss": 0.077,
      "step": 1546
    },
    {
      "epoch": 0.24924477383493776,
      "grad_norm": 1.5741175413131714,
      "learning_rate": 1.9329896907216497e-06,
      "loss": 0.1352,
      "step": 1547
    },
    {
      "epoch": 0.24940588875015104,
      "grad_norm": 1.043892741203308,
      "learning_rate": 1.610824742268041e-06,
      "loss": 0.0913,
      "step": 1548
    },
    {
      "epoch": 0.24956700366536433,
      "grad_norm": 1.5996325016021729,
      "learning_rate": 1.288659793814433e-06,
      "loss": 0.1037,
      "step": 1549
    },
    {
      "epoch": 0.24972811858057758,
      "grad_norm": 1.2329503297805786,
      "learning_rate": 9.664948453608248e-07,
      "loss": 0.0925,
      "step": 1550
    }
  ],
  "logging_steps": 1,
  "max_steps": 1552,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2725128988909568e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
