{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.0010013754848654471,
  "eval_steps": 500,
  "global_step": 712,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 1.4064262427885495e-06,
      "grad_norm": 51.62283706665039,
      "learning_rate": 1e-08,
      "loss": 7.2682,
      "step": 1
    },
    {
      "epoch": 2.812852485577099e-06,
      "grad_norm": 118.96514129638672,
      "learning_rate": 9.985955056179776e-09,
      "loss": 9.6986,
      "step": 2
    },
    {
      "epoch": 4.219278728365648e-06,
      "grad_norm": 31.046594619750977,
      "learning_rate": 9.971910112359552e-09,
      "loss": 7.9233,
      "step": 3
    },
    {
      "epoch": 5.625704971154198e-06,
      "grad_norm": 42.8705940246582,
      "learning_rate": 9.957865168539326e-09,
      "loss": 7.6541,
      "step": 4
    },
    {
      "epoch": 7.032131213942747e-06,
      "grad_norm": 142.6917724609375,
      "learning_rate": 9.943820224719102e-09,
      "loss": 9.2283,
      "step": 5
    },
    {
      "epoch": 8.438557456731297e-06,
      "grad_norm": 39.187381744384766,
      "learning_rate": 9.929775280898876e-09,
      "loss": 7.7075,
      "step": 6
    },
    {
      "epoch": 9.844983699519845e-06,
      "grad_norm": 46.082218170166016,
      "learning_rate": 9.915730337078652e-09,
      "loss": 8.4034,
      "step": 7
    },
    {
      "epoch": 1.1251409942308396e-05,
      "grad_norm": 87.8448486328125,
      "learning_rate": 9.901685393258427e-09,
      "loss": 9.6773,
      "step": 8
    },
    {
      "epoch": 1.2657836185096944e-05,
      "grad_norm": 23.455198287963867,
      "learning_rate": 9.887640449438201e-09,
      "loss": 5.8562,
      "step": 9
    },
    {
      "epoch": 1.4064262427885495e-05,
      "grad_norm": 45.183250427246094,
      "learning_rate": 9.873595505617979e-09,
      "loss": 6.978,
      "step": 10
    },
    {
      "epoch": 1.5470688670674045e-05,
      "grad_norm": 44.941532135009766,
      "learning_rate": 9.859550561797753e-09,
      "loss": 7.3963,
      "step": 11
    },
    {
      "epoch": 1.6877114913462593e-05,
      "grad_norm": 124.74394989013672,
      "learning_rate": 9.845505617977529e-09,
      "loss": 10.6965,
      "step": 12
    },
    {
      "epoch": 1.8283541156251142e-05,
      "grad_norm": 70.72818756103516,
      "learning_rate": 9.831460674157303e-09,
      "loss": 7.7764,
      "step": 13
    },
    {
      "epoch": 1.968996739903969e-05,
      "grad_norm": 66.97817993164062,
      "learning_rate": 9.817415730337079e-09,
      "loss": 5.3588,
      "step": 14
    },
    {
      "epoch": 2.1096393641828243e-05,
      "grad_norm": 74.45321655273438,
      "learning_rate": 9.803370786516854e-09,
      "loss": 7.3085,
      "step": 15
    },
    {
      "epoch": 2.250281988461679e-05,
      "grad_norm": 40.62582015991211,
      "learning_rate": 9.789325842696628e-09,
      "loss": 8.5133,
      "step": 16
    },
    {
      "epoch": 2.390924612740534e-05,
      "grad_norm": 81.59519958496094,
      "learning_rate": 9.775280898876404e-09,
      "loss": 8.197,
      "step": 17
    },
    {
      "epoch": 2.531567237019389e-05,
      "grad_norm": 129.15338134765625,
      "learning_rate": 9.76123595505618e-09,
      "loss": 7.2495,
      "step": 18
    },
    {
      "epoch": 2.672209861298244e-05,
      "grad_norm": 53.96763610839844,
      "learning_rate": 9.747191011235956e-09,
      "loss": 5.4068,
      "step": 19
    },
    {
      "epoch": 2.812852485577099e-05,
      "grad_norm": 42.06624221801758,
      "learning_rate": 9.73314606741573e-09,
      "loss": 6.5343,
      "step": 20
    },
    {
      "epoch": 2.9534951098559538e-05,
      "grad_norm": 45.48374557495117,
      "learning_rate": 9.719101123595506e-09,
      "loss": 7.0608,
      "step": 21
    },
    {
      "epoch": 3.094137734134809e-05,
      "grad_norm": 90.91023254394531,
      "learning_rate": 9.705056179775281e-09,
      "loss": 7.2957,
      "step": 22
    },
    {
      "epoch": 3.234780358413664e-05,
      "grad_norm": 44.4959831237793,
      "learning_rate": 9.691011235955056e-09,
      "loss": 5.921,
      "step": 23
    },
    {
      "epoch": 3.375422982692519e-05,
      "grad_norm": 28.126136779785156,
      "learning_rate": 9.676966292134831e-09,
      "loss": 7.1157,
      "step": 24
    },
    {
      "epoch": 3.5160656069713735e-05,
      "grad_norm": 41.279029846191406,
      "learning_rate": 9.662921348314607e-09,
      "loss": 5.7549,
      "step": 25
    },
    {
      "epoch": 3.6567082312502284e-05,
      "grad_norm": 70.52799987792969,
      "learning_rate": 9.648876404494383e-09,
      "loss": 9.1526,
      "step": 26
    },
    {
      "epoch": 3.797350855529083e-05,
      "grad_norm": 128.4681854248047,
      "learning_rate": 9.634831460674157e-09,
      "loss": 8.2513,
      "step": 27
    },
    {
      "epoch": 3.937993479807938e-05,
      "grad_norm": 39.674049377441406,
      "learning_rate": 9.620786516853933e-09,
      "loss": 7.804,
      "step": 28
    },
    {
      "epoch": 4.078636104086794e-05,
      "grad_norm": 39.95722961425781,
      "learning_rate": 9.606741573033708e-09,
      "loss": 7.3429,
      "step": 29
    },
    {
      "epoch": 4.2192787283656485e-05,
      "grad_norm": 76.72451782226562,
      "learning_rate": 9.592696629213483e-09,
      "loss": 6.6579,
      "step": 30
    },
    {
      "epoch": 4.3599213526445034e-05,
      "grad_norm": 43.3753547668457,
      "learning_rate": 9.578651685393258e-09,
      "loss": 7.4597,
      "step": 31
    },
    {
      "epoch": 4.500563976923358e-05,
      "grad_norm": 35.978614807128906,
      "learning_rate": 9.564606741573034e-09,
      "loss": 5.64,
      "step": 32
    },
    {
      "epoch": 4.641206601202213e-05,
      "grad_norm": 42.37825012207031,
      "learning_rate": 9.55056179775281e-09,
      "loss": 6.3978,
      "step": 33
    },
    {
      "epoch": 4.781849225481068e-05,
      "grad_norm": 20.614267349243164,
      "learning_rate": 9.536516853932584e-09,
      "loss": 6.877,
      "step": 34
    },
    {
      "epoch": 4.922491849759923e-05,
      "grad_norm": 122.99939727783203,
      "learning_rate": 9.52247191011236e-09,
      "loss": 9.1301,
      "step": 35
    },
    {
      "epoch": 5.063134474038778e-05,
      "grad_norm": 44.443626403808594,
      "learning_rate": 9.508426966292136e-09,
      "loss": 6.976,
      "step": 36
    },
    {
      "epoch": 5.203777098317633e-05,
      "grad_norm": 59.11479187011719,
      "learning_rate": 9.49438202247191e-09,
      "loss": 6.8452,
      "step": 37
    },
    {
      "epoch": 5.344419722596488e-05,
      "grad_norm": 25.58909034729004,
      "learning_rate": 9.480337078651685e-09,
      "loss": 6.8515,
      "step": 38
    },
    {
      "epoch": 5.485062346875343e-05,
      "grad_norm": 94.99329376220703,
      "learning_rate": 9.466292134831461e-09,
      "loss": 8.0616,
      "step": 39
    },
    {
      "epoch": 5.625704971154198e-05,
      "grad_norm": 32.24498748779297,
      "learning_rate": 9.452247191011237e-09,
      "loss": 5.5961,
      "step": 40
    },
    {
      "epoch": 5.766347595433053e-05,
      "grad_norm": 48.34436798095703,
      "learning_rate": 9.438202247191011e-09,
      "loss": 6.9044,
      "step": 41
    },
    {
      "epoch": 5.9069902197119075e-05,
      "grad_norm": 70.78397369384766,
      "learning_rate": 9.424157303370787e-09,
      "loss": 9.8309,
      "step": 42
    },
    {
      "epoch": 6.0476328439907624e-05,
      "grad_norm": 64.36692810058594,
      "learning_rate": 9.410112359550563e-09,
      "loss": 10.0117,
      "step": 43
    },
    {
      "epoch": 6.188275468269618e-05,
      "grad_norm": 120.92772674560547,
      "learning_rate": 9.396067415730337e-09,
      "loss": 8.1731,
      "step": 44
    },
    {
      "epoch": 6.328918092548473e-05,
      "grad_norm": 110.4142837524414,
      "learning_rate": 9.382022471910113e-09,
      "loss": 6.6489,
      "step": 45
    },
    {
      "epoch": 6.469560716827328e-05,
      "grad_norm": 82.792724609375,
      "learning_rate": 9.367977528089888e-09,
      "loss": 10.2957,
      "step": 46
    },
    {
      "epoch": 6.610203341106183e-05,
      "grad_norm": 51.97929382324219,
      "learning_rate": 9.353932584269662e-09,
      "loss": 7.4749,
      "step": 47
    },
    {
      "epoch": 6.750845965385037e-05,
      "grad_norm": 82.58306884765625,
      "learning_rate": 9.339887640449438e-09,
      "loss": 7.4774,
      "step": 48
    },
    {
      "epoch": 6.891488589663892e-05,
      "grad_norm": 31.904272079467773,
      "learning_rate": 9.325842696629214e-09,
      "loss": 5.6604,
      "step": 49
    },
    {
      "epoch": 7.032131213942747e-05,
      "grad_norm": 49.756500244140625,
      "learning_rate": 9.31179775280899e-09,
      "loss": 7.0147,
      "step": 50
    },
    {
      "epoch": 7.172773838221602e-05,
      "grad_norm": 103.1654281616211,
      "learning_rate": 9.297752808988764e-09,
      "loss": 5.6283,
      "step": 51
    },
    {
      "epoch": 7.313416462500457e-05,
      "grad_norm": 32.12876892089844,
      "learning_rate": 9.28370786516854e-09,
      "loss": 6.0828,
      "step": 52
    },
    {
      "epoch": 7.454059086779312e-05,
      "grad_norm": 39.029048919677734,
      "learning_rate": 9.269662921348315e-09,
      "loss": 6.508,
      "step": 53
    },
    {
      "epoch": 7.594701711058167e-05,
      "grad_norm": 146.04925537109375,
      "learning_rate": 9.25561797752809e-09,
      "loss": 9.6743,
      "step": 54
    },
    {
      "epoch": 7.735344335337021e-05,
      "grad_norm": 57.3899040222168,
      "learning_rate": 9.241573033707865e-09,
      "loss": 7.581,
      "step": 55
    },
    {
      "epoch": 7.875986959615876e-05,
      "grad_norm": 44.319026947021484,
      "learning_rate": 9.22752808988764e-09,
      "loss": 5.9976,
      "step": 56
    },
    {
      "epoch": 8.016629583894732e-05,
      "grad_norm": 35.83131790161133,
      "learning_rate": 9.213483146067417e-09,
      "loss": 5.669,
      "step": 57
    },
    {
      "epoch": 8.157272208173587e-05,
      "grad_norm": 96.56909942626953,
      "learning_rate": 9.199438202247191e-09,
      "loss": 10.2106,
      "step": 58
    },
    {
      "epoch": 8.297914832452442e-05,
      "grad_norm": 95.98348999023438,
      "learning_rate": 9.185393258426967e-09,
      "loss": 14.0441,
      "step": 59
    },
    {
      "epoch": 8.438557456731297e-05,
      "grad_norm": 47.489803314208984,
      "learning_rate": 9.171348314606742e-09,
      "loss": 10.1138,
      "step": 60
    },
    {
      "epoch": 8.579200081010152e-05,
      "grad_norm": 61.82463073730469,
      "learning_rate": 9.157303370786517e-09,
      "loss": 6.7147,
      "step": 61
    },
    {
      "epoch": 8.719842705289007e-05,
      "grad_norm": 45.45029830932617,
      "learning_rate": 9.143258426966292e-09,
      "loss": 5.5787,
      "step": 62
    },
    {
      "epoch": 8.860485329567862e-05,
      "grad_norm": 97.11516571044922,
      "learning_rate": 9.129213483146066e-09,
      "loss": 8.2525,
      "step": 63
    },
    {
      "epoch": 9.001127953846716e-05,
      "grad_norm": 118.48002624511719,
      "learning_rate": 9.115168539325844e-09,
      "loss": 8.4714,
      "step": 64
    },
    {
      "epoch": 9.141770578125571e-05,
      "grad_norm": 49.977569580078125,
      "learning_rate": 9.101123595505618e-09,
      "loss": 7.35,
      "step": 65
    },
    {
      "epoch": 9.282413202404426e-05,
      "grad_norm": 54.17782211303711,
      "learning_rate": 9.087078651685394e-09,
      "loss": 7.4054,
      "step": 66
    },
    {
      "epoch": 9.423055826683281e-05,
      "grad_norm": 58.91938018798828,
      "learning_rate": 9.07303370786517e-09,
      "loss": 8.0009,
      "step": 67
    },
    {
      "epoch": 9.563698450962136e-05,
      "grad_norm": 77.05894470214844,
      "learning_rate": 9.058988764044944e-09,
      "loss": 8.9817,
      "step": 68
    },
    {
      "epoch": 9.704341075240991e-05,
      "grad_norm": 41.741512298583984,
      "learning_rate": 9.04494382022472e-09,
      "loss": 7.6481,
      "step": 69
    },
    {
      "epoch": 9.844983699519846e-05,
      "grad_norm": 72.69697570800781,
      "learning_rate": 9.030898876404493e-09,
      "loss": 7.5683,
      "step": 70
    },
    {
      "epoch": 9.9856263237987e-05,
      "grad_norm": 83.36207580566406,
      "learning_rate": 9.016853932584271e-09,
      "loss": 9.2605,
      "step": 71
    },
    {
      "epoch": 0.00010126268948077555,
      "grad_norm": 46.49713134765625,
      "learning_rate": 9.002808988764045e-09,
      "loss": 7.2096,
      "step": 72
    },
    {
      "epoch": 0.0001026691157235641,
      "grad_norm": 45.214569091796875,
      "learning_rate": 8.98876404494382e-09,
      "loss": 5.8153,
      "step": 73
    },
    {
      "epoch": 0.00010407554196635266,
      "grad_norm": 93.94303894042969,
      "learning_rate": 8.974719101123597e-09,
      "loss": 8.1817,
      "step": 74
    },
    {
      "epoch": 0.00010548196820914121,
      "grad_norm": 60.590702056884766,
      "learning_rate": 8.96067415730337e-09,
      "loss": 7.3562,
      "step": 75
    },
    {
      "epoch": 0.00010688839445192976,
      "grad_norm": 31.862398147583008,
      "learning_rate": 8.946629213483146e-09,
      "loss": 7.7335,
      "step": 76
    },
    {
      "epoch": 0.00010829482069471831,
      "grad_norm": 53.8409423828125,
      "learning_rate": 8.93258426966292e-09,
      "loss": 10.5932,
      "step": 77
    },
    {
      "epoch": 0.00010970124693750686,
      "grad_norm": 83.13310241699219,
      "learning_rate": 8.918539325842698e-09,
      "loss": 7.8985,
      "step": 78
    },
    {
      "epoch": 0.00011110767318029541,
      "grad_norm": 42.380332946777344,
      "learning_rate": 8.904494382022472e-09,
      "loss": 7.4546,
      "step": 79
    },
    {
      "epoch": 0.00011251409942308396,
      "grad_norm": 111.75152587890625,
      "learning_rate": 8.890449438202248e-09,
      "loss": 8.7237,
      "step": 80
    },
    {
      "epoch": 0.0001139205256658725,
      "grad_norm": 41.37368392944336,
      "learning_rate": 8.876404494382022e-09,
      "loss": 8.1873,
      "step": 81
    },
    {
      "epoch": 0.00011532695190866105,
      "grad_norm": 71.88609313964844,
      "learning_rate": 8.862359550561798e-09,
      "loss": 8.2161,
      "step": 82
    },
    {
      "epoch": 0.0001167333781514496,
      "grad_norm": 106.15409851074219,
      "learning_rate": 8.848314606741574e-09,
      "loss": 8.3886,
      "step": 83
    },
    {
      "epoch": 0.00011813980439423815,
      "grad_norm": 53.987548828125,
      "learning_rate": 8.834269662921348e-09,
      "loss": 10.107,
      "step": 84
    },
    {
      "epoch": 0.0001195462306370267,
      "grad_norm": 103.32427978515625,
      "learning_rate": 8.820224719101123e-09,
      "loss": 8.837,
      "step": 85
    },
    {
      "epoch": 0.00012095265687981525,
      "grad_norm": 47.58047103881836,
      "learning_rate": 8.8061797752809e-09,
      "loss": 7.2297,
      "step": 86
    },
    {
      "epoch": 0.0001223590831226038,
      "grad_norm": 66.28697204589844,
      "learning_rate": 8.792134831460675e-09,
      "loss": 7.6021,
      "step": 87
    },
    {
      "epoch": 0.00012376550936539236,
      "grad_norm": 48.447715759277344,
      "learning_rate": 8.778089887640449e-09,
      "loss": 7.6443,
      "step": 88
    },
    {
      "epoch": 0.0001251719356081809,
      "grad_norm": 82.23638916015625,
      "learning_rate": 8.764044943820225e-09,
      "loss": 8.4282,
      "step": 89
    },
    {
      "epoch": 0.00012657836185096946,
      "grad_norm": 58.689754486083984,
      "learning_rate": 8.75e-09,
      "loss": 7.0527,
      "step": 90
    },
    {
      "epoch": 0.000127984788093758,
      "grad_norm": 34.590126037597656,
      "learning_rate": 8.735955056179775e-09,
      "loss": 5.6066,
      "step": 91
    },
    {
      "epoch": 0.00012939121433654655,
      "grad_norm": 47.950618743896484,
      "learning_rate": 8.72191011235955e-09,
      "loss": 7.0509,
      "step": 92
    },
    {
      "epoch": 0.0001307976405793351,
      "grad_norm": 42.30167770385742,
      "learning_rate": 8.707865168539326e-09,
      "loss": 6.1994,
      "step": 93
    },
    {
      "epoch": 0.00013220406682212365,
      "grad_norm": 61.32732009887695,
      "learning_rate": 8.6938202247191e-09,
      "loss": 8.2591,
      "step": 94
    },
    {
      "epoch": 0.00013361049306491219,
      "grad_norm": 63.06135940551758,
      "learning_rate": 8.679775280898876e-09,
      "loss": 8.9496,
      "step": 95
    },
    {
      "epoch": 0.00013501691930770075,
      "grad_norm": 75.63656616210938,
      "learning_rate": 8.665730337078652e-09,
      "loss": 6.0091,
      "step": 96
    },
    {
      "epoch": 0.00013642334555048928,
      "grad_norm": 85.05404663085938,
      "learning_rate": 8.651685393258428e-09,
      "loss": 6.2326,
      "step": 97
    },
    {
      "epoch": 0.00013782977179327784,
      "grad_norm": 28.516016006469727,
      "learning_rate": 8.637640449438202e-09,
      "loss": 5.5096,
      "step": 98
    },
    {
      "epoch": 0.0001392361980360664,
      "grad_norm": 55.755184173583984,
      "learning_rate": 8.623595505617978e-09,
      "loss": 7.8797,
      "step": 99
    },
    {
      "epoch": 0.00014064262427885494,
      "grad_norm": 58.9558219909668,
      "learning_rate": 8.609550561797753e-09,
      "loss": 6.4621,
      "step": 100
    },
    {
      "epoch": 0.0001420490505216435,
      "grad_norm": 105.18408203125,
      "learning_rate": 8.595505617977527e-09,
      "loss": 8.0706,
      "step": 101
    },
    {
      "epoch": 0.00014345547676443204,
      "grad_norm": 97.67716979980469,
      "learning_rate": 8.581460674157303e-09,
      "loss": 10.6974,
      "step": 102
    },
    {
      "epoch": 0.0001448619030072206,
      "grad_norm": 89.12994384765625,
      "learning_rate": 8.567415730337079e-09,
      "loss": 6.923,
      "step": 103
    },
    {
      "epoch": 0.00014626832925000914,
      "grad_norm": 66.14134979248047,
      "learning_rate": 8.553370786516855e-09,
      "loss": 7.656,
      "step": 104
    },
    {
      "epoch": 0.0001476747554927977,
      "grad_norm": 93.07672119140625,
      "learning_rate": 8.539325842696629e-09,
      "loss": 7.7507,
      "step": 105
    },
    {
      "epoch": 0.00014908118173558623,
      "grad_norm": 67.08795928955078,
      "learning_rate": 8.525280898876405e-09,
      "loss": 7.607,
      "step": 106
    },
    {
      "epoch": 0.0001504876079783748,
      "grad_norm": 116.32463836669922,
      "learning_rate": 8.51123595505618e-09,
      "loss": 8.7654,
      "step": 107
    },
    {
      "epoch": 0.00015189403422116333,
      "grad_norm": 81.06063079833984,
      "learning_rate": 8.497191011235955e-09,
      "loss": 7.8933,
      "step": 108
    },
    {
      "epoch": 0.0001533004604639519,
      "grad_norm": 55.48030471801758,
      "learning_rate": 8.48314606741573e-09,
      "loss": 8.3844,
      "step": 109
    },
    {
      "epoch": 0.00015470688670674043,
      "grad_norm": 55.56242752075195,
      "learning_rate": 8.469101123595506e-09,
      "loss": 7.7443,
      "step": 110
    },
    {
      "epoch": 0.000156113312949529,
      "grad_norm": 53.479549407958984,
      "learning_rate": 8.455056179775282e-09,
      "loss": 5.8526,
      "step": 111
    },
    {
      "epoch": 0.00015751973919231753,
      "grad_norm": 73.25836944580078,
      "learning_rate": 8.441011235955056e-09,
      "loss": 8.5342,
      "step": 112
    },
    {
      "epoch": 0.0001589261654351061,
      "grad_norm": 48.793800354003906,
      "learning_rate": 8.426966292134832e-09,
      "loss": 6.463,
      "step": 113
    },
    {
      "epoch": 0.00016033259167789465,
      "grad_norm": 43.78519058227539,
      "learning_rate": 8.412921348314607e-09,
      "loss": 7.2973,
      "step": 114
    },
    {
      "epoch": 0.00016173901792068318,
      "grad_norm": 103.66072082519531,
      "learning_rate": 8.398876404494382e-09,
      "loss": 8.9092,
      "step": 115
    },
    {
      "epoch": 0.00016314544416347175,
      "grad_norm": 64.62996673583984,
      "learning_rate": 8.384831460674157e-09,
      "loss": 7.3754,
      "step": 116
    },
    {
      "epoch": 0.00016455187040626028,
      "grad_norm": 74.01990509033203,
      "learning_rate": 8.370786516853933e-09,
      "loss": 8.0043,
      "step": 117
    },
    {
      "epoch": 0.00016595829664904884,
      "grad_norm": 31.684181213378906,
      "learning_rate": 8.356741573033709e-09,
      "loss": 6.7388,
      "step": 118
    },
    {
      "epoch": 0.00016736472289183738,
      "grad_norm": 33.86749267578125,
      "learning_rate": 8.342696629213483e-09,
      "loss": 5.8394,
      "step": 119
    },
    {
      "epoch": 0.00016877114913462594,
      "grad_norm": 51.16011428833008,
      "learning_rate": 8.328651685393259e-09,
      "loss": 7.2063,
      "step": 120
    },
    {
      "epoch": 0.00017017757537741448,
      "grad_norm": 73.3387680053711,
      "learning_rate": 8.314606741573035e-09,
      "loss": 7.6093,
      "step": 121
    },
    {
      "epoch": 0.00017158400162020304,
      "grad_norm": 44.31154251098633,
      "learning_rate": 8.300561797752809e-09,
      "loss": 6.9098,
      "step": 122
    },
    {
      "epoch": 0.00017299042786299157,
      "grad_norm": 62.643455505371094,
      "learning_rate": 8.286516853932584e-09,
      "loss": 7.6501,
      "step": 123
    },
    {
      "epoch": 0.00017439685410578014,
      "grad_norm": 86.2094955444336,
      "learning_rate": 8.27247191011236e-09,
      "loss": 8.8747,
      "step": 124
    },
    {
      "epoch": 0.00017580328034856867,
      "grad_norm": 69.54679107666016,
      "learning_rate": 8.258426966292136e-09,
      "loss": 7.2289,
      "step": 125
    },
    {
      "epoch": 0.00017720970659135723,
      "grad_norm": 122.67926025390625,
      "learning_rate": 8.24438202247191e-09,
      "loss": 10.8891,
      "step": 126
    },
    {
      "epoch": 0.00017861613283414577,
      "grad_norm": 29.416501998901367,
      "learning_rate": 8.230337078651686e-09,
      "loss": 5.9516,
      "step": 127
    },
    {
      "epoch": 0.00018002255907693433,
      "grad_norm": 80.43585968017578,
      "learning_rate": 8.216292134831462e-09,
      "loss": 8.6381,
      "step": 128
    },
    {
      "epoch": 0.00018142898531972287,
      "grad_norm": 63.449127197265625,
      "learning_rate": 8.202247191011236e-09,
      "loss": 7.1929,
      "step": 129
    },
    {
      "epoch": 0.00018283541156251143,
      "grad_norm": 47.25070571899414,
      "learning_rate": 8.188202247191011e-09,
      "loss": 5.8922,
      "step": 130
    },
    {
      "epoch": 0.0001842418378053,
      "grad_norm": 83.0173110961914,
      "learning_rate": 8.174157303370787e-09,
      "loss": 7.1394,
      "step": 131
    },
    {
      "epoch": 0.00018564826404808852,
      "grad_norm": 55.62381362915039,
      "learning_rate": 8.160112359550561e-09,
      "loss": 7.4749,
      "step": 132
    },
    {
      "epoch": 0.0001870546902908771,
      "grad_norm": 54.66392517089844,
      "learning_rate": 8.146067415730337e-09,
      "loss": 7.6974,
      "step": 133
    },
    {
      "epoch": 0.00018846111653366562,
      "grad_norm": 39.958396911621094,
      "learning_rate": 8.132022471910113e-09,
      "loss": 9.2519,
      "step": 134
    },
    {
      "epoch": 0.00018986754277645418,
      "grad_norm": 77.29949188232422,
      "learning_rate": 8.117977528089889e-09,
      "loss": 5.728,
      "step": 135
    },
    {
      "epoch": 0.00019127396901924272,
      "grad_norm": 84.47882843017578,
      "learning_rate": 8.103932584269663e-09,
      "loss": 7.1681,
      "step": 136
    },
    {
      "epoch": 0.00019268039526203128,
      "grad_norm": 109.76705932617188,
      "learning_rate": 8.089887640449439e-09,
      "loss": 9.0887,
      "step": 137
    },
    {
      "epoch": 0.00019408682150481982,
      "grad_norm": 64.55268096923828,
      "learning_rate": 8.075842696629213e-09,
      "loss": 9.8173,
      "step": 138
    },
    {
      "epoch": 0.00019549324774760838,
      "grad_norm": 56.61522674560547,
      "learning_rate": 8.061797752808988e-09,
      "loss": 8.1374,
      "step": 139
    },
    {
      "epoch": 0.0001968996739903969,
      "grad_norm": 53.1976318359375,
      "learning_rate": 8.047752808988764e-09,
      "loss": 6.2242,
      "step": 140
    },
    {
      "epoch": 0.00019830610023318548,
      "grad_norm": 57.589263916015625,
      "learning_rate": 8.033707865168538e-09,
      "loss": 6.6531,
      "step": 141
    },
    {
      "epoch": 0.000199712526475974,
      "grad_norm": 84.46348571777344,
      "learning_rate": 8.019662921348316e-09,
      "loss": 11.6571,
      "step": 142
    },
    {
      "epoch": 0.00020111895271876257,
      "grad_norm": 83.1822509765625,
      "learning_rate": 8.00561797752809e-09,
      "loss": 6.6743,
      "step": 143
    },
    {
      "epoch": 0.0002025253789615511,
      "grad_norm": 58.332801818847656,
      "learning_rate": 7.991573033707866e-09,
      "loss": 6.1092,
      "step": 144
    },
    {
      "epoch": 0.00020393180520433967,
      "grad_norm": 62.3929328918457,
      "learning_rate": 7.97752808988764e-09,
      "loss": 7.6279,
      "step": 145
    },
    {
      "epoch": 0.0002053382314471282,
      "grad_norm": 43.16169357299805,
      "learning_rate": 7.963483146067416e-09,
      "loss": 6.8173,
      "step": 146
    },
    {
      "epoch": 0.00020674465768991677,
      "grad_norm": 45.05072784423828,
      "learning_rate": 7.949438202247191e-09,
      "loss": 7.4157,
      "step": 147
    },
    {
      "epoch": 0.00020815108393270533,
      "grad_norm": 56.94273376464844,
      "learning_rate": 7.935393258426965e-09,
      "loss": 6.6583,
      "step": 148
    },
    {
      "epoch": 0.00020955751017549386,
      "grad_norm": 83.837890625,
      "learning_rate": 7.921348314606743e-09,
      "loss": 7.5672,
      "step": 149
    },
    {
      "epoch": 0.00021096393641828243,
      "grad_norm": 48.14291000366211,
      "learning_rate": 7.907303370786517e-09,
      "loss": 9.4726,
      "step": 150
    },
    {
      "epoch": 0.00021237036266107096,
      "grad_norm": 48.07631301879883,
      "learning_rate": 7.893258426966293e-09,
      "loss": 7.1271,
      "step": 151
    },
    {
      "epoch": 0.00021377678890385952,
      "grad_norm": 22.974998474121094,
      "learning_rate": 7.879213483146067e-09,
      "loss": 6.0628,
      "step": 152
    },
    {
      "epoch": 0.00021518321514664806,
      "grad_norm": 63.488319396972656,
      "learning_rate": 7.865168539325843e-09,
      "loss": 6.6742,
      "step": 153
    },
    {
      "epoch": 0.00021658964138943662,
      "grad_norm": 88.92781066894531,
      "learning_rate": 7.851123595505618e-09,
      "loss": 6.7119,
      "step": 154
    },
    {
      "epoch": 0.00021799606763222516,
      "grad_norm": 86.3648910522461,
      "learning_rate": 7.837078651685392e-09,
      "loss": 9.0151,
      "step": 155
    },
    {
      "epoch": 0.00021940249387501372,
      "grad_norm": 134.87937927246094,
      "learning_rate": 7.82303370786517e-09,
      "loss": 9.5346,
      "step": 156
    },
    {
      "epoch": 0.00022080892011780225,
      "grad_norm": 74.98220825195312,
      "learning_rate": 7.808988764044944e-09,
      "loss": 5.9282,
      "step": 157
    },
    {
      "epoch": 0.00022221534636059082,
      "grad_norm": 55.994224548339844,
      "learning_rate": 7.79494382022472e-09,
      "loss": 9.1233,
      "step": 158
    },
    {
      "epoch": 0.00022362177260337935,
      "grad_norm": 122.99669647216797,
      "learning_rate": 7.780898876404494e-09,
      "loss": 7.1288,
      "step": 159
    },
    {
      "epoch": 0.0002250281988461679,
      "grad_norm": 64.05189514160156,
      "learning_rate": 7.76685393258427e-09,
      "loss": 6.8073,
      "step": 160
    },
    {
      "epoch": 0.00022643462508895645,
      "grad_norm": 54.734134674072266,
      "learning_rate": 7.752808988764045e-09,
      "loss": 6.7829,
      "step": 161
    },
    {
      "epoch": 0.000227841051331745,
      "grad_norm": 26.882511138916016,
      "learning_rate": 7.73876404494382e-09,
      "loss": 7.4981,
      "step": 162
    },
    {
      "epoch": 0.00022924747757453357,
      "grad_norm": 72.24102020263672,
      "learning_rate": 7.724719101123597e-09,
      "loss": 7.6567,
      "step": 163
    },
    {
      "epoch": 0.0002306539038173221,
      "grad_norm": 56.04206848144531,
      "learning_rate": 7.710674157303371e-09,
      "loss": 10.2246,
      "step": 164
    },
    {
      "epoch": 0.00023206033006011067,
      "grad_norm": 34.12338638305664,
      "learning_rate": 7.696629213483147e-09,
      "loss": 6.8092,
      "step": 165
    },
    {
      "epoch": 0.0002334667563028992,
      "grad_norm": 71.61689758300781,
      "learning_rate": 7.682584269662921e-09,
      "loss": 7.3913,
      "step": 166
    },
    {
      "epoch": 0.00023487318254568777,
      "grad_norm": 56.895992279052734,
      "learning_rate": 7.668539325842697e-09,
      "loss": 7.6091,
      "step": 167
    },
    {
      "epoch": 0.0002362796087884763,
      "grad_norm": 97.06061553955078,
      "learning_rate": 7.654494382022472e-09,
      "loss": 7.648,
      "step": 168
    },
    {
      "epoch": 0.00023768603503126486,
      "grad_norm": 71.55785369873047,
      "learning_rate": 7.640449438202247e-09,
      "loss": 6.5191,
      "step": 169
    },
    {
      "epoch": 0.0002390924612740534,
      "grad_norm": 87.06892395019531,
      "learning_rate": 7.626404494382022e-09,
      "loss": 8.3087,
      "step": 170
    },
    {
      "epoch": 0.00024049888751684196,
      "grad_norm": 116.77279663085938,
      "learning_rate": 7.612359550561798e-09,
      "loss": 8.4915,
      "step": 171
    },
    {
      "epoch": 0.0002419053137596305,
      "grad_norm": 84.61963653564453,
      "learning_rate": 7.598314606741574e-09,
      "loss": 9.6403,
      "step": 172
    },
    {
      "epoch": 0.00024331174000241906,
      "grad_norm": 44.57888412475586,
      "learning_rate": 7.584269662921348e-09,
      "loss": 7.3871,
      "step": 173
    },
    {
      "epoch": 0.0002447181662452076,
      "grad_norm": 56.89443588256836,
      "learning_rate": 7.570224719101124e-09,
      "loss": 7.9843,
      "step": 174
    },
    {
      "epoch": 0.00024612459248799613,
      "grad_norm": 129.2681427001953,
      "learning_rate": 7.5561797752809e-09,
      "loss": 8.5309,
      "step": 175
    },
    {
      "epoch": 0.0002475310187307847,
      "grad_norm": 83.94017791748047,
      "learning_rate": 7.542134831460674e-09,
      "loss": 8.427,
      "step": 176
    },
    {
      "epoch": 0.00024893744497357325,
      "grad_norm": 29.044940948486328,
      "learning_rate": 7.52808988764045e-09,
      "loss": 6.7175,
      "step": 177
    },
    {
      "epoch": 0.0002503438712163618,
      "grad_norm": 76.95942687988281,
      "learning_rate": 7.514044943820225e-09,
      "loss": 7.4299,
      "step": 178
    },
    {
      "epoch": 0.0002517502974591504,
      "grad_norm": 49.84964370727539,
      "learning_rate": 7.500000000000001e-09,
      "loss": 6.9321,
      "step": 179
    },
    {
      "epoch": 0.0002531567237019389,
      "grad_norm": 53.16878128051758,
      "learning_rate": 7.485955056179775e-09,
      "loss": 6.3343,
      "step": 180
    },
    {
      "epoch": 0.00025456314994472745,
      "grad_norm": 102.35569763183594,
      "learning_rate": 7.471910112359551e-09,
      "loss": 8.5088,
      "step": 181
    },
    {
      "epoch": 0.000255969576187516,
      "grad_norm": 57.608394622802734,
      "learning_rate": 7.457865168539327e-09,
      "loss": 6.3649,
      "step": 182
    },
    {
      "epoch": 0.00025737600243030457,
      "grad_norm": 65.00355529785156,
      "learning_rate": 7.443820224719101e-09,
      "loss": 6.6975,
      "step": 183
    },
    {
      "epoch": 0.0002587824286730931,
      "grad_norm": 112.96483612060547,
      "learning_rate": 7.4297752808988765e-09,
      "loss": 8.1499,
      "step": 184
    },
    {
      "epoch": 0.00026018885491588164,
      "grad_norm": 35.576438903808594,
      "learning_rate": 7.415730337078652e-09,
      "loss": 7.1049,
      "step": 185
    },
    {
      "epoch": 0.0002615952811586702,
      "grad_norm": 27.980180740356445,
      "learning_rate": 7.401685393258427e-09,
      "loss": 7.7168,
      "step": 186
    },
    {
      "epoch": 0.00026300170740145877,
      "grad_norm": 53.68272399902344,
      "learning_rate": 7.387640449438202e-09,
      "loss": 6.7483,
      "step": 187
    },
    {
      "epoch": 0.0002644081336442473,
      "grad_norm": 108.16694641113281,
      "learning_rate": 7.373595505617978e-09,
      "loss": 8.2411,
      "step": 188
    },
    {
      "epoch": 0.00026581455988703584,
      "grad_norm": 62.13187789916992,
      "learning_rate": 7.359550561797753e-09,
      "loss": 5.3456,
      "step": 189
    },
    {
      "epoch": 0.00026722098612982437,
      "grad_norm": 97.83930969238281,
      "learning_rate": 7.345505617977528e-09,
      "loss": 6.7917,
      "step": 190
    },
    {
      "epoch": 0.00026862741237261296,
      "grad_norm": 105.11962890625,
      "learning_rate": 7.331460674157303e-09,
      "loss": 9.2505,
      "step": 191
    },
    {
      "epoch": 0.0002700338386154015,
      "grad_norm": 62.17918395996094,
      "learning_rate": 7.317415730337079e-09,
      "loss": 7.214,
      "step": 192
    },
    {
      "epoch": 0.00027144026485819003,
      "grad_norm": 58.90794372558594,
      "learning_rate": 7.303370786516854e-09,
      "loss": 6.8277,
      "step": 193
    },
    {
      "epoch": 0.00027284669110097857,
      "grad_norm": 50.54172897338867,
      "learning_rate": 7.289325842696629e-09,
      "loss": 7.027,
      "step": 194
    },
    {
      "epoch": 0.00027425311734376715,
      "grad_norm": 40.695926666259766,
      "learning_rate": 7.275280898876404e-09,
      "loss": 7.5044,
      "step": 195
    },
    {
      "epoch": 0.0002756595435865557,
      "grad_norm": 31.118560791015625,
      "learning_rate": 7.26123595505618e-09,
      "loss": 5.6227,
      "step": 196
    },
    {
      "epoch": 0.0002770659698293442,
      "grad_norm": 84.63865661621094,
      "learning_rate": 7.247191011235955e-09,
      "loss": 8.4144,
      "step": 197
    },
    {
      "epoch": 0.0002784723960721328,
      "grad_norm": 64.72898864746094,
      "learning_rate": 7.23314606741573e-09,
      "loss": 10.2122,
      "step": 198
    },
    {
      "epoch": 0.00027987882231492135,
      "grad_norm": 94.22325897216797,
      "learning_rate": 7.2191011235955064e-09,
      "loss": 11.2718,
      "step": 199
    },
    {
      "epoch": 0.0002812852485577099,
      "grad_norm": 62.663414001464844,
      "learning_rate": 7.205056179775281e-09,
      "loss": 9.5736,
      "step": 200
    },
    {
      "epoch": 0.0002826916748004984,
      "grad_norm": 47.282142639160156,
      "learning_rate": 7.191011235955056e-09,
      "loss": 6.6553,
      "step": 201
    },
    {
      "epoch": 0.000284098101043287,
      "grad_norm": 46.91652297973633,
      "learning_rate": 7.176966292134831e-09,
      "loss": 6.8358,
      "step": 202
    },
    {
      "epoch": 0.00028550452728607554,
      "grad_norm": 64.88967895507812,
      "learning_rate": 7.162921348314607e-09,
      "loss": 6.9307,
      "step": 203
    },
    {
      "epoch": 0.0002869109535288641,
      "grad_norm": 46.40707015991211,
      "learning_rate": 7.148876404494382e-09,
      "loss": 7.9423,
      "step": 204
    },
    {
      "epoch": 0.0002883173797716526,
      "grad_norm": 123.08860778808594,
      "learning_rate": 7.134831460674157e-09,
      "loss": 7.5327,
      "step": 205
    },
    {
      "epoch": 0.0002897238060144412,
      "grad_norm": 59.734657287597656,
      "learning_rate": 7.1207865168539335e-09,
      "loss": 7.011,
      "step": 206
    },
    {
      "epoch": 0.00029113023225722974,
      "grad_norm": 60.64311599731445,
      "learning_rate": 7.1067415730337084e-09,
      "loss": 7.0536,
      "step": 207
    },
    {
      "epoch": 0.00029253665850001827,
      "grad_norm": 50.7897834777832,
      "learning_rate": 7.092696629213483e-09,
      "loss": 7.7283,
      "step": 208
    },
    {
      "epoch": 0.0002939430847428068,
      "grad_norm": 43.56886672973633,
      "learning_rate": 7.078651685393258e-09,
      "loss": 7.23,
      "step": 209
    },
    {
      "epoch": 0.0002953495109855954,
      "grad_norm": 50.70800018310547,
      "learning_rate": 7.064606741573034e-09,
      "loss": 7.8126,
      "step": 210
    },
    {
      "epoch": 0.00029675593722838393,
      "grad_norm": 68.22351837158203,
      "learning_rate": 7.050561797752809e-09,
      "loss": 8.9047,
      "step": 211
    },
    {
      "epoch": 0.00029816236347117247,
      "grad_norm": 72.89793395996094,
      "learning_rate": 7.036516853932584e-09,
      "loss": 8.8325,
      "step": 212
    },
    {
      "epoch": 0.00029956878971396106,
      "grad_norm": 91.53496551513672,
      "learning_rate": 7.0224719101123606e-09,
      "loss": 12.0028,
      "step": 213
    },
    {
      "epoch": 0.0003009752159567496,
      "grad_norm": 62.45453643798828,
      "learning_rate": 7.0084269662921355e-09,
      "loss": 7.4691,
      "step": 214
    },
    {
      "epoch": 0.0003023816421995381,
      "grad_norm": 69.50762939453125,
      "learning_rate": 6.9943820224719105e-09,
      "loss": 6.4355,
      "step": 215
    },
    {
      "epoch": 0.00030378806844232666,
      "grad_norm": 60.80147171020508,
      "learning_rate": 6.980337078651685e-09,
      "loss": 7.15,
      "step": 216
    },
    {
      "epoch": 0.00030519449468511525,
      "grad_norm": 89.87378692626953,
      "learning_rate": 6.966292134831461e-09,
      "loss": 7.7562,
      "step": 217
    },
    {
      "epoch": 0.0003066009209279038,
      "grad_norm": 111.03622436523438,
      "learning_rate": 6.952247191011236e-09,
      "loss": 7.498,
      "step": 218
    },
    {
      "epoch": 0.0003080073471706923,
      "grad_norm": 101.29857635498047,
      "learning_rate": 6.938202247191011e-09,
      "loss": 7.4381,
      "step": 219
    },
    {
      "epoch": 0.00030941377341348086,
      "grad_norm": 81.8961410522461,
      "learning_rate": 6.924157303370787e-09,
      "loss": 8.9075,
      "step": 220
    },
    {
      "epoch": 0.00031082019965626945,
      "grad_norm": 84.05876922607422,
      "learning_rate": 6.910112359550562e-09,
      "loss": 7.7916,
      "step": 221
    },
    {
      "epoch": 0.000312226625899058,
      "grad_norm": 40.901424407958984,
      "learning_rate": 6.8960674157303375e-09,
      "loss": 8.3325,
      "step": 222
    },
    {
      "epoch": 0.0003136330521418465,
      "grad_norm": 36.895172119140625,
      "learning_rate": 6.8820224719101125e-09,
      "loss": 7.1715,
      "step": 223
    },
    {
      "epoch": 0.00031503947838463505,
      "grad_norm": 67.43872833251953,
      "learning_rate": 6.867977528089888e-09,
      "loss": 7.2818,
      "step": 224
    },
    {
      "epoch": 0.00031644590462742364,
      "grad_norm": 91.21251678466797,
      "learning_rate": 6.853932584269663e-09,
      "loss": 7.6018,
      "step": 225
    },
    {
      "epoch": 0.0003178523308702122,
      "grad_norm": 51.819454193115234,
      "learning_rate": 6.839887640449438e-09,
      "loss": 6.3153,
      "step": 226
    },
    {
      "epoch": 0.0003192587571130007,
      "grad_norm": 78.08753204345703,
      "learning_rate": 6.825842696629213e-09,
      "loss": 7.9309,
      "step": 227
    },
    {
      "epoch": 0.0003206651833557893,
      "grad_norm": 88.12871551513672,
      "learning_rate": 6.811797752808989e-09,
      "loss": 5.7586,
      "step": 228
    },
    {
      "epoch": 0.00032207160959857783,
      "grad_norm": 33.52904510498047,
      "learning_rate": 6.797752808988764e-09,
      "loss": 7.0547,
      "step": 229
    },
    {
      "epoch": 0.00032347803584136637,
      "grad_norm": 86.23982238769531,
      "learning_rate": 6.783707865168539e-09,
      "loss": 9.0098,
      "step": 230
    },
    {
      "epoch": 0.0003248844620841549,
      "grad_norm": 80.44676208496094,
      "learning_rate": 6.769662921348315e-09,
      "loss": 5.9799,
      "step": 231
    },
    {
      "epoch": 0.0003262908883269435,
      "grad_norm": 50.905555725097656,
      "learning_rate": 6.75561797752809e-09,
      "loss": 7.6309,
      "step": 232
    },
    {
      "epoch": 0.00032769731456973203,
      "grad_norm": 94.60295104980469,
      "learning_rate": 6.741573033707865e-09,
      "loss": 7.5392,
      "step": 233
    },
    {
      "epoch": 0.00032910374081252056,
      "grad_norm": 48.3599967956543,
      "learning_rate": 6.72752808988764e-09,
      "loss": 8.445,
      "step": 234
    },
    {
      "epoch": 0.0003305101670553091,
      "grad_norm": 66.77313232421875,
      "learning_rate": 6.713483146067416e-09,
      "loss": 7.1177,
      "step": 235
    },
    {
      "epoch": 0.0003319165932980977,
      "grad_norm": 124.1497802734375,
      "learning_rate": 6.699438202247191e-09,
      "loss": 8.2179,
      "step": 236
    },
    {
      "epoch": 0.0003333230195408862,
      "grad_norm": 56.69645690917969,
      "learning_rate": 6.685393258426966e-09,
      "loss": 5.8407,
      "step": 237
    },
    {
      "epoch": 0.00033472944578367476,
      "grad_norm": 49.33139419555664,
      "learning_rate": 6.671348314606742e-09,
      "loss": 7.8836,
      "step": 238
    },
    {
      "epoch": 0.0003361358720264633,
      "grad_norm": 111.74767303466797,
      "learning_rate": 6.657303370786517e-09,
      "loss": 9.5494,
      "step": 239
    },
    {
      "epoch": 0.0003375422982692519,
      "grad_norm": 90.96778869628906,
      "learning_rate": 6.643258426966292e-09,
      "loss": 8.4779,
      "step": 240
    },
    {
      "epoch": 0.0003389487245120404,
      "grad_norm": 63.15804672241211,
      "learning_rate": 6.629213483146067e-09,
      "loss": 7.4002,
      "step": 241
    },
    {
      "epoch": 0.00034035515075482895,
      "grad_norm": 55.27478790283203,
      "learning_rate": 6.615168539325843e-09,
      "loss": 7.0575,
      "step": 242
    },
    {
      "epoch": 0.0003417615769976175,
      "grad_norm": 47.054595947265625,
      "learning_rate": 6.601123595505618e-09,
      "loss": 7.7983,
      "step": 243
    },
    {
      "epoch": 0.0003431680032404061,
      "grad_norm": 43.804962158203125,
      "learning_rate": 6.587078651685393e-09,
      "loss": 6.6091,
      "step": 244
    },
    {
      "epoch": 0.0003445744294831946,
      "grad_norm": 88.77991485595703,
      "learning_rate": 6.5730337078651694e-09,
      "loss": 9.0597,
      "step": 245
    },
    {
      "epoch": 0.00034598085572598315,
      "grad_norm": 37.26911544799805,
      "learning_rate": 6.558988764044944e-09,
      "loss": 6.9913,
      "step": 246
    },
    {
      "epoch": 0.00034738728196877174,
      "grad_norm": 47.53590393066406,
      "learning_rate": 6.544943820224719e-09,
      "loss": 7.3814,
      "step": 247
    },
    {
      "epoch": 0.00034879370821156027,
      "grad_norm": 53.13241958618164,
      "learning_rate": 6.530898876404494e-09,
      "loss": 9.828,
      "step": 248
    },
    {
      "epoch": 0.0003502001344543488,
      "grad_norm": 76.35169982910156,
      "learning_rate": 6.51685393258427e-09,
      "loss": 8.6416,
      "step": 249
    },
    {
      "epoch": 0.00035160656069713734,
      "grad_norm": 81.3721694946289,
      "learning_rate": 6.502808988764045e-09,
      "loss": 9.7694,
      "step": 250
    },
    {
      "epoch": 0.00035301298693992593,
      "grad_norm": 81.22247314453125,
      "learning_rate": 6.48876404494382e-09,
      "loss": 7.6914,
      "step": 251
    },
    {
      "epoch": 0.00035441941318271447,
      "grad_norm": 29.2895565032959,
      "learning_rate": 6.4747191011235965e-09,
      "loss": 5.5594,
      "step": 252
    },
    {
      "epoch": 0.000355825839425503,
      "grad_norm": 30.63796615600586,
      "learning_rate": 6.4606741573033715e-09,
      "loss": 6.9721,
      "step": 253
    },
    {
      "epoch": 0.00035723226566829154,
      "grad_norm": 30.22955894470215,
      "learning_rate": 6.446629213483146e-09,
      "loss": 5.4984,
      "step": 254
    },
    {
      "epoch": 0.0003586386919110801,
      "grad_norm": 67.81150817871094,
      "learning_rate": 6.432584269662921e-09,
      "loss": 7.5107,
      "step": 255
    },
    {
      "epoch": 0.00036004511815386866,
      "grad_norm": 59.14628982543945,
      "learning_rate": 6.418539325842697e-09,
      "loss": 6.6496,
      "step": 256
    },
    {
      "epoch": 0.0003614515443966572,
      "grad_norm": 58.8853759765625,
      "learning_rate": 6.404494382022472e-09,
      "loss": 8.0323,
      "step": 257
    },
    {
      "epoch": 0.00036285797063944573,
      "grad_norm": 87.63040161132812,
      "learning_rate": 6.390449438202247e-09,
      "loss": 8.145,
      "step": 258
    },
    {
      "epoch": 0.0003642643968822343,
      "grad_norm": 31.71189308166504,
      "learning_rate": 6.376404494382022e-09,
      "loss": 5.6804,
      "step": 259
    },
    {
      "epoch": 0.00036567082312502285,
      "grad_norm": 56.79777526855469,
      "learning_rate": 6.3623595505617985e-09,
      "loss": 7.7701,
      "step": 260
    },
    {
      "epoch": 0.0003670772493678114,
      "grad_norm": 110.8057861328125,
      "learning_rate": 6.3483146067415735e-09,
      "loss": 7.852,
      "step": 261
    },
    {
      "epoch": 0.0003684836756106,
      "grad_norm": 74.0119400024414,
      "learning_rate": 6.3342696629213484e-09,
      "loss": 6.9518,
      "step": 262
    },
    {
      "epoch": 0.0003698901018533885,
      "grad_norm": 35.36204147338867,
      "learning_rate": 6.320224719101124e-09,
      "loss": 5.7394,
      "step": 263
    },
    {
      "epoch": 0.00037129652809617705,
      "grad_norm": 105.34039306640625,
      "learning_rate": 6.306179775280899e-09,
      "loss": 8.9142,
      "step": 264
    },
    {
      "epoch": 0.0003727029543389656,
      "grad_norm": 48.75224685668945,
      "learning_rate": 6.292134831460674e-09,
      "loss": 7.3112,
      "step": 265
    },
    {
      "epoch": 0.0003741093805817542,
      "grad_norm": 42.599735260009766,
      "learning_rate": 6.278089887640449e-09,
      "loss": 7.9403,
      "step": 266
    },
    {
      "epoch": 0.0003755158068245427,
      "grad_norm": 133.20533752441406,
      "learning_rate": 6.264044943820225e-09,
      "loss": 9.4525,
      "step": 267
    },
    {
      "epoch": 0.00037692223306733124,
      "grad_norm": 34.211883544921875,
      "learning_rate": 6.25e-09,
      "loss": 6.9458,
      "step": 268
    },
    {
      "epoch": 0.0003783286593101198,
      "grad_norm": 52.36872482299805,
      "learning_rate": 6.2359550561797755e-09,
      "loss": 6.6504,
      "step": 269
    },
    {
      "epoch": 0.00037973508555290837,
      "grad_norm": 72.8955078125,
      "learning_rate": 6.221910112359551e-09,
      "loss": 6.5183,
      "step": 270
    },
    {
      "epoch": 0.0003811415117956969,
      "grad_norm": 59.985294342041016,
      "learning_rate": 6.207865168539326e-09,
      "loss": 6.8902,
      "step": 271
    },
    {
      "epoch": 0.00038254793803848544,
      "grad_norm": 73.75630187988281,
      "learning_rate": 6.193820224719101e-09,
      "loss": 8.6924,
      "step": 272
    },
    {
      "epoch": 0.00038395436428127397,
      "grad_norm": 50.483150482177734,
      "learning_rate": 6.179775280898876e-09,
      "loss": 8.0838,
      "step": 273
    },
    {
      "epoch": 0.00038536079052406256,
      "grad_norm": 92.6390609741211,
      "learning_rate": 6.165730337078652e-09,
      "loss": 8.5677,
      "step": 274
    },
    {
      "epoch": 0.0003867672167668511,
      "grad_norm": 21.72918701171875,
      "learning_rate": 6.151685393258427e-09,
      "loss": 5.991,
      "step": 275
    },
    {
      "epoch": 0.00038817364300963963,
      "grad_norm": 88.03589630126953,
      "learning_rate": 6.137640449438202e-09,
      "loss": 7.6693,
      "step": 276
    },
    {
      "epoch": 0.0003895800692524282,
      "grad_norm": 48.70506286621094,
      "learning_rate": 6.123595505617978e-09,
      "loss": 7.864,
      "step": 277
    },
    {
      "epoch": 0.00039098649549521676,
      "grad_norm": 62.9134635925293,
      "learning_rate": 6.109550561797753e-09,
      "loss": 7.8721,
      "step": 278
    },
    {
      "epoch": 0.0003923929217380053,
      "grad_norm": 80.31470489501953,
      "learning_rate": 6.095505617977528e-09,
      "loss": 7.5493,
      "step": 279
    },
    {
      "epoch": 0.0003937993479807938,
      "grad_norm": 53.06330871582031,
      "learning_rate": 6.081460674157303e-09,
      "loss": 7.4307,
      "step": 280
    },
    {
      "epoch": 0.0003952057742235824,
      "grad_norm": 77.31880950927734,
      "learning_rate": 6.067415730337079e-09,
      "loss": 5.7456,
      "step": 281
    },
    {
      "epoch": 0.00039661220046637095,
      "grad_norm": 87.26480865478516,
      "learning_rate": 6.053370786516854e-09,
      "loss": 11.611,
      "step": 282
    },
    {
      "epoch": 0.0003980186267091595,
      "grad_norm": 24.88907241821289,
      "learning_rate": 6.039325842696629e-09,
      "loss": 6.4007,
      "step": 283
    },
    {
      "epoch": 0.000399425052951948,
      "grad_norm": 105.47373962402344,
      "learning_rate": 6.025280898876404e-09,
      "loss": 8.9426,
      "step": 284
    },
    {
      "epoch": 0.0004008314791947366,
      "grad_norm": 82.53099060058594,
      "learning_rate": 6.01123595505618e-09,
      "loss": 8.0894,
      "step": 285
    },
    {
      "epoch": 0.00040223790543752515,
      "grad_norm": 44.09560012817383,
      "learning_rate": 5.997191011235955e-09,
      "loss": 5.7347,
      "step": 286
    },
    {
      "epoch": 0.0004036443316803137,
      "grad_norm": 77.75996398925781,
      "learning_rate": 5.98314606741573e-09,
      "loss": 7.5401,
      "step": 287
    },
    {
      "epoch": 0.0004050507579231022,
      "grad_norm": 83.05438232421875,
      "learning_rate": 5.969101123595506e-09,
      "loss": 8.8911,
      "step": 288
    },
    {
      "epoch": 0.0004064571841658908,
      "grad_norm": 38.709163665771484,
      "learning_rate": 5.955056179775281e-09,
      "loss": 6.8178,
      "step": 289
    },
    {
      "epoch": 0.00040786361040867934,
      "grad_norm": 56.26765823364258,
      "learning_rate": 5.941011235955056e-09,
      "loss": 6.8542,
      "step": 290
    },
    {
      "epoch": 0.0004092700366514679,
      "grad_norm": 59.31204605102539,
      "learning_rate": 5.926966292134831e-09,
      "loss": 8.0681,
      "step": 291
    },
    {
      "epoch": 0.0004106764628942564,
      "grad_norm": 87.66608428955078,
      "learning_rate": 5.912921348314607e-09,
      "loss": 11.1073,
      "step": 292
    },
    {
      "epoch": 0.000412082889137045,
      "grad_norm": 47.34912872314453,
      "learning_rate": 5.898876404494382e-09,
      "loss": 5.8861,
      "step": 293
    },
    {
      "epoch": 0.00041348931537983353,
      "grad_norm": 95.55918884277344,
      "learning_rate": 5.884831460674157e-09,
      "loss": 9.4168,
      "step": 294
    },
    {
      "epoch": 0.00041489574162262207,
      "grad_norm": 106.04622650146484,
      "learning_rate": 5.870786516853933e-09,
      "loss": 7.1841,
      "step": 295
    },
    {
      "epoch": 0.00041630216786541066,
      "grad_norm": 39.455772399902344,
      "learning_rate": 5.856741573033708e-09,
      "loss": 5.9707,
      "step": 296
    },
    {
      "epoch": 0.0004177085941081992,
      "grad_norm": 88.13320922851562,
      "learning_rate": 5.842696629213483e-09,
      "loss": 5.8349,
      "step": 297
    },
    {
      "epoch": 0.00041911502035098773,
      "grad_norm": 49.34663009643555,
      "learning_rate": 5.828651685393258e-09,
      "loss": 5.761,
      "step": 298
    },
    {
      "epoch": 0.00042052144659377626,
      "grad_norm": 53.598846435546875,
      "learning_rate": 5.8146067415730345e-09,
      "loss": 6.9338,
      "step": 299
    },
    {
      "epoch": 0.00042192787283656485,
      "grad_norm": 55.64794158935547,
      "learning_rate": 5.8005617977528094e-09,
      "loss": 6.7371,
      "step": 300
    },
    {
      "epoch": 0.0004233342990793534,
      "grad_norm": 52.15082550048828,
      "learning_rate": 5.786516853932584e-09,
      "loss": 5.7087,
      "step": 301
    },
    {
      "epoch": 0.0004247407253221419,
      "grad_norm": 52.631561279296875,
      "learning_rate": 5.77247191011236e-09,
      "loss": 7.1138,
      "step": 302
    },
    {
      "epoch": 0.00042614715156493046,
      "grad_norm": 68.482421875,
      "learning_rate": 5.758426966292135e-09,
      "loss": 8.0415,
      "step": 303
    },
    {
      "epoch": 0.00042755357780771905,
      "grad_norm": 40.92408752441406,
      "learning_rate": 5.74438202247191e-09,
      "loss": 5.6483,
      "step": 304
    },
    {
      "epoch": 0.0004289600040505076,
      "grad_norm": 66.08252716064453,
      "learning_rate": 5.730337078651685e-09,
      "loss": 7.778,
      "step": 305
    },
    {
      "epoch": 0.0004303664302932961,
      "grad_norm": 35.335323333740234,
      "learning_rate": 5.7162921348314616e-09,
      "loss": 5.6115,
      "step": 306
    },
    {
      "epoch": 0.00043177285653608465,
      "grad_norm": 80.30724334716797,
      "learning_rate": 5.7022471910112365e-09,
      "loss": 11.6113,
      "step": 307
    },
    {
      "epoch": 0.00043317928277887324,
      "grad_norm": 46.702205657958984,
      "learning_rate": 5.6882022471910114e-09,
      "loss": 7.621,
      "step": 308
    },
    {
      "epoch": 0.0004345857090216618,
      "grad_norm": 43.00053405761719,
      "learning_rate": 5.674157303370787e-09,
      "loss": 7.7293,
      "step": 309
    },
    {
      "epoch": 0.0004359921352644503,
      "grad_norm": 42.75455856323242,
      "learning_rate": 5.660112359550562e-09,
      "loss": 6.7015,
      "step": 310
    },
    {
      "epoch": 0.0004373985615072389,
      "grad_norm": 135.83804321289062,
      "learning_rate": 5.646067415730337e-09,
      "loss": 8.8103,
      "step": 311
    },
    {
      "epoch": 0.00043880498775002744,
      "grad_norm": 67.60472106933594,
      "learning_rate": 5.632022471910112e-09,
      "loss": 6.901,
      "step": 312
    },
    {
      "epoch": 0.00044021141399281597,
      "grad_norm": 58.859596252441406,
      "learning_rate": 5.617977528089888e-09,
      "loss": 7.7598,
      "step": 313
    },
    {
      "epoch": 0.0004416178402356045,
      "grad_norm": 55.92790603637695,
      "learning_rate": 5.603932584269663e-09,
      "loss": 8.0683,
      "step": 314
    },
    {
      "epoch": 0.0004430242664783931,
      "grad_norm": 42.844425201416016,
      "learning_rate": 5.5898876404494385e-09,
      "loss": 5.929,
      "step": 315
    },
    {
      "epoch": 0.00044443069272118163,
      "grad_norm": 52.27474594116211,
      "learning_rate": 5.5758426966292134e-09,
      "loss": 7.2084,
      "step": 316
    },
    {
      "epoch": 0.00044583711896397017,
      "grad_norm": 36.30876159667969,
      "learning_rate": 5.561797752808989e-09,
      "loss": 6.7347,
      "step": 317
    },
    {
      "epoch": 0.0004472435452067587,
      "grad_norm": 59.32711410522461,
      "learning_rate": 5.547752808988764e-09,
      "loss": 5.5814,
      "step": 318
    },
    {
      "epoch": 0.0004486499714495473,
      "grad_norm": 36.42837905883789,
      "learning_rate": 5.533707865168539e-09,
      "loss": 7.1643,
      "step": 319
    },
    {
      "epoch": 0.0004500563976923358,
      "grad_norm": 36.90993881225586,
      "learning_rate": 5.519662921348315e-09,
      "loss": 7.44,
      "step": 320
    },
    {
      "epoch": 0.00045146282393512436,
      "grad_norm": 89.2170639038086,
      "learning_rate": 5.50561797752809e-09,
      "loss": 7.5279,
      "step": 321
    },
    {
      "epoch": 0.0004528692501779129,
      "grad_norm": 49.10225296020508,
      "learning_rate": 5.491573033707865e-09,
      "loss": 6.6081,
      "step": 322
    },
    {
      "epoch": 0.0004542756764207015,
      "grad_norm": 65.24089050292969,
      "learning_rate": 5.47752808988764e-09,
      "loss": 8.1547,
      "step": 323
    },
    {
      "epoch": 0.00045568210266349,
      "grad_norm": 87.50634765625,
      "learning_rate": 5.463483146067416e-09,
      "loss": 11.6212,
      "step": 324
    },
    {
      "epoch": 0.00045708852890627855,
      "grad_norm": 51.035274505615234,
      "learning_rate": 5.449438202247191e-09,
      "loss": 5.8188,
      "step": 325
    },
    {
      "epoch": 0.00045849495514906714,
      "grad_norm": 129.66038513183594,
      "learning_rate": 5.435393258426966e-09,
      "loss": 9.5959,
      "step": 326
    },
    {
      "epoch": 0.0004599013813918557,
      "grad_norm": 87.67124938964844,
      "learning_rate": 5.421348314606742e-09,
      "loss": 8.4614,
      "step": 327
    },
    {
      "epoch": 0.0004613078076346442,
      "grad_norm": 55.59702682495117,
      "learning_rate": 5.407303370786517e-09,
      "loss": 7.511,
      "step": 328
    },
    {
      "epoch": 0.00046271423387743275,
      "grad_norm": 50.73508071899414,
      "learning_rate": 5.393258426966292e-09,
      "loss": 7.3715,
      "step": 329
    },
    {
      "epoch": 0.00046412066012022134,
      "grad_norm": 52.99897384643555,
      "learning_rate": 5.379213483146067e-09,
      "loss": 7.133,
      "step": 330
    },
    {
      "epoch": 0.0004655270863630099,
      "grad_norm": 62.672035217285156,
      "learning_rate": 5.365168539325843e-09,
      "loss": 6.4106,
      "step": 331
    },
    {
      "epoch": 0.0004669335126057984,
      "grad_norm": 49.14546585083008,
      "learning_rate": 5.351123595505618e-09,
      "loss": 6.553,
      "step": 332
    },
    {
      "epoch": 0.00046833993884858694,
      "grad_norm": 41.01247787475586,
      "learning_rate": 5.337078651685393e-09,
      "loss": 6.5511,
      "step": 333
    },
    {
      "epoch": 0.00046974636509137553,
      "grad_norm": 40.06755447387695,
      "learning_rate": 5.323033707865169e-09,
      "loss": 5.8667,
      "step": 334
    },
    {
      "epoch": 0.00047115279133416407,
      "grad_norm": 60.64030075073242,
      "learning_rate": 5.308988764044944e-09,
      "loss": 7.9793,
      "step": 335
    },
    {
      "epoch": 0.0004725592175769526,
      "grad_norm": 50.222042083740234,
      "learning_rate": 5.294943820224719e-09,
      "loss": 5.7459,
      "step": 336
    },
    {
      "epoch": 0.00047396564381974114,
      "grad_norm": 49.136383056640625,
      "learning_rate": 5.280898876404494e-09,
      "loss": 6.2438,
      "step": 337
    },
    {
      "epoch": 0.0004753720700625297,
      "grad_norm": 36.1444206237793,
      "learning_rate": 5.2668539325842704e-09,
      "loss": 7.69,
      "step": 338
    },
    {
      "epoch": 0.00047677849630531826,
      "grad_norm": 73.29962158203125,
      "learning_rate": 5.252808988764045e-09,
      "loss": 6.3297,
      "step": 339
    },
    {
      "epoch": 0.0004781849225481068,
      "grad_norm": 81.12821197509766,
      "learning_rate": 5.23876404494382e-09,
      "loss": 7.8092,
      "step": 340
    },
    {
      "epoch": 0.00047959134879089533,
      "grad_norm": 31.51947021484375,
      "learning_rate": 5.224719101123596e-09,
      "loss": 7.0178,
      "step": 341
    },
    {
      "epoch": 0.0004809977750336839,
      "grad_norm": 37.57365417480469,
      "learning_rate": 5.210674157303371e-09,
      "loss": 6.5281,
      "step": 342
    },
    {
      "epoch": 0.00048240420127647246,
      "grad_norm": 58.0626220703125,
      "learning_rate": 5.196629213483146e-09,
      "loss": 7.2047,
      "step": 343
    },
    {
      "epoch": 0.000483810627519261,
      "grad_norm": 51.76875305175781,
      "learning_rate": 5.182584269662921e-09,
      "loss": 7.3127,
      "step": 344
    },
    {
      "epoch": 0.0004852170537620496,
      "grad_norm": 44.76655197143555,
      "learning_rate": 5.1685393258426975e-09,
      "loss": 7.602,
      "step": 345
    },
    {
      "epoch": 0.0004866234800048381,
      "grad_norm": 37.07097625732422,
      "learning_rate": 5.1544943820224724e-09,
      "loss": 5.5257,
      "step": 346
    },
    {
      "epoch": 0.00048802990624762665,
      "grad_norm": 38.1342887878418,
      "learning_rate": 5.140449438202247e-09,
      "loss": 7.2673,
      "step": 347
    },
    {
      "epoch": 0.0004894363324904152,
      "grad_norm": 86.31440734863281,
      "learning_rate": 5.126404494382022e-09,
      "loss": 8.9065,
      "step": 348
    },
    {
      "epoch": 0.0004908427587332038,
      "grad_norm": 105.77519989013672,
      "learning_rate": 5.112359550561798e-09,
      "loss": 9.139,
      "step": 349
    },
    {
      "epoch": 0.0004922491849759923,
      "grad_norm": 54.359703063964844,
      "learning_rate": 5.098314606741573e-09,
      "loss": 7.6689,
      "step": 350
    },
    {
      "epoch": 0.0004936556112187808,
      "grad_norm": 50.11273956298828,
      "learning_rate": 5.084269662921348e-09,
      "loss": 7.201,
      "step": 351
    },
    {
      "epoch": 0.0004950620374615694,
      "grad_norm": 52.26247024536133,
      "learning_rate": 5.070224719101124e-09,
      "loss": 6.7827,
      "step": 352
    },
    {
      "epoch": 0.0004964684637043579,
      "grad_norm": 163.1973419189453,
      "learning_rate": 5.0561797752808995e-09,
      "loss": 8.0173,
      "step": 353
    },
    {
      "epoch": 0.0004978748899471465,
      "grad_norm": 45.71217346191406,
      "learning_rate": 5.0421348314606745e-09,
      "loss": 7.36,
      "step": 354
    },
    {
      "epoch": 0.0004992813161899351,
      "grad_norm": 56.09514236450195,
      "learning_rate": 5.028089887640449e-09,
      "loss": 6.0705,
      "step": 355
    },
    {
      "epoch": 0.0005006877424327236,
      "grad_norm": 73.7667236328125,
      "learning_rate": 5.014044943820225e-09,
      "loss": 6.8664,
      "step": 356
    },
    {
      "epoch": 0.0005020941686755122,
      "grad_norm": 34.555580139160156,
      "learning_rate": 5e-09,
      "loss": 6.6124,
      "step": 357
    },
    {
      "epoch": 0.0005035005949183008,
      "grad_norm": 92.61610412597656,
      "learning_rate": 4.985955056179776e-09,
      "loss": 7.772,
      "step": 358
    },
    {
      "epoch": 0.0005049070211610892,
      "grad_norm": 52.81138229370117,
      "learning_rate": 4.971910112359551e-09,
      "loss": 7.7605,
      "step": 359
    },
    {
      "epoch": 0.0005063134474038778,
      "grad_norm": 59.42206954956055,
      "learning_rate": 4.957865168539326e-09,
      "loss": 5.7122,
      "step": 360
    },
    {
      "epoch": 0.0005077198736466663,
      "grad_norm": 65.61891174316406,
      "learning_rate": 4.943820224719101e-09,
      "loss": 8.3864,
      "step": 361
    },
    {
      "epoch": 0.0005091262998894549,
      "grad_norm": 78.49464416503906,
      "learning_rate": 4.9297752808988765e-09,
      "loss": 7.1014,
      "step": 362
    },
    {
      "epoch": 0.0005105327261322435,
      "grad_norm": 147.43453979492188,
      "learning_rate": 4.915730337078651e-09,
      "loss": 9.48,
      "step": 363
    },
    {
      "epoch": 0.000511939152375032,
      "grad_norm": 75.01488494873047,
      "learning_rate": 4.901685393258427e-09,
      "loss": 6.8872,
      "step": 364
    },
    {
      "epoch": 0.0005133455786178206,
      "grad_norm": 92.92706298828125,
      "learning_rate": 4.887640449438202e-09,
      "loss": 11.119,
      "step": 365
    },
    {
      "epoch": 0.0005147520048606091,
      "grad_norm": 57.828765869140625,
      "learning_rate": 4.873595505617978e-09,
      "loss": 7.2916,
      "step": 366
    },
    {
      "epoch": 0.0005161584311033976,
      "grad_norm": 90.3273696899414,
      "learning_rate": 4.859550561797753e-09,
      "loss": 8.6033,
      "step": 367
    },
    {
      "epoch": 0.0005175648573461862,
      "grad_norm": 47.66203689575195,
      "learning_rate": 4.845505617977528e-09,
      "loss": 7.4651,
      "step": 368
    },
    {
      "epoch": 0.0005189712835889747,
      "grad_norm": 202.52877807617188,
      "learning_rate": 4.8314606741573035e-09,
      "loss": 12.0968,
      "step": 369
    },
    {
      "epoch": 0.0005203777098317633,
      "grad_norm": 24.621112823486328,
      "learning_rate": 4.8174157303370785e-09,
      "loss": 6.895,
      "step": 370
    },
    {
      "epoch": 0.0005217841360745519,
      "grad_norm": 128.38186645507812,
      "learning_rate": 4.803370786516854e-09,
      "loss": 7.3545,
      "step": 371
    },
    {
      "epoch": 0.0005231905623173404,
      "grad_norm": 80.66773223876953,
      "learning_rate": 4.789325842696629e-09,
      "loss": 5.9362,
      "step": 372
    },
    {
      "epoch": 0.0005245969885601289,
      "grad_norm": 34.383872985839844,
      "learning_rate": 4.775280898876405e-09,
      "loss": 6.4948,
      "step": 373
    },
    {
      "epoch": 0.0005260034148029175,
      "grad_norm": 64.837890625,
      "learning_rate": 4.76123595505618e-09,
      "loss": 9.8217,
      "step": 374
    },
    {
      "epoch": 0.000527409841045706,
      "grad_norm": 35.96417999267578,
      "learning_rate": 4.747191011235955e-09,
      "loss": 7.5599,
      "step": 375
    },
    {
      "epoch": 0.0005288162672884946,
      "grad_norm": 120.0561752319336,
      "learning_rate": 4.733146067415731e-09,
      "loss": 10.2296,
      "step": 376
    },
    {
      "epoch": 0.0005302226935312832,
      "grad_norm": 50.203941345214844,
      "learning_rate": 4.7191011235955055e-09,
      "loss": 7.6358,
      "step": 377
    },
    {
      "epoch": 0.0005316291197740717,
      "grad_norm": 44.56571960449219,
      "learning_rate": 4.705056179775281e-09,
      "loss": 7.2543,
      "step": 378
    },
    {
      "epoch": 0.0005330355460168603,
      "grad_norm": 24.975746154785156,
      "learning_rate": 4.691011235955056e-09,
      "loss": 7.2108,
      "step": 379
    },
    {
      "epoch": 0.0005344419722596487,
      "grad_norm": 73.24470520019531,
      "learning_rate": 4.676966292134831e-09,
      "loss": 5.6621,
      "step": 380
    },
    {
      "epoch": 0.0005358483985024373,
      "grad_norm": 43.12714385986328,
      "learning_rate": 4.662921348314607e-09,
      "loss": 6.7327,
      "step": 381
    },
    {
      "epoch": 0.0005372548247452259,
      "grad_norm": 51.55318832397461,
      "learning_rate": 4.648876404494382e-09,
      "loss": 7.0075,
      "step": 382
    },
    {
      "epoch": 0.0005386612509880144,
      "grad_norm": 50.000465393066406,
      "learning_rate": 4.634831460674158e-09,
      "loss": 7.0436,
      "step": 383
    },
    {
      "epoch": 0.000540067677230803,
      "grad_norm": 104.77932739257812,
      "learning_rate": 4.620786516853933e-09,
      "loss": 7.9285,
      "step": 384
    },
    {
      "epoch": 0.0005414741034735916,
      "grad_norm": 70.9351577758789,
      "learning_rate": 4.606741573033708e-09,
      "loss": 5.9389,
      "step": 385
    },
    {
      "epoch": 0.0005428805297163801,
      "grad_norm": 57.636802673339844,
      "learning_rate": 4.592696629213483e-09,
      "loss": 7.8579,
      "step": 386
    },
    {
      "epoch": 0.0005442869559591686,
      "grad_norm": 38.265167236328125,
      "learning_rate": 4.578651685393258e-09,
      "loss": 6.6196,
      "step": 387
    },
    {
      "epoch": 0.0005456933822019571,
      "grad_norm": 47.4437255859375,
      "learning_rate": 4.564606741573033e-09,
      "loss": 5.8321,
      "step": 388
    },
    {
      "epoch": 0.0005470998084447457,
      "grad_norm": 75.7911148071289,
      "learning_rate": 4.550561797752809e-09,
      "loss": 9.5323,
      "step": 389
    },
    {
      "epoch": 0.0005485062346875343,
      "grad_norm": 37.09014129638672,
      "learning_rate": 4.536516853932585e-09,
      "loss": 6.5372,
      "step": 390
    },
    {
      "epoch": 0.0005499126609303228,
      "grad_norm": 47.6191291809082,
      "learning_rate": 4.52247191011236e-09,
      "loss": 6.2373,
      "step": 391
    },
    {
      "epoch": 0.0005513190871731114,
      "grad_norm": 70.00617980957031,
      "learning_rate": 4.5084269662921355e-09,
      "loss": 7.9026,
      "step": 392
    },
    {
      "epoch": 0.0005527255134159,
      "grad_norm": 72.20478820800781,
      "learning_rate": 4.49438202247191e-09,
      "loss": 8.9987,
      "step": 393
    },
    {
      "epoch": 0.0005541319396586884,
      "grad_norm": 49.74281692504883,
      "learning_rate": 4.480337078651685e-09,
      "loss": 7.0636,
      "step": 394
    },
    {
      "epoch": 0.000555538365901477,
      "grad_norm": 75.63180541992188,
      "learning_rate": 4.46629213483146e-09,
      "loss": 9.5546,
      "step": 395
    },
    {
      "epoch": 0.0005569447921442656,
      "grad_norm": 37.441741943359375,
      "learning_rate": 4.452247191011236e-09,
      "loss": 7.2354,
      "step": 396
    },
    {
      "epoch": 0.0005583512183870541,
      "grad_norm": 47.83421325683594,
      "learning_rate": 4.438202247191011e-09,
      "loss": 7.3949,
      "step": 397
    },
    {
      "epoch": 0.0005597576446298427,
      "grad_norm": 31.05235481262207,
      "learning_rate": 4.424157303370787e-09,
      "loss": 5.724,
      "step": 398
    },
    {
      "epoch": 0.0005611640708726312,
      "grad_norm": 87.55815124511719,
      "learning_rate": 4.410112359550562e-09,
      "loss": 8.0841,
      "step": 399
    },
    {
      "epoch": 0.0005625704971154198,
      "grad_norm": 64.78657531738281,
      "learning_rate": 4.3960674157303375e-09,
      "loss": 8.1624,
      "step": 400
    },
    {
      "epoch": 0.0005639769233582084,
      "grad_norm": 51.03322982788086,
      "learning_rate": 4.3820224719101124e-09,
      "loss": 7.2005,
      "step": 401
    },
    {
      "epoch": 0.0005653833496009968,
      "grad_norm": 91.04379272460938,
      "learning_rate": 4.367977528089887e-09,
      "loss": 7.8243,
      "step": 402
    },
    {
      "epoch": 0.0005667897758437854,
      "grad_norm": 75.84540557861328,
      "learning_rate": 4.353932584269663e-09,
      "loss": 7.164,
      "step": 403
    },
    {
      "epoch": 0.000568196202086574,
      "grad_norm": 92.5384521484375,
      "learning_rate": 4.339887640449438e-09,
      "loss": 8.3906,
      "step": 404
    },
    {
      "epoch": 0.0005696026283293625,
      "grad_norm": 84.0121078491211,
      "learning_rate": 4.325842696629214e-09,
      "loss": 6.8411,
      "step": 405
    },
    {
      "epoch": 0.0005710090545721511,
      "grad_norm": 67.7183837890625,
      "learning_rate": 4.311797752808989e-09,
      "loss": 6.2344,
      "step": 406
    },
    {
      "epoch": 0.0005724154808149397,
      "grad_norm": 98.46870422363281,
      "learning_rate": 4.297752808988764e-09,
      "loss": 8.4398,
      "step": 407
    },
    {
      "epoch": 0.0005738219070577282,
      "grad_norm": 48.74967956542969,
      "learning_rate": 4.2837078651685395e-09,
      "loss": 5.8071,
      "step": 408
    },
    {
      "epoch": 0.0005752283333005167,
      "grad_norm": 58.02124786376953,
      "learning_rate": 4.2696629213483144e-09,
      "loss": 7.0091,
      "step": 409
    },
    {
      "epoch": 0.0005766347595433052,
      "grad_norm": 43.223392486572266,
      "learning_rate": 4.25561797752809e-09,
      "loss": 7.2567,
      "step": 410
    },
    {
      "epoch": 0.0005780411857860938,
      "grad_norm": 53.965293884277344,
      "learning_rate": 4.241573033707865e-09,
      "loss": 7.4459,
      "step": 411
    },
    {
      "epoch": 0.0005794476120288824,
      "grad_norm": 49.5999870300293,
      "learning_rate": 4.227528089887641e-09,
      "loss": 6.3244,
      "step": 412
    },
    {
      "epoch": 0.0005808540382716709,
      "grad_norm": 53.39985275268555,
      "learning_rate": 4.213483146067416e-09,
      "loss": 7.1168,
      "step": 413
    },
    {
      "epoch": 0.0005822604645144595,
      "grad_norm": 81.69300079345703,
      "learning_rate": 4.199438202247191e-09,
      "loss": 7.1312,
      "step": 414
    },
    {
      "epoch": 0.0005836668907572481,
      "grad_norm": 49.25372314453125,
      "learning_rate": 4.1853932584269666e-09,
      "loss": 6.6926,
      "step": 415
    },
    {
      "epoch": 0.0005850733170000365,
      "grad_norm": 57.793357849121094,
      "learning_rate": 4.1713483146067415e-09,
      "loss": 7.2738,
      "step": 416
    },
    {
      "epoch": 0.0005864797432428251,
      "grad_norm": 43.60878372192383,
      "learning_rate": 4.157303370786517e-09,
      "loss": 6.9406,
      "step": 417
    },
    {
      "epoch": 0.0005878861694856136,
      "grad_norm": 111.3676528930664,
      "learning_rate": 4.143258426966292e-09,
      "loss": 7.2707,
      "step": 418
    },
    {
      "epoch": 0.0005892925957284022,
      "grad_norm": 99.18256378173828,
      "learning_rate": 4.129213483146068e-09,
      "loss": 11.1108,
      "step": 419
    },
    {
      "epoch": 0.0005906990219711908,
      "grad_norm": 96.73295593261719,
      "learning_rate": 4.115168539325843e-09,
      "loss": 6.9048,
      "step": 420
    },
    {
      "epoch": 0.0005921054482139793,
      "grad_norm": 57.85336685180664,
      "learning_rate": 4.101123595505618e-09,
      "loss": 7.3259,
      "step": 421
    },
    {
      "epoch": 0.0005935118744567679,
      "grad_norm": 63.640621185302734,
      "learning_rate": 4.087078651685394e-09,
      "loss": 7.3201,
      "step": 422
    },
    {
      "epoch": 0.0005949183006995565,
      "grad_norm": 130.30043029785156,
      "learning_rate": 4.0730337078651686e-09,
      "loss": 8.3357,
      "step": 423
    },
    {
      "epoch": 0.0005963247269423449,
      "grad_norm": 22.236806869506836,
      "learning_rate": 4.058988764044944e-09,
      "loss": 5.8137,
      "step": 424
    },
    {
      "epoch": 0.0005977311531851335,
      "grad_norm": 73.10712432861328,
      "learning_rate": 4.044943820224719e-09,
      "loss": 8.5067,
      "step": 425
    },
    {
      "epoch": 0.0005991375794279221,
      "grad_norm": 45.221405029296875,
      "learning_rate": 4.030898876404494e-09,
      "loss": 6.7985,
      "step": 426
    },
    {
      "epoch": 0.0006005440056707106,
      "grad_norm": 71.91303253173828,
      "learning_rate": 4.016853932584269e-09,
      "loss": 7.6414,
      "step": 427
    },
    {
      "epoch": 0.0006019504319134992,
      "grad_norm": 56.385841369628906,
      "learning_rate": 4.002808988764045e-09,
      "loss": 6.8224,
      "step": 428
    },
    {
      "epoch": 0.0006033568581562877,
      "grad_norm": 47.277706146240234,
      "learning_rate": 3.98876404494382e-09,
      "loss": 6.986,
      "step": 429
    },
    {
      "epoch": 0.0006047632843990763,
      "grad_norm": 48.786861419677734,
      "learning_rate": 3.974719101123596e-09,
      "loss": 7.0817,
      "step": 430
    },
    {
      "epoch": 0.0006061697106418648,
      "grad_norm": 97.08422088623047,
      "learning_rate": 3.960674157303371e-09,
      "loss": 7.4431,
      "step": 431
    },
    {
      "epoch": 0.0006075761368846533,
      "grad_norm": 39.78855895996094,
      "learning_rate": 3.946629213483146e-09,
      "loss": 7.504,
      "step": 432
    },
    {
      "epoch": 0.0006089825631274419,
      "grad_norm": 109.68327331542969,
      "learning_rate": 3.932584269662921e-09,
      "loss": 7.9318,
      "step": 433
    },
    {
      "epoch": 0.0006103889893702305,
      "grad_norm": 106.17149353027344,
      "learning_rate": 3.918539325842696e-09,
      "loss": 10.0212,
      "step": 434
    },
    {
      "epoch": 0.000611795415613019,
      "grad_norm": 47.84178161621094,
      "learning_rate": 3.904494382022472e-09,
      "loss": 7.7424,
      "step": 435
    },
    {
      "epoch": 0.0006132018418558076,
      "grad_norm": 85.72727966308594,
      "learning_rate": 3.890449438202247e-09,
      "loss": 8.3203,
      "step": 436
    },
    {
      "epoch": 0.000614608268098596,
      "grad_norm": 48.71012496948242,
      "learning_rate": 3.876404494382023e-09,
      "loss": 6.8614,
      "step": 437
    },
    {
      "epoch": 0.0006160146943413846,
      "grad_norm": 28.512990951538086,
      "learning_rate": 3.8623595505617985e-09,
      "loss": 5.6262,
      "step": 438
    },
    {
      "epoch": 0.0006174211205841732,
      "grad_norm": 87.41329193115234,
      "learning_rate": 3.8483146067415734e-09,
      "loss": 7.6668,
      "step": 439
    },
    {
      "epoch": 0.0006188275468269617,
      "grad_norm": 78.25431060791016,
      "learning_rate": 3.834269662921348e-09,
      "loss": 8.1174,
      "step": 440
    },
    {
      "epoch": 0.0006202339730697503,
      "grad_norm": 56.98104476928711,
      "learning_rate": 3.820224719101123e-09,
      "loss": 6.8777,
      "step": 441
    },
    {
      "epoch": 0.0006216403993125389,
      "grad_norm": 95.04203796386719,
      "learning_rate": 3.806179775280899e-09,
      "loss": 9.281,
      "step": 442
    },
    {
      "epoch": 0.0006230468255553274,
      "grad_norm": 52.59215545654297,
      "learning_rate": 3.792134831460674e-09,
      "loss": 7.44,
      "step": 443
    },
    {
      "epoch": 0.000624453251798116,
      "grad_norm": 28.0666446685791,
      "learning_rate": 3.77808988764045e-09,
      "loss": 5.2394,
      "step": 444
    },
    {
      "epoch": 0.0006258596780409045,
      "grad_norm": 94.39606475830078,
      "learning_rate": 3.764044943820225e-09,
      "loss": 10.7258,
      "step": 445
    },
    {
      "epoch": 0.000627266104283693,
      "grad_norm": 53.052276611328125,
      "learning_rate": 3.7500000000000005e-09,
      "loss": 5.9084,
      "step": 446
    },
    {
      "epoch": 0.0006286725305264816,
      "grad_norm": 104.3106918334961,
      "learning_rate": 3.7359550561797754e-09,
      "loss": 6.5807,
      "step": 447
    },
    {
      "epoch": 0.0006300789567692701,
      "grad_norm": 45.313438415527344,
      "learning_rate": 3.7219101123595504e-09,
      "loss": 6.9262,
      "step": 448
    },
    {
      "epoch": 0.0006314853830120587,
      "grad_norm": 32.85796356201172,
      "learning_rate": 3.707865168539326e-09,
      "loss": 7.3973,
      "step": 449
    },
    {
      "epoch": 0.0006328918092548473,
      "grad_norm": 40.82676696777344,
      "learning_rate": 3.693820224719101e-09,
      "loss": 6.5263,
      "step": 450
    },
    {
      "epoch": 0.0006342982354976358,
      "grad_norm": 60.804378509521484,
      "learning_rate": 3.6797752808988764e-09,
      "loss": 7.3849,
      "step": 451
    },
    {
      "epoch": 0.0006357046617404243,
      "grad_norm": 46.64373016357422,
      "learning_rate": 3.6657303370786514e-09,
      "loss": 7.7651,
      "step": 452
    },
    {
      "epoch": 0.0006371110879832129,
      "grad_norm": 99.70623016357422,
      "learning_rate": 3.651685393258427e-09,
      "loss": 8.974,
      "step": 453
    },
    {
      "epoch": 0.0006385175142260014,
      "grad_norm": 44.542877197265625,
      "learning_rate": 3.637640449438202e-09,
      "loss": 6.3116,
      "step": 454
    },
    {
      "epoch": 0.00063992394046879,
      "grad_norm": 57.30747985839844,
      "learning_rate": 3.6235955056179774e-09,
      "loss": 8.0888,
      "step": 455
    },
    {
      "epoch": 0.0006413303667115786,
      "grad_norm": 86.944091796875,
      "learning_rate": 3.6095505617977532e-09,
      "loss": 7.5111,
      "step": 456
    },
    {
      "epoch": 0.0006427367929543671,
      "grad_norm": 126.49275970458984,
      "learning_rate": 3.595505617977528e-09,
      "loss": 11.8215,
      "step": 457
    },
    {
      "epoch": 0.0006441432191971557,
      "grad_norm": 47.177223205566406,
      "learning_rate": 3.5814606741573035e-09,
      "loss": 7.2529,
      "step": 458
    },
    {
      "epoch": 0.0006455496454399441,
      "grad_norm": 73.7306900024414,
      "learning_rate": 3.5674157303370785e-09,
      "loss": 8.5365,
      "step": 459
    },
    {
      "epoch": 0.0006469560716827327,
      "grad_norm": 34.197959899902344,
      "learning_rate": 3.5533707865168542e-09,
      "loss": 6.6958,
      "step": 460
    },
    {
      "epoch": 0.0006483624979255213,
      "grad_norm": 52.83530044555664,
      "learning_rate": 3.539325842696629e-09,
      "loss": 6.9252,
      "step": 461
    },
    {
      "epoch": 0.0006497689241683098,
      "grad_norm": 52.8624267578125,
      "learning_rate": 3.5252808988764045e-09,
      "loss": 6.3146,
      "step": 462
    },
    {
      "epoch": 0.0006511753504110984,
      "grad_norm": 47.475276947021484,
      "learning_rate": 3.5112359550561803e-09,
      "loss": 7.0584,
      "step": 463
    },
    {
      "epoch": 0.000652581776653887,
      "grad_norm": 68.01277160644531,
      "learning_rate": 3.4971910112359552e-09,
      "loss": 7.1877,
      "step": 464
    },
    {
      "epoch": 0.0006539882028966755,
      "grad_norm": 54.36450958251953,
      "learning_rate": 3.4831460674157306e-09,
      "loss": 7.8006,
      "step": 465
    },
    {
      "epoch": 0.0006553946291394641,
      "grad_norm": 56.418453216552734,
      "learning_rate": 3.4691011235955055e-09,
      "loss": 7.3975,
      "step": 466
    },
    {
      "epoch": 0.0006568010553822525,
      "grad_norm": 118.5323715209961,
      "learning_rate": 3.455056179775281e-09,
      "loss": 8.2664,
      "step": 467
    },
    {
      "epoch": 0.0006582074816250411,
      "grad_norm": 41.8703727722168,
      "learning_rate": 3.4410112359550562e-09,
      "loss": 7.2517,
      "step": 468
    },
    {
      "epoch": 0.0006596139078678297,
      "grad_norm": 64.07315063476562,
      "learning_rate": 3.4269662921348316e-09,
      "loss": 8.051,
      "step": 469
    },
    {
      "epoch": 0.0006610203341106182,
      "grad_norm": 79.52981567382812,
      "learning_rate": 3.4129213483146065e-09,
      "loss": 9.9899,
      "step": 470
    },
    {
      "epoch": 0.0006624267603534068,
      "grad_norm": 97.96281433105469,
      "learning_rate": 3.398876404494382e-09,
      "loss": 9.8976,
      "step": 471
    },
    {
      "epoch": 0.0006638331865961954,
      "grad_norm": 63.820953369140625,
      "learning_rate": 3.3848314606741577e-09,
      "loss": 7.2726,
      "step": 472
    },
    {
      "epoch": 0.0006652396128389839,
      "grad_norm": 51.024497985839844,
      "learning_rate": 3.3707865168539326e-09,
      "loss": 7.3772,
      "step": 473
    },
    {
      "epoch": 0.0006666460390817724,
      "grad_norm": 102.58784484863281,
      "learning_rate": 3.356741573033708e-09,
      "loss": 6.7994,
      "step": 474
    },
    {
      "epoch": 0.000668052465324561,
      "grad_norm": 105.99397277832031,
      "learning_rate": 3.342696629213483e-09,
      "loss": 10.3597,
      "step": 475
    },
    {
      "epoch": 0.0006694588915673495,
      "grad_norm": 72.39954376220703,
      "learning_rate": 3.3286516853932587e-09,
      "loss": 6.744,
      "step": 476
    },
    {
      "epoch": 0.0006708653178101381,
      "grad_norm": 90.89036560058594,
      "learning_rate": 3.3146067415730336e-09,
      "loss": 7.3611,
      "step": 477
    },
    {
      "epoch": 0.0006722717440529266,
      "grad_norm": 69.85240173339844,
      "learning_rate": 3.300561797752809e-09,
      "loss": 7.6486,
      "step": 478
    },
    {
      "epoch": 0.0006736781702957152,
      "grad_norm": 61.847618103027344,
      "learning_rate": 3.2865168539325847e-09,
      "loss": 7.8969,
      "step": 479
    },
    {
      "epoch": 0.0006750845965385038,
      "grad_norm": 36.105079650878906,
      "learning_rate": 3.2724719101123597e-09,
      "loss": 8.4521,
      "step": 480
    },
    {
      "epoch": 0.0006764910227812922,
      "grad_norm": 38.81999969482422,
      "learning_rate": 3.258426966292135e-09,
      "loss": 7.5422,
      "step": 481
    },
    {
      "epoch": 0.0006778974490240808,
      "grad_norm": 32.269439697265625,
      "learning_rate": 3.24438202247191e-09,
      "loss": 6.2814,
      "step": 482
    },
    {
      "epoch": 0.0006793038752668694,
      "grad_norm": 42.781898498535156,
      "learning_rate": 3.2303370786516857e-09,
      "loss": 6.7184,
      "step": 483
    },
    {
      "epoch": 0.0006807103015096579,
      "grad_norm": 73.62557983398438,
      "learning_rate": 3.2162921348314607e-09,
      "loss": 6.2402,
      "step": 484
    },
    {
      "epoch": 0.0006821167277524465,
      "grad_norm": 100.04428100585938,
      "learning_rate": 3.202247191011236e-09,
      "loss": 7.0189,
      "step": 485
    },
    {
      "epoch": 0.000683523153995235,
      "grad_norm": 58.4175910949707,
      "learning_rate": 3.188202247191011e-09,
      "loss": 6.158,
      "step": 486
    },
    {
      "epoch": 0.0006849295802380236,
      "grad_norm": 39.443058013916016,
      "learning_rate": 3.1741573033707867e-09,
      "loss": 6.9852,
      "step": 487
    },
    {
      "epoch": 0.0006863360064808122,
      "grad_norm": 109.7547607421875,
      "learning_rate": 3.160112359550562e-09,
      "loss": 9.0845,
      "step": 488
    },
    {
      "epoch": 0.0006877424327236006,
      "grad_norm": 57.34946823120117,
      "learning_rate": 3.146067415730337e-09,
      "loss": 6.5523,
      "step": 489
    },
    {
      "epoch": 0.0006891488589663892,
      "grad_norm": 56.8545036315918,
      "learning_rate": 3.1320224719101124e-09,
      "loss": 5.5374,
      "step": 490
    },
    {
      "epoch": 0.0006905552852091778,
      "grad_norm": 39.763179779052734,
      "learning_rate": 3.1179775280898877e-09,
      "loss": 7.9804,
      "step": 491
    },
    {
      "epoch": 0.0006919617114519663,
      "grad_norm": 82.64453887939453,
      "learning_rate": 3.103932584269663e-09,
      "loss": 7.7753,
      "step": 492
    },
    {
      "epoch": 0.0006933681376947549,
      "grad_norm": 76.73717498779297,
      "learning_rate": 3.089887640449438e-09,
      "loss": 9.5178,
      "step": 493
    },
    {
      "epoch": 0.0006947745639375435,
      "grad_norm": 50.949954986572266,
      "learning_rate": 3.0758426966292134e-09,
      "loss": 6.78,
      "step": 494
    },
    {
      "epoch": 0.000696180990180332,
      "grad_norm": 41.09761047363281,
      "learning_rate": 3.061797752808989e-09,
      "loss": 7.0922,
      "step": 495
    },
    {
      "epoch": 0.0006975874164231205,
      "grad_norm": 48.80540084838867,
      "learning_rate": 3.047752808988764e-09,
      "loss": 7.5719,
      "step": 496
    },
    {
      "epoch": 0.000698993842665909,
      "grad_norm": 111.8210220336914,
      "learning_rate": 3.0337078651685395e-09,
      "loss": 7.0399,
      "step": 497
    },
    {
      "epoch": 0.0007004002689086976,
      "grad_norm": 40.363433837890625,
      "learning_rate": 3.0196629213483144e-09,
      "loss": 7.2505,
      "step": 498
    },
    {
      "epoch": 0.0007018066951514862,
      "grad_norm": 40.1840934753418,
      "learning_rate": 3.00561797752809e-09,
      "loss": 7.627,
      "step": 499
    },
    {
      "epoch": 0.0007032131213942747,
      "grad_norm": 75.48822784423828,
      "learning_rate": 2.991573033707865e-09,
      "loss": 10.4614,
      "step": 500
    },
    {
      "epoch": 0.0007046195476370633,
      "grad_norm": 59.207740783691406,
      "learning_rate": 2.9775280898876405e-09,
      "loss": 9.8018,
      "step": 501
    },
    {
      "epoch": 0.0007060259738798519,
      "grad_norm": 44.73796081542969,
      "learning_rate": 2.9634831460674154e-09,
      "loss": 8.1555,
      "step": 502
    },
    {
      "epoch": 0.0007074324001226403,
      "grad_norm": 40.63089370727539,
      "learning_rate": 2.949438202247191e-09,
      "loss": 6.8991,
      "step": 503
    },
    {
      "epoch": 0.0007088388263654289,
      "grad_norm": 122.32804870605469,
      "learning_rate": 2.9353932584269665e-09,
      "loss": 10.1846,
      "step": 504
    },
    {
      "epoch": 0.0007102452526082175,
      "grad_norm": 45.52051544189453,
      "learning_rate": 2.9213483146067415e-09,
      "loss": 7.7284,
      "step": 505
    },
    {
      "epoch": 0.000711651678851006,
      "grad_norm": 62.92042922973633,
      "learning_rate": 2.9073033707865172e-09,
      "loss": 8.1859,
      "step": 506
    },
    {
      "epoch": 0.0007130581050937946,
      "grad_norm": 43.081459045410156,
      "learning_rate": 2.893258426966292e-09,
      "loss": 5.7852,
      "step": 507
    },
    {
      "epoch": 0.0007144645313365831,
      "grad_norm": 31.653453826904297,
      "learning_rate": 2.8792134831460675e-09,
      "loss": 6.5299,
      "step": 508
    },
    {
      "epoch": 0.0007158709575793717,
      "grad_norm": 41.56304931640625,
      "learning_rate": 2.8651685393258425e-09,
      "loss": 5.9831,
      "step": 509
    },
    {
      "epoch": 0.0007172773838221602,
      "grad_norm": 153.96295166015625,
      "learning_rate": 2.8511235955056182e-09,
      "loss": 6.9112,
      "step": 510
    },
    {
      "epoch": 0.0007186838100649487,
      "grad_norm": 100.95860290527344,
      "learning_rate": 2.8370786516853936e-09,
      "loss": 11.8569,
      "step": 511
    },
    {
      "epoch": 0.0007200902363077373,
      "grad_norm": 37.21943283081055,
      "learning_rate": 2.8230337078651685e-09,
      "loss": 6.7845,
      "step": 512
    },
    {
      "epoch": 0.0007214966625505259,
      "grad_norm": 48.4636116027832,
      "learning_rate": 2.808988764044944e-09,
      "loss": 6.9656,
      "step": 513
    },
    {
      "epoch": 0.0007229030887933144,
      "grad_norm": 109.89411926269531,
      "learning_rate": 2.7949438202247193e-09,
      "loss": 9.4528,
      "step": 514
    },
    {
      "epoch": 0.000724309515036103,
      "grad_norm": 155.92181396484375,
      "learning_rate": 2.7808988764044946e-09,
      "loss": 9.7181,
      "step": 515
    },
    {
      "epoch": 0.0007257159412788915,
      "grad_norm": 48.555633544921875,
      "learning_rate": 2.7668539325842695e-09,
      "loss": 5.6376,
      "step": 516
    },
    {
      "epoch": 0.00072712236752168,
      "grad_norm": 29.275909423828125,
      "learning_rate": 2.752808988764045e-09,
      "loss": 6.9786,
      "step": 517
    },
    {
      "epoch": 0.0007285287937644686,
      "grad_norm": 51.760929107666016,
      "learning_rate": 2.73876404494382e-09,
      "loss": 5.9299,
      "step": 518
    },
    {
      "epoch": 0.0007299352200072571,
      "grad_norm": 100.45990753173828,
      "learning_rate": 2.7247191011235956e-09,
      "loss": 7.3067,
      "step": 519
    },
    {
      "epoch": 0.0007313416462500457,
      "grad_norm": 79.4769287109375,
      "learning_rate": 2.710674157303371e-09,
      "loss": 6.4205,
      "step": 520
    },
    {
      "epoch": 0.0007327480724928343,
      "grad_norm": 62.169700622558594,
      "learning_rate": 2.696629213483146e-09,
      "loss": 7.677,
      "step": 521
    },
    {
      "epoch": 0.0007341544987356228,
      "grad_norm": 75.13926696777344,
      "learning_rate": 2.6825842696629217e-09,
      "loss": 7.928,
      "step": 522
    },
    {
      "epoch": 0.0007355609249784114,
      "grad_norm": 64.32005310058594,
      "learning_rate": 2.6685393258426966e-09,
      "loss": 7.1308,
      "step": 523
    },
    {
      "epoch": 0.0007369673512212,
      "grad_norm": 72.76525115966797,
      "learning_rate": 2.654494382022472e-09,
      "loss": 7.8079,
      "step": 524
    },
    {
      "epoch": 0.0007383737774639884,
      "grad_norm": 59.433223724365234,
      "learning_rate": 2.640449438202247e-09,
      "loss": 7.4193,
      "step": 525
    },
    {
      "epoch": 0.000739780203706777,
      "grad_norm": 49.925045013427734,
      "learning_rate": 2.6264044943820227e-09,
      "loss": 8.1192,
      "step": 526
    },
    {
      "epoch": 0.0007411866299495655,
      "grad_norm": 72.58126831054688,
      "learning_rate": 2.612359550561798e-09,
      "loss": 6.541,
      "step": 527
    },
    {
      "epoch": 0.0007425930561923541,
      "grad_norm": 86.38910675048828,
      "learning_rate": 2.598314606741573e-09,
      "loss": 10.2711,
      "step": 528
    },
    {
      "epoch": 0.0007439994824351427,
      "grad_norm": 83.17583465576172,
      "learning_rate": 2.5842696629213487e-09,
      "loss": 9.4696,
      "step": 529
    },
    {
      "epoch": 0.0007454059086779312,
      "grad_norm": 68.63801574707031,
      "learning_rate": 2.5702247191011237e-09,
      "loss": 10.0771,
      "step": 530
    },
    {
      "epoch": 0.0007468123349207198,
      "grad_norm": 83.36463928222656,
      "learning_rate": 2.556179775280899e-09,
      "loss": 11.2388,
      "step": 531
    },
    {
      "epoch": 0.0007482187611635083,
      "grad_norm": 41.233116149902344,
      "learning_rate": 2.542134831460674e-09,
      "loss": 6.9592,
      "step": 532
    },
    {
      "epoch": 0.0007496251874062968,
      "grad_norm": 46.615562438964844,
      "learning_rate": 2.5280898876404498e-09,
      "loss": 7.2087,
      "step": 533
    },
    {
      "epoch": 0.0007510316136490854,
      "grad_norm": 46.786216735839844,
      "learning_rate": 2.5140449438202247e-09,
      "loss": 7.4679,
      "step": 534
    },
    {
      "epoch": 0.0007524380398918739,
      "grad_norm": 79.68522644042969,
      "learning_rate": 2.5e-09,
      "loss": 7.8809,
      "step": 535
    },
    {
      "epoch": 0.0007538444661346625,
      "grad_norm": 80.63094329833984,
      "learning_rate": 2.4859550561797754e-09,
      "loss": 6.5933,
      "step": 536
    },
    {
      "epoch": 0.0007552508923774511,
      "grad_norm": 42.287357330322266,
      "learning_rate": 2.4719101123595503e-09,
      "loss": 5.5097,
      "step": 537
    },
    {
      "epoch": 0.0007566573186202396,
      "grad_norm": 81.18120574951172,
      "learning_rate": 2.4578651685393257e-09,
      "loss": 9.2529,
      "step": 538
    },
    {
      "epoch": 0.0007580637448630281,
      "grad_norm": 54.143104553222656,
      "learning_rate": 2.443820224719101e-09,
      "loss": 6.5481,
      "step": 539
    },
    {
      "epoch": 0.0007594701711058167,
      "grad_norm": 88.41303253173828,
      "learning_rate": 2.4297752808988764e-09,
      "loss": 8.3013,
      "step": 540
    },
    {
      "epoch": 0.0007608765973486052,
      "grad_norm": 66.00526428222656,
      "learning_rate": 2.4157303370786518e-09,
      "loss": 8.0974,
      "step": 541
    },
    {
      "epoch": 0.0007622830235913938,
      "grad_norm": 62.531063079833984,
      "learning_rate": 2.401685393258427e-09,
      "loss": 6.6095,
      "step": 542
    },
    {
      "epoch": 0.0007636894498341824,
      "grad_norm": 51.63917922973633,
      "learning_rate": 2.3876404494382025e-09,
      "loss": 7.1072,
      "step": 543
    },
    {
      "epoch": 0.0007650958760769709,
      "grad_norm": 51.460731506347656,
      "learning_rate": 2.3735955056179774e-09,
      "loss": 9.9219,
      "step": 544
    },
    {
      "epoch": 0.0007665023023197595,
      "grad_norm": 52.086883544921875,
      "learning_rate": 2.3595505617977528e-09,
      "loss": 7.3866,
      "step": 545
    },
    {
      "epoch": 0.0007679087285625479,
      "grad_norm": 59.930667877197266,
      "learning_rate": 2.345505617977528e-09,
      "loss": 8.0686,
      "step": 546
    },
    {
      "epoch": 0.0007693151548053365,
      "grad_norm": 61.52822494506836,
      "learning_rate": 2.3314606741573035e-09,
      "loss": 7.5895,
      "step": 547
    },
    {
      "epoch": 0.0007707215810481251,
      "grad_norm": 74.77330780029297,
      "learning_rate": 2.317415730337079e-09,
      "loss": 9.7596,
      "step": 548
    },
    {
      "epoch": 0.0007721280072909136,
      "grad_norm": 46.182621002197266,
      "learning_rate": 2.303370786516854e-09,
      "loss": 6.4222,
      "step": 549
    },
    {
      "epoch": 0.0007735344335337022,
      "grad_norm": 129.16055297851562,
      "learning_rate": 2.289325842696629e-09,
      "loss": 8.722,
      "step": 550
    },
    {
      "epoch": 0.0007749408597764908,
      "grad_norm": 99.19380187988281,
      "learning_rate": 2.2752808988764045e-09,
      "loss": 8.7927,
      "step": 551
    },
    {
      "epoch": 0.0007763472860192793,
      "grad_norm": 42.844139099121094,
      "learning_rate": 2.26123595505618e-09,
      "loss": 6.9947,
      "step": 552
    },
    {
      "epoch": 0.0007777537122620679,
      "grad_norm": 38.9582405090332,
      "learning_rate": 2.247191011235955e-09,
      "loss": 7.155,
      "step": 553
    },
    {
      "epoch": 0.0007791601385048564,
      "grad_norm": 90.75716400146484,
      "learning_rate": 2.23314606741573e-09,
      "loss": 8.0591,
      "step": 554
    },
    {
      "epoch": 0.0007805665647476449,
      "grad_norm": 58.72828674316406,
      "learning_rate": 2.2191011235955055e-09,
      "loss": 7.6514,
      "step": 555
    },
    {
      "epoch": 0.0007819729909904335,
      "grad_norm": 64.98668670654297,
      "learning_rate": 2.205056179775281e-09,
      "loss": 8.3181,
      "step": 556
    },
    {
      "epoch": 0.000783379417233222,
      "grad_norm": 33.90578842163086,
      "learning_rate": 2.1910112359550562e-09,
      "loss": 6.5813,
      "step": 557
    },
    {
      "epoch": 0.0007847858434760106,
      "grad_norm": 52.68926239013672,
      "learning_rate": 2.1769662921348316e-09,
      "loss": 8.5225,
      "step": 558
    },
    {
      "epoch": 0.0007861922697187992,
      "grad_norm": 90.88200378417969,
      "learning_rate": 2.162921348314607e-09,
      "loss": 8.8389,
      "step": 559
    },
    {
      "epoch": 0.0007875986959615877,
      "grad_norm": 87.47970581054688,
      "learning_rate": 2.148876404494382e-09,
      "loss": 7.6671,
      "step": 560
    },
    {
      "epoch": 0.0007890051222043762,
      "grad_norm": 104.44634246826172,
      "learning_rate": 2.1348314606741572e-09,
      "loss": 9.6393,
      "step": 561
    },
    {
      "epoch": 0.0007904115484471648,
      "grad_norm": 101.82579803466797,
      "learning_rate": 2.1207865168539326e-09,
      "loss": 8.32,
      "step": 562
    },
    {
      "epoch": 0.0007918179746899533,
      "grad_norm": 47.413597106933594,
      "learning_rate": 2.106741573033708e-09,
      "loss": 6.7727,
      "step": 563
    },
    {
      "epoch": 0.0007932244009327419,
      "grad_norm": 39.81709289550781,
      "learning_rate": 2.0926966292134833e-09,
      "loss": 6.6351,
      "step": 564
    },
    {
      "epoch": 0.0007946308271755304,
      "grad_norm": 68.4862060546875,
      "learning_rate": 2.0786516853932586e-09,
      "loss": 6.755,
      "step": 565
    },
    {
      "epoch": 0.000796037253418319,
      "grad_norm": 61.7104377746582,
      "learning_rate": 2.064606741573034e-09,
      "loss": 6.6847,
      "step": 566
    },
    {
      "epoch": 0.0007974436796611076,
      "grad_norm": 79.56301879882812,
      "learning_rate": 2.050561797752809e-09,
      "loss": 7.3671,
      "step": 567
    },
    {
      "epoch": 0.000798850105903896,
      "grad_norm": 74.93370056152344,
      "learning_rate": 2.0365168539325843e-09,
      "loss": 9.903,
      "step": 568
    },
    {
      "epoch": 0.0008002565321466846,
      "grad_norm": 66.68924713134766,
      "learning_rate": 2.0224719101123596e-09,
      "loss": 5.3455,
      "step": 569
    },
    {
      "epoch": 0.0008016629583894732,
      "grad_norm": 117.52432250976562,
      "learning_rate": 2.0084269662921346e-09,
      "loss": 8.3914,
      "step": 570
    },
    {
      "epoch": 0.0008030693846322617,
      "grad_norm": 47.845340728759766,
      "learning_rate": 1.99438202247191e-09,
      "loss": 7.1571,
      "step": 571
    },
    {
      "epoch": 0.0008044758108750503,
      "grad_norm": 71.78963470458984,
      "learning_rate": 1.9803370786516857e-09,
      "loss": 7.5836,
      "step": 572
    },
    {
      "epoch": 0.0008058822371178389,
      "grad_norm": 88.7890625,
      "learning_rate": 1.9662921348314606e-09,
      "loss": 7.1444,
      "step": 573
    },
    {
      "epoch": 0.0008072886633606274,
      "grad_norm": 53.403465270996094,
      "learning_rate": 1.952247191011236e-09,
      "loss": 7.1074,
      "step": 574
    },
    {
      "epoch": 0.000808695089603416,
      "grad_norm": 32.665985107421875,
      "learning_rate": 1.9382022471910114e-09,
      "loss": 7.1256,
      "step": 575
    },
    {
      "epoch": 0.0008101015158462044,
      "grad_norm": 131.95262145996094,
      "learning_rate": 1.9241573033707867e-09,
      "loss": 9.0014,
      "step": 576
    },
    {
      "epoch": 0.000811507942088993,
      "grad_norm": 32.10274887084961,
      "learning_rate": 1.9101123595505617e-09,
      "loss": 5.5166,
      "step": 577
    },
    {
      "epoch": 0.0008129143683317816,
      "grad_norm": 52.30550003051758,
      "learning_rate": 1.896067415730337e-09,
      "loss": 6.1635,
      "step": 578
    },
    {
      "epoch": 0.0008143207945745701,
      "grad_norm": 37.17583465576172,
      "learning_rate": 1.8820224719101124e-09,
      "loss": 5.6805,
      "step": 579
    },
    {
      "epoch": 0.0008157272208173587,
      "grad_norm": 101.87432098388672,
      "learning_rate": 1.8679775280898877e-09,
      "loss": 8.2814,
      "step": 580
    },
    {
      "epoch": 0.0008171336470601473,
      "grad_norm": 38.74977493286133,
      "learning_rate": 1.853932584269663e-09,
      "loss": 5.6902,
      "step": 581
    },
    {
      "epoch": 0.0008185400733029357,
      "grad_norm": 57.32298278808594,
      "learning_rate": 1.8398876404494382e-09,
      "loss": 6.5745,
      "step": 582
    },
    {
      "epoch": 0.0008199464995457243,
      "grad_norm": 128.49349975585938,
      "learning_rate": 1.8258426966292136e-09,
      "loss": 6.7508,
      "step": 583
    },
    {
      "epoch": 0.0008213529257885128,
      "grad_norm": 68.44063568115234,
      "learning_rate": 1.8117977528089887e-09,
      "loss": 10.7038,
      "step": 584
    },
    {
      "epoch": 0.0008227593520313014,
      "grad_norm": 94.90504455566406,
      "learning_rate": 1.797752808988764e-09,
      "loss": 7.2208,
      "step": 585
    },
    {
      "epoch": 0.00082416577827409,
      "grad_norm": 49.96173858642578,
      "learning_rate": 1.7837078651685392e-09,
      "loss": 7.4976,
      "step": 586
    },
    {
      "epoch": 0.0008255722045168785,
      "grad_norm": 46.73810577392578,
      "learning_rate": 1.7696629213483146e-09,
      "loss": 6.2227,
      "step": 587
    },
    {
      "epoch": 0.0008269786307596671,
      "grad_norm": 34.65469741821289,
      "learning_rate": 1.7556179775280901e-09,
      "loss": 5.6063,
      "step": 588
    },
    {
      "epoch": 0.0008283850570024557,
      "grad_norm": 30.96004295349121,
      "learning_rate": 1.7415730337078653e-09,
      "loss": 6.5175,
      "step": 589
    },
    {
      "epoch": 0.0008297914832452441,
      "grad_norm": 55.7960090637207,
      "learning_rate": 1.7275280898876404e-09,
      "loss": 6.1597,
      "step": 590
    },
    {
      "epoch": 0.0008311979094880327,
      "grad_norm": 70.24303436279297,
      "learning_rate": 1.7134831460674158e-09,
      "loss": 8.013,
      "step": 591
    },
    {
      "epoch": 0.0008326043357308213,
      "grad_norm": 45.10006332397461,
      "learning_rate": 1.699438202247191e-09,
      "loss": 5.7163,
      "step": 592
    },
    {
      "epoch": 0.0008340107619736098,
      "grad_norm": 54.54464340209961,
      "learning_rate": 1.6853932584269663e-09,
      "loss": 8.4384,
      "step": 593
    },
    {
      "epoch": 0.0008354171882163984,
      "grad_norm": 80.24736785888672,
      "learning_rate": 1.6713483146067414e-09,
      "loss": 8.349,
      "step": 594
    },
    {
      "epoch": 0.0008368236144591869,
      "grad_norm": 78.60565185546875,
      "learning_rate": 1.6573033707865168e-09,
      "loss": 7.995,
      "step": 595
    },
    {
      "epoch": 0.0008382300407019755,
      "grad_norm": 26.95827865600586,
      "learning_rate": 1.6432584269662924e-09,
      "loss": 7.5671,
      "step": 596
    },
    {
      "epoch": 0.000839636466944764,
      "grad_norm": 45.846492767333984,
      "learning_rate": 1.6292134831460675e-09,
      "loss": 7.365,
      "step": 597
    },
    {
      "epoch": 0.0008410428931875525,
      "grad_norm": 44.407657623291016,
      "learning_rate": 1.6151685393258429e-09,
      "loss": 6.6877,
      "step": 598
    },
    {
      "epoch": 0.0008424493194303411,
      "grad_norm": 51.0265998840332,
      "learning_rate": 1.601123595505618e-09,
      "loss": 7.1789,
      "step": 599
    },
    {
      "epoch": 0.0008438557456731297,
      "grad_norm": 83.83740997314453,
      "learning_rate": 1.5870786516853934e-09,
      "loss": 8.4845,
      "step": 600
    },
    {
      "epoch": 0.0008452621719159182,
      "grad_norm": 31.046588897705078,
      "learning_rate": 1.5730337078651685e-09,
      "loss": 7.2729,
      "step": 601
    },
    {
      "epoch": 0.0008466685981587068,
      "grad_norm": 34.41434860229492,
      "learning_rate": 1.5589887640449439e-09,
      "loss": 6.9387,
      "step": 602
    },
    {
      "epoch": 0.0008480750244014954,
      "grad_norm": 29.56547737121582,
      "learning_rate": 1.544943820224719e-09,
      "loss": 7.4273,
      "step": 603
    },
    {
      "epoch": 0.0008494814506442838,
      "grad_norm": 35.30451202392578,
      "learning_rate": 1.5308988764044946e-09,
      "loss": 7.3296,
      "step": 604
    },
    {
      "epoch": 0.0008508878768870724,
      "grad_norm": 73.43519592285156,
      "learning_rate": 1.5168539325842697e-09,
      "loss": 7.251,
      "step": 605
    },
    {
      "epoch": 0.0008522943031298609,
      "grad_norm": 38.23054504394531,
      "learning_rate": 1.502808988764045e-09,
      "loss": 8.1253,
      "step": 606
    },
    {
      "epoch": 0.0008537007293726495,
      "grad_norm": 128.4187774658203,
      "learning_rate": 1.4887640449438202e-09,
      "loss": 8.4722,
      "step": 607
    },
    {
      "epoch": 0.0008551071556154381,
      "grad_norm": 49.00090408325195,
      "learning_rate": 1.4747191011235956e-09,
      "loss": 7.1492,
      "step": 608
    },
    {
      "epoch": 0.0008565135818582266,
      "grad_norm": 66.81050872802734,
      "learning_rate": 1.4606741573033707e-09,
      "loss": 7.5164,
      "step": 609
    },
    {
      "epoch": 0.0008579200081010152,
      "grad_norm": 117.27339935302734,
      "learning_rate": 1.446629213483146e-09,
      "loss": 7.6781,
      "step": 610
    },
    {
      "epoch": 0.0008593264343438038,
      "grad_norm": 105.00277709960938,
      "learning_rate": 1.4325842696629212e-09,
      "loss": 8.0036,
      "step": 611
    },
    {
      "epoch": 0.0008607328605865922,
      "grad_norm": 85.13128662109375,
      "learning_rate": 1.4185393258426968e-09,
      "loss": 8.1198,
      "step": 612
    },
    {
      "epoch": 0.0008621392868293808,
      "grad_norm": 45.89858627319336,
      "learning_rate": 1.404494382022472e-09,
      "loss": 5.5366,
      "step": 613
    },
    {
      "epoch": 0.0008635457130721693,
      "grad_norm": 59.35641098022461,
      "learning_rate": 1.3904494382022473e-09,
      "loss": 7.2451,
      "step": 614
    },
    {
      "epoch": 0.0008649521393149579,
      "grad_norm": 68.94078063964844,
      "learning_rate": 1.3764044943820225e-09,
      "loss": 6.1607,
      "step": 615
    },
    {
      "epoch": 0.0008663585655577465,
      "grad_norm": 73.17015075683594,
      "learning_rate": 1.3623595505617978e-09,
      "loss": 6.5194,
      "step": 616
    },
    {
      "epoch": 0.000867764991800535,
      "grad_norm": 44.31682205200195,
      "learning_rate": 1.348314606741573e-09,
      "loss": 5.8858,
      "step": 617
    },
    {
      "epoch": 0.0008691714180433236,
      "grad_norm": 103.49669647216797,
      "learning_rate": 1.3342696629213483e-09,
      "loss": 7.8808,
      "step": 618
    },
    {
      "epoch": 0.0008705778442861121,
      "grad_norm": 109.14623260498047,
      "learning_rate": 1.3202247191011235e-09,
      "loss": 8.3679,
      "step": 619
    },
    {
      "epoch": 0.0008719842705289006,
      "grad_norm": 70.85491943359375,
      "learning_rate": 1.306179775280899e-09,
      "loss": 8.7079,
      "step": 620
    },
    {
      "epoch": 0.0008733906967716892,
      "grad_norm": 109.40182495117188,
      "learning_rate": 1.2921348314606744e-09,
      "loss": 7.7685,
      "step": 621
    },
    {
      "epoch": 0.0008747971230144778,
      "grad_norm": 113.34817504882812,
      "learning_rate": 1.2780898876404495e-09,
      "loss": 8.7427,
      "step": 622
    },
    {
      "epoch": 0.0008762035492572663,
      "grad_norm": 38.61219787597656,
      "learning_rate": 1.2640449438202249e-09,
      "loss": 8.9927,
      "step": 623
    },
    {
      "epoch": 0.0008776099755000549,
      "grad_norm": 46.66850662231445,
      "learning_rate": 1.25e-09,
      "loss": 6.2196,
      "step": 624
    },
    {
      "epoch": 0.0008790164017428434,
      "grad_norm": 111.30189514160156,
      "learning_rate": 1.2359550561797752e-09,
      "loss": 6.3846,
      "step": 625
    },
    {
      "epoch": 0.0008804228279856319,
      "grad_norm": 116.95514678955078,
      "learning_rate": 1.2219101123595505e-09,
      "loss": 7.0627,
      "step": 626
    },
    {
      "epoch": 0.0008818292542284205,
      "grad_norm": 106.1827163696289,
      "learning_rate": 1.2078651685393259e-09,
      "loss": 8.2245,
      "step": 627
    },
    {
      "epoch": 0.000883235680471209,
      "grad_norm": 31.092721939086914,
      "learning_rate": 1.1938202247191012e-09,
      "loss": 5.3938,
      "step": 628
    },
    {
      "epoch": 0.0008846421067139976,
      "grad_norm": 101.77027130126953,
      "learning_rate": 1.1797752808988764e-09,
      "loss": 8.693,
      "step": 629
    },
    {
      "epoch": 0.0008860485329567862,
      "grad_norm": 41.580970764160156,
      "learning_rate": 1.1657303370786517e-09,
      "loss": 5.7526,
      "step": 630
    },
    {
      "epoch": 0.0008874549591995747,
      "grad_norm": 40.344871520996094,
      "learning_rate": 1.151685393258427e-09,
      "loss": 9.7792,
      "step": 631
    },
    {
      "epoch": 0.0008888613854423633,
      "grad_norm": 86.54110717773438,
      "learning_rate": 1.1376404494382022e-09,
      "loss": 9.2395,
      "step": 632
    },
    {
      "epoch": 0.0008902678116851517,
      "grad_norm": 51.829612731933594,
      "learning_rate": 1.1235955056179776e-09,
      "loss": 6.1287,
      "step": 633
    },
    {
      "epoch": 0.0008916742379279403,
      "grad_norm": 124.28186798095703,
      "learning_rate": 1.1095505617977527e-09,
      "loss": 7.9217,
      "step": 634
    },
    {
      "epoch": 0.0008930806641707289,
      "grad_norm": 59.93460464477539,
      "learning_rate": 1.0955056179775281e-09,
      "loss": 6.8737,
      "step": 635
    },
    {
      "epoch": 0.0008944870904135174,
      "grad_norm": 59.61752700805664,
      "learning_rate": 1.0814606741573035e-09,
      "loss": 7.991,
      "step": 636
    },
    {
      "epoch": 0.000895893516656306,
      "grad_norm": 41.02978515625,
      "learning_rate": 1.0674157303370786e-09,
      "loss": 6.5924,
      "step": 637
    },
    {
      "epoch": 0.0008972999428990946,
      "grad_norm": 41.999122619628906,
      "learning_rate": 1.053370786516854e-09,
      "loss": 7.182,
      "step": 638
    },
    {
      "epoch": 0.0008987063691418831,
      "grad_norm": 46.96742630004883,
      "learning_rate": 1.0393258426966293e-09,
      "loss": 9.0665,
      "step": 639
    },
    {
      "epoch": 0.0009001127953846716,
      "grad_norm": 33.2470703125,
      "learning_rate": 1.0252808988764045e-09,
      "loss": 7.4485,
      "step": 640
    },
    {
      "epoch": 0.0009015192216274602,
      "grad_norm": 48.39318084716797,
      "learning_rate": 1.0112359550561798e-09,
      "loss": 7.3905,
      "step": 641
    },
    {
      "epoch": 0.0009029256478702487,
      "grad_norm": 118.34640502929688,
      "learning_rate": 9.97191011235955e-10,
      "loss": 7.1774,
      "step": 642
    },
    {
      "epoch": 0.0009043320741130373,
      "grad_norm": 74.128662109375,
      "learning_rate": 9.831460674157303e-10,
      "loss": 6.8079,
      "step": 643
    },
    {
      "epoch": 0.0009057385003558258,
      "grad_norm": 84.10595703125,
      "learning_rate": 9.691011235955057e-10,
      "loss": 8.3866,
      "step": 644
    },
    {
      "epoch": 0.0009071449265986144,
      "grad_norm": 128.49192810058594,
      "learning_rate": 9.550561797752808e-10,
      "loss": 8.6073,
      "step": 645
    },
    {
      "epoch": 0.000908551352841403,
      "grad_norm": 54.81070327758789,
      "learning_rate": 9.410112359550562e-10,
      "loss": 6.9157,
      "step": 646
    },
    {
      "epoch": 0.0009099577790841914,
      "grad_norm": 106.40399169921875,
      "learning_rate": 9.269662921348315e-10,
      "loss": 9.1719,
      "step": 647
    },
    {
      "epoch": 0.00091136420532698,
      "grad_norm": 54.23118209838867,
      "learning_rate": 9.129213483146068e-10,
      "loss": 6.6806,
      "step": 648
    },
    {
      "epoch": 0.0009127706315697686,
      "grad_norm": 45.9439582824707,
      "learning_rate": 8.98876404494382e-10,
      "loss": 7.4135,
      "step": 649
    },
    {
      "epoch": 0.0009141770578125571,
      "grad_norm": 73.84149169921875,
      "learning_rate": 8.848314606741573e-10,
      "loss": 8.3188,
      "step": 650
    },
    {
      "epoch": 0.0009155834840553457,
      "grad_norm": 56.33081817626953,
      "learning_rate": 8.707865168539326e-10,
      "loss": 7.2299,
      "step": 651
    },
    {
      "epoch": 0.0009169899102981343,
      "grad_norm": 38.85358810424805,
      "learning_rate": 8.567415730337079e-10,
      "loss": 7.4391,
      "step": 652
    },
    {
      "epoch": 0.0009183963365409228,
      "grad_norm": 121.34550476074219,
      "learning_rate": 8.426966292134831e-10,
      "loss": 8.5044,
      "step": 653
    },
    {
      "epoch": 0.0009198027627837114,
      "grad_norm": 67.20993041992188,
      "learning_rate": 8.286516853932584e-10,
      "loss": 6.5049,
      "step": 654
    },
    {
      "epoch": 0.0009212091890264998,
      "grad_norm": 54.9505729675293,
      "learning_rate": 8.146067415730338e-10,
      "loss": 5.4269,
      "step": 655
    },
    {
      "epoch": 0.0009226156152692884,
      "grad_norm": 44.63911819458008,
      "learning_rate": 8.00561797752809e-10,
      "loss": 7.5648,
      "step": 656
    },
    {
      "epoch": 0.000924022041512077,
      "grad_norm": 51.741756439208984,
      "learning_rate": 7.865168539325843e-10,
      "loss": 8.4372,
      "step": 657
    },
    {
      "epoch": 0.0009254284677548655,
      "grad_norm": 53.155303955078125,
      "learning_rate": 7.724719101123595e-10,
      "loss": 7.5552,
      "step": 658
    },
    {
      "epoch": 0.0009268348939976541,
      "grad_norm": 73.25978088378906,
      "learning_rate": 7.584269662921349e-10,
      "loss": 8.8442,
      "step": 659
    },
    {
      "epoch": 0.0009282413202404427,
      "grad_norm": 39.30281448364258,
      "learning_rate": 7.443820224719101e-10,
      "loss": 6.8173,
      "step": 660
    },
    {
      "epoch": 0.0009296477464832312,
      "grad_norm": 48.84534454345703,
      "learning_rate": 7.303370786516854e-10,
      "loss": 6.6666,
      "step": 661
    },
    {
      "epoch": 0.0009310541727260197,
      "grad_norm": 28.31987762451172,
      "learning_rate": 7.162921348314606e-10,
      "loss": 6.5389,
      "step": 662
    },
    {
      "epoch": 0.0009324605989688082,
      "grad_norm": 136.94480895996094,
      "learning_rate": 7.02247191011236e-10,
      "loss": 7.8376,
      "step": 663
    },
    {
      "epoch": 0.0009338670252115968,
      "grad_norm": 45.8758659362793,
      "learning_rate": 6.882022471910112e-10,
      "loss": 7.1505,
      "step": 664
    },
    {
      "epoch": 0.0009352734514543854,
      "grad_norm": 73.95365905761719,
      "learning_rate": 6.741573033707865e-10,
      "loss": 6.4528,
      "step": 665
    },
    {
      "epoch": 0.0009366798776971739,
      "grad_norm": 64.51121520996094,
      "learning_rate": 6.601123595505617e-10,
      "loss": 7.5104,
      "step": 666
    },
    {
      "epoch": 0.0009380863039399625,
      "grad_norm": 52.00899887084961,
      "learning_rate": 6.460674157303372e-10,
      "loss": 6.7956,
      "step": 667
    },
    {
      "epoch": 0.0009394927301827511,
      "grad_norm": 50.75996017456055,
      "learning_rate": 6.320224719101124e-10,
      "loss": 5.7311,
      "step": 668
    },
    {
      "epoch": 0.0009408991564255395,
      "grad_norm": 85.8649673461914,
      "learning_rate": 6.179775280898876e-10,
      "loss": 8.6621,
      "step": 669
    },
    {
      "epoch": 0.0009423055826683281,
      "grad_norm": 97.52642059326172,
      "learning_rate": 6.039325842696629e-10,
      "loss": 8.8616,
      "step": 670
    },
    {
      "epoch": 0.0009437120089111167,
      "grad_norm": 35.69184494018555,
      "learning_rate": 5.898876404494382e-10,
      "loss": 6.7936,
      "step": 671
    },
    {
      "epoch": 0.0009451184351539052,
      "grad_norm": 148.48922729492188,
      "learning_rate": 5.758426966292135e-10,
      "loss": 9.2021,
      "step": 672
    },
    {
      "epoch": 0.0009465248613966938,
      "grad_norm": 67.59369659423828,
      "learning_rate": 5.617977528089888e-10,
      "loss": 7.1012,
      "step": 673
    },
    {
      "epoch": 0.0009479312876394823,
      "grad_norm": 104.26171875,
      "learning_rate": 5.477528089887641e-10,
      "loss": 9.2116,
      "step": 674
    },
    {
      "epoch": 0.0009493377138822709,
      "grad_norm": 56.385406494140625,
      "learning_rate": 5.337078651685393e-10,
      "loss": 6.4181,
      "step": 675
    },
    {
      "epoch": 0.0009507441401250595,
      "grad_norm": 30.157405853271484,
      "learning_rate": 5.196629213483147e-10,
      "loss": 6.5704,
      "step": 676
    },
    {
      "epoch": 0.0009521505663678479,
      "grad_norm": 39.6089973449707,
      "learning_rate": 5.056179775280899e-10,
      "loss": 5.3453,
      "step": 677
    },
    {
      "epoch": 0.0009535569926106365,
      "grad_norm": 32.05674743652344,
      "learning_rate": 4.915730337078652e-10,
      "loss": 6.8974,
      "step": 678
    },
    {
      "epoch": 0.0009549634188534251,
      "grad_norm": 132.89749145507812,
      "learning_rate": 4.775280898876404e-10,
      "loss": 8.2029,
      "step": 679
    },
    {
      "epoch": 0.0009563698450962136,
      "grad_norm": 66.89863586425781,
      "learning_rate": 4.6348314606741577e-10,
      "loss": 8.5342,
      "step": 680
    },
    {
      "epoch": 0.0009577762713390022,
      "grad_norm": 51.07952117919922,
      "learning_rate": 4.49438202247191e-10,
      "loss": 6.9125,
      "step": 681
    },
    {
      "epoch": 0.0009591826975817907,
      "grad_norm": 55.58515930175781,
      "learning_rate": 4.353932584269663e-10,
      "loss": 6.5407,
      "step": 682
    },
    {
      "epoch": 0.0009605891238245793,
      "grad_norm": 50.995914459228516,
      "learning_rate": 4.213483146067416e-10,
      "loss": 7.2444,
      "step": 683
    },
    {
      "epoch": 0.0009619955500673678,
      "grad_norm": 92.7785415649414,
      "learning_rate": 4.073033707865169e-10,
      "loss": 10.6599,
      "step": 684
    },
    {
      "epoch": 0.0009634019763101563,
      "grad_norm": 73.54936218261719,
      "learning_rate": 3.9325842696629213e-10,
      "loss": 8.4427,
      "step": 685
    },
    {
      "epoch": 0.0009648084025529449,
      "grad_norm": 63.671630859375,
      "learning_rate": 3.7921348314606743e-10,
      "loss": 6.4,
      "step": 686
    },
    {
      "epoch": 0.0009662148287957335,
      "grad_norm": 56.81027603149414,
      "learning_rate": 3.651685393258427e-10,
      "loss": 5.8782,
      "step": 687
    },
    {
      "epoch": 0.000967621255038522,
      "grad_norm": 39.62815856933594,
      "learning_rate": 3.51123595505618e-10,
      "loss": 6.7944,
      "step": 688
    },
    {
      "epoch": 0.0009690276812813106,
      "grad_norm": 73.85757446289062,
      "learning_rate": 3.3707865168539324e-10,
      "loss": 7.3172,
      "step": 689
    },
    {
      "epoch": 0.0009704341075240992,
      "grad_norm": 49.814697265625,
      "learning_rate": 3.230337078651686e-10,
      "loss": 6.1561,
      "step": 690
    },
    {
      "epoch": 0.0009718405337668876,
      "grad_norm": 146.2742919921875,
      "learning_rate": 3.089887640449438e-10,
      "loss": 10.2662,
      "step": 691
    },
    {
      "epoch": 0.0009732469600096762,
      "grad_norm": 66.5814208984375,
      "learning_rate": 2.949438202247191e-10,
      "loss": 6.6707,
      "step": 692
    },
    {
      "epoch": 0.0009746533862524647,
      "grad_norm": 52.477203369140625,
      "learning_rate": 2.808988764044944e-10,
      "loss": 6.9457,
      "step": 693
    },
    {
      "epoch": 0.0009760598124952533,
      "grad_norm": 78.383056640625,
      "learning_rate": 2.6685393258426965e-10,
      "loss": 8.1867,
      "step": 694
    },
    {
      "epoch": 0.0009774662387380419,
      "grad_norm": 83.30888366699219,
      "learning_rate": 2.5280898876404495e-10,
      "loss": 6.8893,
      "step": 695
    },
    {
      "epoch": 0.0009788726649808304,
      "grad_norm": 79.40353393554688,
      "learning_rate": 2.387640449438202e-10,
      "loss": 6.3385,
      "step": 696
    },
    {
      "epoch": 0.0009802790912236189,
      "grad_norm": 64.19384765625,
      "learning_rate": 2.247191011235955e-10,
      "loss": 8.125,
      "step": 697
    },
    {
      "epoch": 0.0009816855174664076,
      "grad_norm": 117.97189331054688,
      "learning_rate": 2.106741573033708e-10,
      "loss": 7.3704,
      "step": 698
    },
    {
      "epoch": 0.000983091943709196,
      "grad_norm": 40.45357894897461,
      "learning_rate": 1.9662921348314606e-10,
      "loss": 6.9838,
      "step": 699
    },
    {
      "epoch": 0.0009844983699519845,
      "grad_norm": 102.8111343383789,
      "learning_rate": 1.8258426966292134e-10,
      "loss": 9.0157,
      "step": 700
    },
    {
      "epoch": 0.0009859047961947732,
      "grad_norm": 45.9720573425293,
      "learning_rate": 1.6853932584269662e-10,
      "loss": 7.9179,
      "step": 701
    },
    {
      "epoch": 0.0009873112224375617,
      "grad_norm": 81.57991027832031,
      "learning_rate": 1.544943820224719e-10,
      "loss": 10.6428,
      "step": 702
    },
    {
      "epoch": 0.0009887176486803502,
      "grad_norm": 39.15752410888672,
      "learning_rate": 1.404494382022472e-10,
      "loss": 7.0894,
      "step": 703
    },
    {
      "epoch": 0.0009901240749231389,
      "grad_norm": 37.178009033203125,
      "learning_rate": 1.2640449438202248e-10,
      "loss": 7.519,
      "step": 704
    },
    {
      "epoch": 0.0009915305011659273,
      "grad_norm": 59.83745574951172,
      "learning_rate": 1.1235955056179775e-10,
      "loss": 6.311,
      "step": 705
    },
    {
      "epoch": 0.0009929369274087158,
      "grad_norm": 94.1764144897461,
      "learning_rate": 9.831460674157303e-11,
      "loss": 10.2022,
      "step": 706
    },
    {
      "epoch": 0.0009943433536515045,
      "grad_norm": 138.20420837402344,
      "learning_rate": 8.426966292134831e-11,
      "loss": 7.3892,
      "step": 707
    },
    {
      "epoch": 0.000995749779894293,
      "grad_norm": 57.37165069580078,
      "learning_rate": 7.02247191011236e-11,
      "loss": 8.2926,
      "step": 708
    },
    {
      "epoch": 0.0009971562061370815,
      "grad_norm": 38.365570068359375,
      "learning_rate": 5.617977528089888e-11,
      "loss": 7.3047,
      "step": 709
    },
    {
      "epoch": 0.0009985626323798702,
      "grad_norm": 60.770469665527344,
      "learning_rate": 4.2134831460674155e-11,
      "loss": 6.9387,
      "step": 710
    },
    {
      "epoch": 0.0009999690586226587,
      "grad_norm": 38.45618438720703,
      "learning_rate": 2.808988764044944e-11,
      "loss": 7.886,
      "step": 711
    },
    {
      "epoch": 0.0010013754848654471,
      "grad_norm": 211.25692749023438,
      "learning_rate": 1.404494382022472e-11,
      "loss": 10.4096,
      "step": 712
    }
  ],
  "logging_steps": 1,
  "max_steps": 712,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 88665639334272.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
